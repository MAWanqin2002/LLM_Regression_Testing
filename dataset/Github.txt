1
User mattip: <!--
  If you're new to Python and you're not sure whether what you're experiencing is a bug, the CPython issue tracker is not
  the right place to seek help. Consider the following options instead:

  - reading the Python tutorial: https://docs.python.org/3/tutorial/
  - posting in the "Users" category on discuss.python.org: https://discuss.python.org/c/users/7
  - emailing the Python-list mailing list: https://mail.python.org/mailman/listinfo/python-list
  - searching our issue tracker (https://github.com/python/cpython/issues) to see if
    your problem has already been reported
-->

# Bug report

In response to #104049, a fix and a new test were added in #104067 and related PRs. The test [in test_httpserver.py](https://github.com/python/cpython/pull/104067/files#diff-5c8de474c50a44ead423e2d9ce85c1932b9c87ba9f5a3215c0b8b996c65e62b9R424) uses an fstring with an escape sequence, but does not use `r`:


I think the code (in two places) should be include an `r`: `fr'listing for ...`

On PyPy this is causing the `test___all__.py` test to fail, since it emits a warning when compiling the `*.py` to `*.pyc`. I am not sure why CPython is not seeing a similar problem. Perhaps `compileall` is run first? Or the warning filter captures the compilation as well as the import?

# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- CPython versions tested on: PyPy3.9 HEAD using stdlib 3.9.17
- Operating system and architecture: any

<!--
You can freely edit this text. Remove any lines you believe are unnecessary.
-->

User terryjreedy: @mattip CPython does not issue this warning `SyntaxWarning: invalid escape sequence '\?'` until 3.12, though only for normal strings.  What warning is PyPy3.9 emitting?  (Note: try to minimize reproducing examples, as I did below ;-)

@pablogsal In 3.12 and 3.13, f-strings are not warning about invalid escapes that get warnings in real strings.  This seems like a bug.
```
>>> f'\?'
'\\?'
>>> len(f'\?')
2
>>> '\?'
<stdin>:1: SyntaxWarning: invalid escape sequence '\?'
'\\?'


User mattip: On python3.9.17 from Ubuntu, I get this

User mattip: Adding an `r` makes it run without a warning

User terryjreedy: Security patch #104067 was backported to 3.7, which is why it affects PyPy3.9.

@ethanfurman @JelleZijlstra Do the tests added in #104067 require invalid escapes without an 'r' prefix?  Or could the escape be changed or the 'r' added? 


User pablogsal: > @pablogsal In 3.12 and 3.13, f-strings are not warning about invalid escapes that get warnings in real strings. This seems like a bug.

Fixed!
User mattip: ~Thanks. I see that was backported to 3.12. I am curious why not backport it to all versions that contain the test added in #104067 (maybe excepting ones that will no longer be released)? People running the test without precompiling pyc files will see a DeprecationWarning, no?~

Edit: GAA, I didn't actually look at the fix: #105800 is not what I expected.
User mattip: Try as I might, I cannot get CPython3.9 to emit a `DeprecationWarning` in the `test_httpservers.py` test. So I will add the `r` in the PyPy port of the stdlib tests. I am not sure why the test does not emit the warning on CPython3.9, since the f-string needs an `r` if run outside the test, but :shrug:

User pablogsal: Relevant: https://github.com/python/cpython/issues/105821
User lysnikolaou: This can be closed. @mattip An `r` prefix has been added in #105822.
User lazka: Here it only warned/failed when bytecompile was needed, so if there wasn't a .pyc file already for test_httpservers.
---------END---------
2
User ClaireCJS: # Bug report

shutil has a renaming bug

What happened: I hit my first-ever failure due to shutil failure

Folder name is `Kai üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø üè≥Ô∏è‚Äç‚öß 25% off OF (@hey)` and cannot be renamed.



This error message is wrong. The source filename is not incorrect. It exists! I'm looking at it!
And the destination filename? It's cleansed of unicode characters and is a straight-ASCII filename that most definitely is valid.

**The interesting part is that this rename works if you remove the percentage sign, and it also works if you remove the unicode characters. But it does not work if both are present!**


Note that the backstory to this is, I've written a unicode-filename cleanser that is supposed to remove all unicode from all files and folders.  I've successfully run it on hundreds of folder names with unicode in them, and hundreds of thousands of files with unicode in them.

This is the first failure in shutil.


# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- Python 3.10.9
- Windows 10 [Version 10.0.19045.3031] (64-bit)




User eryksun: Please paste the exception in a code block to preserve the exact representation of the source directory name.

User ClaireCJS: It will take me a few hours for the conditions to be recreated and then I will make it happen again and do that :)
User eryksun: If you know the path of the parent directory, you could just provide the ASCII formatted directory listing in a code block. For example:



User ClaireCJS: It doesn't actually exist right now. It's the result of a downloading program that has a queue that can't be managed (the error this thread is about happens in my postprocessing program where I try to fix problematic filename characters--the irony is not lost on me), so I have get to through the queue again. Which is actually good because that will be a better reproduction than me trying to manually create the problematic filename.
User terryjreedy: You should be able to edit you first post by putting *```* lines before and after the exception already there.
User arhadthedev: @terryjreedy Done, on the OP's behalf.
User eryksun: > You should be able to edit you first post by putting pycon
>>> s = '.\\Kai üè¥\U000e0067\U000e0062\U000e0073\U000e0063\U000e0074\U000e007f üè≥Ô∏è\u200d‚ößÔ∏è 25% off OF (@hey)'
>>> print(ascii(s))
'.\\Kai \U0001f3f4\U000e0067\U000e0062\U000e0073\U000e0063\U000e0074\U000e007f \U0001f3f3\ufe0f\u200d\u26a7\ufe0f 25% off OF (@hey)'
```
I have no problem creating and accessing a directory with this name in an NTFS or exFAT filesystem. If it's some other filesystem, maybe it has a problem with the non-BMP characters, which are passed to the filesystem as UTF-16 surrogate pairs. However, that doesn't explain why it works if the percent character is removed. Percent isn't a reserved character. Also, I'm not sure what "remove the percentage sign" actually means. Can the directory be renamed in some other program? Or was a new directory created with a name that's the same except without the percent character?

User ClaireCJS: Sorry if I was unclear. I poked around with the situation a bit before moving on (and I wish I'd made a copy first, so I don't have to wait for the conditions to happen again). 

I am indeed using a renaming program I wrote.  

So:

- the program could successfully rename the file if I removed the % from  the filename first.  (Which would make you think there are special processing issues with the % character, but no, it deals with percents just fine in every other situation, and percents are one of the characters I scrub out of filenames for being problematic sometimes)

and

- the program could successfully rename if i removed the emoji

but

- but not both at once. For some reason. Surrogate characters could very well be the culprit. I'm dealing with input that comes from end-users.


So it seemed the program could succeed with either part that I would be inclined to think might be the cause of the problem (either the emoji, or the percent, or perhaps one of the invisible VARIATION-SELECTOR unicode characters), but not with both parts.

I'm going to try harder to reproduce the behavior when the situation comes back up again tomorrow.  Circumstances prevent its recreation until tomorrow.
User ClaireCJS: Okay, the 3rd party downloading tool has re-created the offending foldername, and I've re-run it through my fix_unicode_filenames.py, and it has failed to rename this (while never failing with hundreds of other unicode/emoji file/foldernames), re-generating the error in question:

Here is the FULLLLL output.



And a screenshot for good measure:

![image](https://github.com/python/cpython/assets/789591/29882148-c858-445e-9760-e65408718d49)

User eryksun: The second example attempts to rename with an invalid filename. NTFS has a maximum name length of 255 16-bit characters (i.e. 255 BMP characters, or 127 non-BMP characters stored as UTF-16 surrogate pairs). 



In the first example, the destination name is valid. It transcribes the zero width joiner (U+200d) as a space instead of "{ZERO WIDTH JOINER}", and it ends with the shorter string `"(@hey)"` instead of `"(@cutekittenkink)"`.



User ClaireCJS: D'ohhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh

I guess... Could this maybe become a feature request for a more meaningful error message so that someone else in my situation doesn't embarrass themselves with a bug report? üòÖ
User eryksun: A filename is invalid either because it contains reserved characters (typically `\/*?<>":|` and ordinals 1-31) or because it's too long. The maximum name length is 255 16-bit characters. The common disk filesystems on Windows (NTFS, ReFS, FAT32, and exFAT) support the upper limit. Some filesystems may have a lower limit.

On Linux, the maximum filename length is 255 8-bit characters. Given UTF-8 encoding, that's between 63 and 255 Unicode characters.

User ronaldoussoren: > D'ohhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
> 
> I guess... Could this maybe become a feature request for a more meaningful error message so that someone else in my situation doesn't embarrass themselves with a bug report? üòÖ

The error message is based on the system error code, in particular the "The filename, directory name, or volume label syntax is incorrect" bit.  The system error code doesn't differentiate between causes for a wrong name, that's why we cannot give a better error message, at least not without trying to reproduce the algorithm the OS uses the validate names.

To validate my reading of the issue: There is no bug here because the script tried to rename a file to a name that is not accepted by the OS (in this case a too long name). Is that correct? 

BTW. Don't feel emberrassed about filing an issue, you're not the first or last that's stumped by some issue that turns out to be easy when someone else points that out.
---------END---------
3
User gtzampanakis: In Python 3.11.4, running "make test" fails under gcc 4.8.5 because both -std=gnu11 and -std=c++11 are specified when compiling _testcpp11ext.

Run the following:



Output:



# Your environment




---------END---------
4
User markshannon: `LOAD_CLOSURE` is identical to `LOAD_FAST_CHECK` in every way except its name and number.

The justification for its existence is that "We keep LOAD_CLOSURE so that the bytecode stays more readable.".
Which is insufficient justification to keep it given that it:
* Uses an instruction, a limited resource which adds bulk to the interpreter
* Prevents superinstruction formation.
* Prevents removal of checks for uninitialized variables.

<!-- gh-linked-prs -->
### Linked PRs
* gh-106059
<!-- /gh-linked-prs -->

User heysujal: I'd like to work on this issue 
User markshannon: Are you sure? This is more complicated than it looks as it involves some awkward compiler internals.
User heysujal: Oh, I thought it would be just deleting some statements
User markshannon: It should be. But the compiler handles local variables in a somewhat convoluted way
User heysujal: Ok, then someone else can pick the issue up.
User polynomialherder: I've been working through this -- I'll update if I have questions or get stuck
User polynomialherder: @markshannon My plan is to add a member to the `_PyCfgInstruction` and `_PyCompile_Instruction`  struct types  instruction called `i_fc` or `i_fromclosure` that indicates whether the instruction originated in a closure, and then add a macro `LOAD_FAST_CLOSURE` (or similar) for adding the `LOAD_FAST` instructions with that member set to 1. Then I'll replace the existing LOAD_CLOSURE calls with those appropriately marked `LOAD_FAST` calls. Then downstream functions like `fix_cell_offsets` can check that member before processing. Let me know if there are any obvious problems with this design, or if you have thoughts on better approaches

For this unit of work, do we want to preserve the current performance characteristics to reduce risk? For instance, I see that replacing `LOAD_CLOSURE` with `LOAD_FAST` would immediately impact some of the optimization functions in `flowgraph.c` like `insert_superinstructions` and `fast_scan_many_locals` that presently don't operate on `LOAD_CLOSURE` instructions. I'm not sure what the consequences would be of allowing those functions to operate on `LOAD_FAST` instructions that would have previously been `LOAD_CLOSURE`, perhaps it would "just work" -- or whether we should add a check of that `i_fc` member to preserve the current behavior

Let me know your thoughts on this -- I'm pretty new to the cpython source code so still piecing together the local variable lifecycle. Thanks!
User markshannon: Rather than add special cases  to `_PyCfgInstruction` and `_PyCompile_Instruction` would it make more sense use the names of the variables as operands, rather than indices into two different lists?
The numbering can then be done in the assembler.

There is no reason to treat `LOAD_CLOSURE` differently to `LOAD_FAST` in the optimizer.

User carljm: If you don't want to tackle a full refactoring of how local variable and cell offsets are handled in this PR, a simpler incremental approach would be to make `LOAD_CLOSURE` a pseudo-instruction (similar to e.g. `STORE_FAST_MAYBE_NULL`) which is emitted in codegen (and thus can still be recognized in `fix_cell_offsets`) but is converted to `LOAD_FAST` in `_PyCfg_ConvertPseudoOps`. This would eliminate `LOAD_CLOSURE` as a real instruction that has to be handled at runtime.

The refactor of cell index handling could still be done as a follow-up PR, allowing removal of the `LOAD_CLOSURE` pseudo-op.
User gvanrossum: (From your post to core-mentorship:)

> When I first set up my cpython development environment, I followed the guidance on the developer‚Äôs guide and have been studying the compiler design page closely. From my reading of it, when I remove an opcode, I need to
> 
> - Remove references to the opcode from the code and update any logic impacted
> - Remove the opcode from Lib/opcode.py and the dis module documentation
> - Run make regen-opcode regen-opcode-targets to regenerate Include/opcode.h and Python/opcode_targets.h
> - Update the magic number in Lib/importlib/_bootstrap_external.py and PC/launcher.c
> - "Add the new bytecode target" 
> - Run make regen-importlib
> 
> The first step is probably most of the work, but I'm a little bit unclear about the second to last step.
> 
> The compiler design page in the developer guide has written
> 
> > Changes to Lib/importlib/_bootstrap_external.py will take effect only after running make regen-importlib. Running this command before adding the new bytecode target to Python/ceval.c will result in an error. You should only run make regen-importlib after the new bytecode target has been added.
> 
> It makes sense to me that if we‚Äôre introducing a new opcode, we should ideally have logic to handle the new instructions in `ceval.c`, but it‚Äôs not clear to me what error would be triggered if we‚Äôre not referencing the new instruction, so I feel I‚Äôm not understanding something. I also don't see most opcodes referenced in ceval.c -- for instance, no LOAD_FAST, STORE_FAST, or JUMP_* instructions, at least not explicitly.

The instructions may well be out of date. (A PR to update them would be welcome of course!)

As of 3.12, the cases in the switch are generated by tooling in Tool/cases_generator (check out the README.md there). This reads Python/bytecodes.c and writes Python/generated_cases.c.h, which in `#include`d by ceval.c in the switch body. **IMPORTANT:** Whenever you edit bytecodes.c, run `make regen-cases` to regenerate the output files.
likely that Carl's suggestion is right: make LOAD_CLOSURE a pseudo op (search for "pseudo" in bytecodes.c) so the compiler can distinguish between it and LOAD_FAST if it needs to, make the assembler translate it to LOAD_FAST. I think that it's totally fine if a sequence of e.g. LOAD_FAST, LOAD_CLOSURE ends up being replaced by a single LOAD_FAST_LOAD_FAST instruction (I think that's what Mark meant).

If you are still stuck, can you push your work to a branch in your GitHub fork of the cpython repo and link it here, and post the errors you are getting (and from which command)?
User polynomialherder: Thank you all for the tips, this helped a lot. I'll proceed with the approach you all suggested, will report back soon with progress
User polynomialherder: As far as automated tests go, would adding a test for `_PyCfg_ConvertPseudoOps` be worthwhile? Or is there a better way to validate this change?
User gvanrossum: Usually when messing around with the interpreter, once you get it to _build_ it's pretty solid, and if you get it to pass the test suite, it's golden. It seems you've already reached that stage. :-) Platinum would be to pass the buildbots, esp. the refleak subset. IIUC only triagers and core devs can trigger a buildbot run (by setting a magic label on the PR), and will do so as part of the PR review. I can do that for you.

Whether a test for `_PyCfg_ConvertPseudoOps` is needed, maybe @iritkatriel has an opinion.
User iritkatriel: ``_PyCfg_ConvertPseudoOps`` can be tested as part of the assembler. See Lib/test/test_compiler_assemble.py, which calls ``_PyCompile_Assemble``. This is pretty new, so there aren't many tests there yet.
User polynomialherder: Thanks @iritkatriel  -- just to make sure before I proceed,  are you suggesting that I expose `_PyCfg_ConvertPseudoOps` in the same way that `_PyCompile_Assemble` has been exposed in `_testinternalcapi.c` so that it can be called from `Lib/test_compiler_assemble.py` (perhaps via helpers in `test.support.bytecode_helper`)?
User iritkatriel: I wouldn't do that. I'd just add a test to Lib/test/test_compiler_assemble.py that creates a code object from an instruction sequence that contains the pseudo ops, and then checks that the code works as expected.

If the pseudo ops are not replaced the code object can't possibly work.


User polynomialherder: As I wrote in the `Core-mentorship` thread, I'll start to think now about how to refactor the cell/local variable tracking  so that we can remove `LOAD_CLOSURE` altogether, unless anyone else is actively working on that piece right now. If anyone has any thoughts on how to proceed OTTOYH, let me know -- otherwise I'll ping back with questions

I'll also keep an eye on the PR which converts `LOAD_CLOSURE` to a pseudo-op and continue to make changes there as needed
---------END---------
5
User hs-vc: I have encountered unexpected behavior while using the  method of the  class. I performed the following test using Python 3.10.10 and 3.11.4.



Based on my understanding, the normalize() method is intended to produce a canonical representation of a decimal number. However, in this case, the values of v1 and v2 are not matching, leading to an AssertionError.

<!-- gh-linked-prs -->
### Linked PRs
* gh-106093
* gh-106128
* gh-106129
<!-- /gh-linked-prs -->

User tomasr8: From the [docs](https://docs.python.org/3/library/decimal.html):

> Unlike hardware based binary floating point, the decimal module has a user alterable precision (defaulting to 28 places)

Your example is 29 places so it gets rounded
User terryjreedy: @rhettinger Decimal question
User ericvsmith: I couldn't see it documented anywhere that `.normalize()` does any rounding. Maybe we should add that?

@mdickinson 
User mdickinson: @ericvsmith Yes, updating the docs is probably a good idea. (I'm not offering to do it, I'm afraid; I don't have the bandwidth in the near future.)

FWIW, the behaviour itself _is_ deliberate, even if questionable: the `normalize` method implements the [reduce](https://speleotrove.com/decimal/daops.html#refredu) operation of the specification. (Aside: the specification changed the name of the operation from "normalize" to "reduce", but Python kept the old name; I'm guessing that the reason for the name change is that "normalize" is confusing, since it has no relationship to "normal" and "subnormal".)

The reduce operation is documented (in the spec) as equivalent to the "plus" operation along with trimming of trailing zeros, and the "plus" operation similarly rounds to the current context (and we have test cases from Mike Cowlishaw that confirm that intention).

Slightly surprisingly, IEEE 754 doesn't seem to have any equivalent operation (perhaps because there are issues at the top end of the dynamic range - e.g., in the IEEE 754 `decimal32` format, `1.234000e96` is representable, but `1.234e96` is not), so we can't use IEEE 754 as a guide to whether the operation "should" round or not. But then we don't need to, since the IBM spec is reasonably clear here.

User rhettinger: > I couldn't see it documented anywhere that .normalize() does any rounding.

That is because rounding is the default for all operations that return a Decimal object except for `__new__`.  The principle is that all numbers are exact even if they exceed the current context precision, that operations are applied to those exact inputs, and that the context (including rounding) is applied *after* the computation.  This leads to surprises if your mental model incorrectly assumes that the numbers are rounded *before* the operation.  To continue the OP's example:



The docs say that `normalize()` is "used for producing canonical values for attributes of an equivalence class."  We could amend that to say,  is "used for producing canonical values an equivalence class within either the current context or the specified context".  That would explain why these two sets have different sizes:


User rhettinger: Also, I'm thinking of adding an entry to the [Decimal FAQ](https://docs.python.org/3/library/decimal.html#decimal-faq) section to demonstrate and explain the notion that numbers are considered exact, that they are created independent of the current context (and can have greater precision), and that contexts are applied *after* an operation:


User rhettinger: Also, I'm thinking that the docs and docstring for `normalize()` should include the wording from the specification:

> It has the same semantics as the plus operation, except that if the final result is finite it is reduced to its simplest form, with all trailing zeros removed and its sign preserved. That is, while the coefficient is non-zero and a multiple of ten the coefficient is divided by ten and the exponent is incremented by 1. Otherwise (the coefficient is zero) the exponent is set to 0. In all cases the sign is unchanged.
---------END---------
6
User pbzweihander: # Bug report

Save the above code as `main.py`:



... and run `python main.py | python` causes:



(`BrokenPipeError` is not relevant here)

Or, run the following code:



... causes the following error:



In Python 3.8.10, the error message is changed as follows:



I think this is a bug, and the bug is due to parsing the Python code to AST causing `RecursionError`.

# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- CPython versions tested on: `Python 3.11.3`
- Operating system and architecture: Arch Linux `6.3.5-arch1-1`

<!--
You can freely edit this text. Remove any lines you believe are unnecessary.
-->

User sunmy2019: Note Python <= 3.10 is no longer maintained for bug fixes.
User corona10: See related issues too.
* https://github.com/python/cpython/issues/60731
* https://github.com/python/cpython/issues/50015
User corona10: > I think this is a bug, and the bug is due to parsing the Python code to AST causing RecursionError.

FYI, it is intended behavior so it is not a bug but if you think that it should be improved. Please let us know the practical usecase.
User pbzweihander: I stepped on this bug when I using  [Amaranth](https://github.com/amaranth-lang/amaranth). Amaranth's `Memory` generates code with many `elif`s. I also think I should report this to Amaranth's issue tracker.
User terryjreedy: The likely solution to a MemoryError is to get more memory.  As I remember, the change from RecursionError to MemoryError is due to a patch to allow use of more of the memory present.
---------END---------
7
User ericsnowcurrently: Once we moved to per-interpreter GIL, the promises in [the docs](https://docs.python.org/3.12/c-api/memory.html#allocator-domains) no longer hold:



It's still fine for pymalloc, but any custom, non-wrapping "mem"/"object" allocators would need to be updated to be thread-safe or per-interpreter.

I have a PR up that does an okay job of adapting such allocators: gh-105619.  However, it penalizes use of such an allocator in subinterpreters that have their own GIL.

Honestly, I'm leaning toward documenting that such allocators must be thread-safe or per-interpreter.  From what I understand, the documented guarantees (in the docs and in PEP 445) are more about representing what pymalloc needs than what custom allocators need.

Perhaps the biggest question is: what projects would be impacted?  I haven't had a chance yet to search for projects that use custom mem/object allocators that aren't thread-safe.  I suspect there aren't more than two or three.
User encukou: Can we add a flag, so that e.g. `PyMem_SetAllocator(PYMEM_DOMAIN_MEM | PyMEM_THREADSAFE, ...)` would set `PYMEM_DOMAIN_MEM` without the wrappers?

---------END---------
8
User vstinner: The ctypes module was first maintained outside Python, then moved into Python stdlib. Its test suite wasn't cleaned recently. I create this issue to track work on this topic.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105758
* gh-105762
* gh-105768
* gh-105797
* gh-105798
* gh-105803
* gh-105814
* gh-105817
* gh-105818
* gh-105819
* gh-105826
<!-- /gh-linked-prs -->

User vstinner: Ok, I think that I merged enough changes. I close the issue :-)
---------END---------
9
User brettcannon: 

See https://bytecodealliance.zulipchat.com/#narrow/stream/223391-wasm/topic/Python.20guest.20runtime.20and.20bindings/near/365948067 where this is being brought up.
User brettcannon: This is probably blocked by upstream having the right support, else I don't see how trying to compile against `dlopen()` will succeed until it's available.
User dicej: I have [a prototype](https://github.com/dicej/component-linking-demo/blob/main/dl.c) of `dlopen` support which I plan to add to `wasi-libc` once I've put it through some real-world tests (including Python extensions).
---------END---------
10
User tps800: # Bug report

Trying to compile python 3.10.12 (as trying to compile 3.11.4, 3.12.0b2) on windows leads to following error:



Renaming `Modules` to `modules` makes the error vanish, but leads to directory `..\Modules\...` not found at other places.

# Your environment

- Visual Studio 2022, 17.6.2
- Cygwin 3.5.x
- MSYS64
- Git 2.41.0_windows_1
- Python 3.10.11 or Python 3.11.4 (Python 3.12.0b1 is not usable -- to many errors).
- Windows 10 with file system partly case sensitive (not only case aware -- git configured to checkout to case sensitive file system and set case sensitivity -> git cloned parts of a file system are case sensitive this way. Directories with case only distinguished entries are possible).

- CPython versions tested on: 3.10.12
- Operating system and architecture: Windows 10, amd64, FS: NTFS

User eryksun: In 3.11, the include in "PC/_testconsole.c" should target "..\\Modules\\_io\\_iomodule.h" instead of "..\\modules\\_io\\_iomodule.h".

https://github.com/python/cpython/blob/v3.11.4/PC/_testconsole.c#L13

This isn't a problem in 3.12+ since "PC/_testconsole.c" no longer includes "Modules/_io/_iomodule.h".
User zooba: Contributions welcome. This is outside of our normal supported dev environment.

If it doesn't apply to 3.13, then start with a 3.12 PR and we can backport from there.
---------END---------
11
User ringohoffman: # Bug report

Related: https://github.com/huggingface/transformers/issues/8978







# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- CPython versions tested on: Python 3.8.16 (default, Jan 17 2023, 23:13:24) [GCC 11.2.0] :: Anaconda, Inc. on linux
- Operating system and architecture: Ubuntu 20.04.6 LTS x86_64

<!--
You can freely edit this text. Remove any lines you believe are unnecessary.
-->

User eendebakpt: @ringohoffman I can confirm the `deepcopy` (or `copy.copy`) fails on python 3.11 with the example above. What is the reason for mixing the `dataclass` decorator and the `dict` or `OrderedDict` subclassing?

For the `OrderedDict` subclassing the `__reduce__` on the `ModelOutput` becomes `<method '__reduce__' of 'collections.OrderedDict' objects>` which is not handling the copy of the dataclass correctly (in combination with the `copy._reconstruct`). I do not know whether this combination is supposed to work

User ringohoffman: `transformers` uses this pattern as part of their `ModelOutput` base class so you can access elements both by attribute and by item: https://github.com/huggingface/transformers/blob/0c3fdccf2f271fb7c44f6ea6e9f4ee234795f2c5/src/transformers/utils/generic.py#L315-L332

`dict` and its subclasses have special handling by `torch` in terms of how it handles splitting tensors that are nested elements of `dict` across threads and processes: https://github.com/pytorch/pytorch/blob/d0ff640ec865dd8c2898b59f1dab689992ed4621/torch/nn/parallel/scatter_gather.py#L50-L51
User sobolevn: CC @rhettinger 
---------END---------
12
User SlaushVunter: # Bug report

If a bound method is used as the condition for BaseExceptionGroup.split the following exception is raised:
> TypeError: expected a function, exception type or tuple of exception types

Consider the following example:

If we replace the bound method with a lambda (`lambda e: self._log_and_ignore_error(e)`) then no exception is raised.

I think it would be useful to accept any callable here. I guess the code update would be simple too, just replace **PyFunction_Check** with **PyCallable_Check** in

Update the exception message to allow callables, and change the assert in

to call **PyCallable_Check** again.

# Your environment
- CPython versions tested on: 3.11.2

The c code snippets are from origin/main.

<!-- gh-linked-prs -->
### Linked PRs
* gh-106035
<!-- /gh-linked-prs -->

User sunmy2019: @iritkatriel 
User iritkatriel: Thank you @SlaushVunter for the report. We could make more callables work with split(), but not all of them because that would be ambiguous - an Exception subclass is callable but it is matched by type and not as a predicate.  Also, this would be a feature request rather than a bug report because [the documentation](https://docs.python.org/3/library/exceptions.html#BaseExceptionGroup.subgroup) says function, not callable. This means that it can only be changed in version 3.13. 

That said, I don't think the code you provided above is a good motivating example for this. Iterating over leaf exceptions of an exception group like this gives you the leaf exceptions with truncated tracebacks (too see how to do this, look at the leaf_generator function in https://peps.python.org/pep-0654/#handling-exception-groups). 

I would probably just ``split`` on some condition, log the matching subgroup (as a whole group) and then raise the non-matching part. The traceback of an exception group contains all the traceback information, laid out in a structured and concise manner.


User SlaushVunter: > 

Thank you for the quick reply, and the quick fix! And sorry for my late answer.

My example was the result of a simplification. In my original example I did not want to log the traceback for the matched leaf exceptions (communication errors), because they are not needed, the error message is enough. Anyway thank you for your suggestion also.
---------END---------
13
User erezinman: Python's ABCs should have `__slots__` attributes to not interfere with slotting in derived classes (especially when the class has no members). However, it is not the case for `contextlib.ContextManager` (as for other `contextlib` ABCs):



Changing `ContextManager` to `MutableMapping`, for example (and setting relevant dunders), does raise an exception.

Observed on Ubuntu 20.04 and Python 3.9.16.
User sunmy2019: Indeed, can you open a PR for that?
User erezinman: @sunmy2019 
Yeah, I'll try to get to that later. To which branch?
User sunmy2019: Changes like this are considered new features. So `main`
---------END---------
14
User sobolevn: # Feature or enhancement

Right now `assert` generate errors that are hard to read, mostly because they lack context.
Example:



# Pitch

Let's make them better:



This is a good starting point.

Later this can be enhanced to use more context.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105935
<!-- /gh-linked-prs -->

User sunmy2019: You need to modify `Lib/traceback.py` and (maybe) make `_format_syntax_error` more general.
User markshannon: > You need to modify `Lib/traceback.py` and (maybe) make `_format_syntax_error` more general.

I don't think that is necessary. Changing the location of the `RAISE_VARARGS` instruction in the assertion should be sufficient.
User heysujal: Can I work on this issue ?
User sobolevn: @heysujal sorry, I am already working on it :(
Please, feel free to take any other free issue. If you want any help, please feel free to ping me.
User sobolevn: @markshannon @Fidget-Spinner I might be missing something, but looks like `assert` already supports this. There are two possible scenarios:
1. `assert` is used in a named file, then it works
2. `assert` is used in a repl, then it does not work (obviously)

Examples, file called `ex.py`:



Result:



The same happens for CPython tests, if I add some failing assert to `test_typing`:



But, REPL does not work for obvious reason: it cannot allow to read source lines. Example (the same code as in the first example):



So, I am not sure what the task is.
User markshannon: What version of Python are you testing against? 
I'm not seeing that behavior and the code suggests that the location is the whole line, not the expression.
https://github.com/python/cpython/blob/main/Python/compile.c#L3893

User sobolevn: I am using the `main` version.
User sobolevn: @markshannon never mind, I was running my own fork with the changes I made ü§¶ 
User markshannon: Good to know that your changes work üôÇ 
User markshannon: We've all done it. I hate to think how much time I've wasted debugging the wrong branch in the past.
User markshannon: We don't want to increase the cost of execution if the assertion passes, which limits what transformations we can apply.
We can improve a few cases, though.

### Boolean operation.

`assert cond1 and cond2` is compiled as:

We could compile it as:

which is a bit bulkier, but is as fast as it executes the same VM instructions when the assert passes.

### Comparison of local variable to constant or local variable

`assert var1 == var2`
Since we can recompute `var1` or `var2` without side effects, we can embed the values in the error message:

This is complicated somewhat if `str(var1)` raises.
We can add an instrinsic to handle the odd cases.


User sobolevn: I can surely work on this after the initial PR is merged üëç 
---------END---------
15
User duanev: # Documentation

[Parsing arguments and building values](https://docs.python.org/3/c-api/arg.html#arg-parsing) contains hardly any examples.  Multiple arguments as well as multiple types of arguments should be demonstrated.

---------END---------
16
User ericsnowcurrently: Currently `_xxsubinterpreters.run_string()` fails if the interpreter has other threads running, e.g. subthreads created by a previous `run_string()` call.  This should be fixed to allow other threads to be running.
---------END---------
17
User marcusmueller: # Bug report

## Background Story

GNU Radio has a python program design tool, the GNU Radio companion. It creates Python programs from visually designed flow graphs.

User expect to be able to define command line parameters. We support that by generating `ArgumentParser`-based argument parsers.

Since GNU Radio is used a lot with scientific notation, it's very common for users to enter numbers such as `2.4e9` on the command line, or `-3e-12`.

The latter is what breaks

## Problem



works with



but doesn't work with


 
as the latter is misinterpreted, with `-3e12` being interpreted as optional argument that's undefined.

## Root analysis

argparse.py specifies 

https://github.com/python/cpython/blob/8da9d1b16319f4a6bd78435016ef1f4bef6e2b41/Lib/argparse.py#L1402-L1403

That's too restrictive.



would be appropriate and actually cover most negative number formats

# Your environment

Python 3.11.3 (F37 x86), but also CPython main (v3.12.0b1-321-g8da9d1b163)

# Impact

Yeah, it's annoying, because we can't go out and repair deployed code, monkey-patch everyone's parser. This leads to surprising outages. 

The proposed alternative regex might be made default, with the option to switch back to the over-restrictive parser if arguments starting with `-\d*\.?\d+e` are registered (which, honestly, sounds unlikely).

# Related issue

https://github.com/python/cpython/issues/53580 which I found is related, but less solution-oriented seems to have been closed as "WONTFIX" with the slightly vague argument that the change is hard (based on proposing an overly simple regex). Seeing my proposal, hm, not sure I agree.

# Alternatives

### Alternative Regexes
(that regex I propose might not be optional. If we can actually find the regex that describes negative floating point numbers, that's what I'd propose we use)

### Passing of a user-defined negative-number regex

Since monkeypatching does work, at least for the main parser, a colleague brought up the idea of adding a keyword argument to `ArgumentParser` which would allow for user-defined modification of that string, at zero risk of breaking existing use cases

---------END---------
18
User csreddy98: Url parser is allowing only 'v' in the IPvFuture. As per the standards the 'v' could be case-insensitive as mentioned [here](https://www.rfc-editor.org/rfc/rfc3986#page-19)


- `urlparse('http://[v1fe.d:9]/')` is valid
- `urlparse('http://[V1fe.d:9]/')` throws `ValueError`

while `http://[V1fe.d:9]/` is also a valid IPvFuture hostname.

Checked in python version 3.11.4

<!-- gh-linked-prs -->
### Linked PRs
* gh-105709
<!-- /gh-linked-prs -->

---------END---------
19
User TheCarpetMerchant: # Bug report

During `make -j 4` after having configured optimizations :
The installed version of openssl is `1.1.1n-0+deb11u4+rpt1`


# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- Operating system and architecture: aarch 64 (Raspbian 11), Raspberry Pi 4 Model B 4GB
User chgnrdv: Have you installed `libssl-dev` package? I'm not Raspbian user, but pretty sure that installation process doesn't differ much from Debian one.
User TheCarpetMerchant: I've installed the following package after reading a few stackoverflow posts :
`libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev`

OpenSSL works without problems.
User ronaldoussoren: Try installing ``libffi-dev`` as well, that should fix the build error for ``_ctypes``.  [The devguide](https://devguide.python.org/getting-started/setup-building/) contains a list of packages to install on Debian, which should also work on Raspian. In short:



That command includes a number of additional packages relative to what you already already installed. 
User TheCarpetMerchant: Still getting the same error, albeit with fewer missing packages :

User ronaldoussoren: It's hard to say without seeing a full build log, but this likely still indicates that some build requirements are missing, e.g. the various '-dev ' packages that should be installed according to the devguide. 
User TheCarpetMerchant: Where can I find the 'deb-src' url for Raspbian ?

The following packages are installed :

User erlend-aasland: You should be able to run `./.github/workflows/posix-deps-apt.sh` from within your git cloned CPython repo, which will install pretty much all dependencies for you.


---------END---------
20
User mjpieters: #103848 updated the URL parsing algorithm to handle IPv6 and IPvFuture addresses when parsing URLs.

However, the algorithm is incomplete. `[` and `]` are only permitted in the hostname portion **if they are the first and last characters** and only if they then contain an IPv6 or IPvFuture address. The current implementation ignores everything before the first `[` and everything after the first `]` found in the `netloc` portion.

The WhatWG URL standard states that [`[` and `]` are forbidden characters in a hostname](https://url.spec.whatwg.org/#host-miscellaneous), and the [host parser](https://url.spec.whatwg.org/#concept-host-parser) only looks for IPv6 or IPvFuture if the `[` and `]` characters are the first and last characters of the section, respectively.

The current implementation thus accepts such bizarre hostnames as:

- `http://prefix.[v1.example]/`
- `http://[v1.example].postfix/`

but then only reports the portion between the brackets as the hostname:



The `.netloc` attribute, in both cases, contains the whole string.

Both URLs should have been rejected instead.

# Your environment

- CPython versions tested on: 3.12.0b1
- Operating system and architecture: Darwin M1


User arhadthedev: @orsenthil (as an urllib module [expert](https://devguide.python.org/core-developers/experts/))
User csreddy98: I think this should return a valid object only if the hostname starts with a `[` and ends with `]`. With the current logic any string with `[` and `]` inside the hostname is being considered as a valid IPv6 or IPvFutire hostnames. I believe we must verify the start and end characters of a hostname as mentioned [here](https://www.rfc-editor.org/rfc/rfc3986#page-19) for IPv6 and optionally IPvFuture.
User lscottod: Just to add to this thread, 

I'm using django, django-environ and postgres. This issue is quite significant since I, unfortunately, happen to have brackets inside the postgres passwords in several prod/staging servers. This password is given inside a URL to django-environ that parses it (using urllib) then sends it back to django. 

As far as Python 3.11.3, everything was going well, but since 3.11.4 it's all broken now. 

It is related to what is stated above. urlsplit now spots '[' and ']' inside the netloc and what's inside theses brackets (a fragment of the password) is wrongfully considered a hostname. It then throws an exception since it tries to convert it to an ip address. 

The lines that seem to be of importance : 
in urllib/parse.py -- urlsplit()




 It feels like an important breaking change/regression that doesn't seem documented
---------END---------
21
User ericsnowcurrently: There's an isolation leak somewhere.  It may be just in the _xxsubinterpreters module, but I suspect it's not.

See https://github.com/python/cpython/pull/99114#issuecomment-1520952650.

Reproducers:

* https://gist.github.com/tonybaloney/262986212e1061b97908657a53a605d6
* https://gist.github.com/tonybaloney/504be6d1541610f50963c32a3f1c379d

FYI, I see crashes on this fairly infrequently.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105740
* gh-105765
<!-- /gh-linked-prs -->

User ericsnowcurrently: Probably the same thing:

([AMD64 Arch Linux TraceRefs 3.12](https://buildbot.python.org/all/#/builders/1197/builds/58/steps/5/logs/stdio))


User ericsnowcurrently: maybe related: https://github.com/python/cpython/issues/105690
---------END---------
22
User jose1711: # Bug report

With an empty `.python_history` file:


Here's another non-interactive test-case


# Your environment

- CPython versions tested on:

This is the latest available `python3` version available from YUM repository.

- Operating system and architecture:
AIX 7.2, TL5 SP5, PowerPC:

User sobolevn: Can you reproduce this with 3.11 or 3.12 or 3.13?
3.7 does not support bugfixes for quite some time now and soon will be reach its EOL.
User jose1711: The latest Python3 version available in AIX toolbox is 3.9.16: https://www.ibm.com/support/pages/aix-toolbox-open-source-software-downloads-alpha.

The behaviour with 3.9 is different. REPL no longer raises error:


However the second test-case still fails:

---------END---------
23
User UndoneStudios: <!--
  If you're new to Python and you're not sure whether what you're experiencing is a bug, the CPython issue tracker is not
  the right place to seek help. Consider the following options instead:

  - reading the Python tutorial: https://docs.python.org/3/tutorial/
  - posting in the "Users" category on discuss.python.org: https://discuss.python.org/c/users/7
  - emailing the Python-list mailing list: https://mail.python.org/mailman/listinfo/python-list
  - searching our issue tracker (https://github.com/python/cpython/issues) to see if
    your problem has already been reported
-->

When executing


and giving an input of, say, `ghi` to the prompt, `help` terminates with an error.

However, the IDLE calltip for help changes from 

to

(This is after ending the `help` session and typing `help(`)

What is weird is that if I do the same thing again, except that I provide an input of `abc`, as an example of a valid name, the calltip restores back to what it originally was.

Besides, the calltip also changes for other names. If I reproduce this issue, and then I type `print(`, the same calltip 

comes.

Is this intended behaviour?

**NOTE:** This behaviour is not specific to `help`. See https://github.com/python/cpython/issues/105689#issuecomment-1590474399.

# My environment

- CPython versions tested on: 3.11.3
- Operating system and architecture: Windows 11

(Note: I wanted to attach a picture, but calltips go away as soon as I take a screenshot)
User terryjreedy: The first bug, which is not an IDLE issue, is the regression of help() crashing upon invalid names.  It should catch the exception and print stuff, including a new help prompt, as it used to do.  #105702.  The issue here is whether IDLE could be made immune to this or whether something like inspect, which IDLE uses, is messed up.  So I will try to find out where the wrong tip is coming from.

User UndoneStudios: It seems to me that the first issue only appeared in 3.11 or so. Before that, it would say:

and continue.
Now, it's an error, and the prompt ends there.
User terryjreedy: The discussion of the help bug is on the issue I opened and the issue it duplicated, #102541.  There was a related issue just before 3.11.0 was released.

The wrong message line `Import can't find module, or can't find name in module.` comes from exceptions.c, 1674-9.

Among whatever else this call does, it makes the line the doc string of ImportError.  Since ImportError has no accessible signature, the line is the calltip for ImportError.  For some reason I will investigate another day, calltip.getargspec(ob) is being called with ob == ImportError, not with the function the user types.



User UndoneStudios: But why does *that* also affect every other object's calltip (like `print`'s?)
User UndoneStudios: Also, I've found another bug. 

> What is weird is that if I do the same thing again, except that I provide an input of abc, as an example of a valid name, the calltip restores back to what it originally was.

Apparently, only some inputs will restore it back to its original value. Trying `print` does not change it back.
User terryjreedy: > But why does that also affect every other object's calltip (like print's?)

Because IDLE is displaying the calltip for ImportError regardless of what is typed.  I am looking into this now.  Directly causing ImportError does not do this.  `help` is doing something strange.

User terryjreedy: Progress: let m = help error line number with `    raise ImportError('''\` and n = user input line number with, for instance, `print(`.  (Initially, n = m+5 because of the additional help error lines.)  The latter invokes calltip.Calltip.open_calltip, which starts with 

sur_paren should be and normally is `('n.0', 'n.end')` (with `n` replaced with the actual line number).  But after the help error, it is instead `('m.21', 'n.end')`  (21 is the column with '('.)   Hyperparser subsequently looks backward from the error '(' to find 'ImportError'.  Even after more input and output, sur_paren continues to find 'm.21'.  Print the error message from help() has somehow left Shell in an incorrect state.

The HyperParser initialization correctly sees 'insert' as referring to line n.  The problem is with get_surrounding_brackets.  This is exposed to users as Edit => Show Surrounding Parens (default hotkey ^0 (zero) as least on Windows).  Normally, if one types `help(` and ^0, only the last '(' is highlighted.  After the help error message that prints an unmatched '(', ^0 highlights back to that unmatched paren in the error output.  There are really two bugs here: 1) looking for a '(' before the first one found, and 2) crossing the boundary between output and following input.  

Debugging this appears to involve the separate pyparse module.  A deep solution would be separate text widgets for code input and resulting output but I have only started a draft issue for this.

What may be a factor is that Shell treat responses to executed input('prompt') statements, as with `help>`, as code input -- colorizing and displaying calltips in response to '('s.  This is a known bug but has so far merely been annoying.

User UndoneStudios: I've found a potential fix: I've replaced line 1744 in `pydoc.py` to be like this:

It seems to be fixed now. All calltips are now working properly after causing an incorrect input

User UndoneStudios: Actually, these kind of issues can be easily reproduced with code similar to the following:

After this, *every* calltip gets replaced with Exception's `__doc__`.

Apparently, calltips need to be modified, not just `help`.
User terryjreedy: Thank you.  The help() bug is fixed, so a reproducer is needed to experiment and to test an fix in the main branch.  The reduced code with comments.

Given that the fence-finder goes back too far, I think the unclosed `'''` causes `int(` or whatever to be seen as part of an unfinished multiline string, making the unmatched `(` in the traceback to seem like the closest opener.

More experiments: `print('("' + '""')` disables calltips because `(` is preceded by nothing.  Repeating restores calltips because the 2nd printed `"""` closed the first.  `print('int("' + '""')` makes calltips the `int` calltip.

Please leave the title as I rewrote it.

 The best solution before splitting Shell as discussed above is to make the fence-finder in Shell start at the beginning of the statement.  This should be possible now.  But with help() fixed, I don't consider this a high priority.

User UndoneStudios: About the title, I think that'll be okay for now.

It turns out the minimal required code to reproduce this is

I would consider this a somewhat midline priority thing to fix because programs that raise multiline errors can mess up calltips for programs in interactive mode afterward (this bug is ONLY reproducible in interactive mode).
User terryjreedy: Leave this bug open until fixed.
---------END---------
24
User markshannon: As we move towards generating more components from the instruction definition file (bytecodes.c), it helps if the instructions have regular formats and stack effects.
Currently the stack effects can be simple, variable, conditional or complex.
* Simple: Does not depend on the operand (`oparg`)
* Variable: Either pops or pushes `oparg * k` items.
* Conditional: Pushes a value conditional on `oparg & 1`
* Complex: The stack effect depends on a set of flags embedded in `oparg`.

We can easily get rid of the complex case. There are only two cases, `MAKE_FUNCTION` and `FORMAT_VALUE`. Both can easily broken down into simple (and faster) parts.

We might want to get rid of condition stack effects, as it would simplify things, but performance may suffer as both `LOAD_GLOBAL` and `LOAD_ATTR` are conditional and they are performance critical.


<!-- gh-linked-prs -->
### Linked PRs
* gh-105680
* gh-105843
<!-- /gh-linked-prs -->

User gvanrossum: @markshannon Can this be closed yet? I doubt that we'll be removing conditional stack effects (at least conditional pushes). Note that we now support conditional pushes in the final op of a macro. (If needed the same approach could be used to support conditional pops in the first op.)
User markshannon: Yes, this can be closed.
We might want to remove conditional stack effects in the future, but not now.
---------END---------
25
User DBJim: The `SND_PURGE` and `SND_NOWAIT` flags defined in winsound are unsupported for many years and ignored by modern Windows.

I propose to remove them in 3.13.

Reference:
https://learn.microsoft.com/en-us/previous-versions//dd743680(v=vs.85)

<!-- gh-linked-prs -->
### Linked PRs
* gh-105681
<!-- /gh-linked-prs -->

User zooba: Unfortunately, they're not marked as deprecated with a planned removal in our docs. The first step should be a deprecation warning if someone passes them into our function.

But really, is this an important change to make? They don't cost anything, and they live inside their own extension module so aren't even adding memory pressure for most users.

I'd rather not remove things that could break users when it's not going to add any value. (For reference, it took me [one search](https://github.com/search?q=snd_nowait+language%3APython&ref=opensearch&type=code&l=Python) to find a number of code samples that currently work but will raise `AttributeError` after this change, so we need to be able to explain to them why this is a _good_ thing.)
User DBJim: Thanks for your feedback @zooba 

I think there is value in cleaning up, but I'll be guided by yourself and the community whether it's worth it to remove these with a deprecation period.
User JelleZijlstra: I would suggest not changing anything here; the documentation already says these flags are unsupported, and as @zooba says there's very little value in removing them from the implementation.

This would be a good use case for Victor's proposed soft deprecation though (https://discuss.python.org/t/formalize-the-concept-of-soft-deprecation-dont-schedule-removal-in-pep-387-backwards-compatibility-policy/27957?u=jelle).
User DBJim: Thanks, good idea. I will close the issue and I'll keep an eye on the proposed soft deprecation. Appreciate the feedback.
---------END---------
26
User nedbat: Python 3.12 introduced a change in tracing behavior.  Now a conditional in a finally block will revisit the condition before exiting the block.

Here is test.py:


When traced under 3.11:


When traced under 3.12:


The extra line is marked with `<******`.  There's no reason for the if statement to be traced again.

User Eclips4: Confirmed on current main branch
User Eclips4: Bisected to #98001, @nedbat can you confirm that?
User Eclips4: Why change of location is needed here?
https://github.com/python/cpython/blob/20a56d8becba1a5a958b167fdb43b1a1b9228095/Python/compile.c#L3236
I've deleted it(same for `compiler_try_star_finally`) and seems this fixes the issue, but I don't sure that's correct solution.

---------END---------
27
User sweeneyde: A recent test failure:



The issue is `reference_find`:



When `s == p == ''`, we get `for i in range(0)`, and then `-1` is returned instead of the actual result which should be 0.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105640
<!-- /gh-linked-prs -->

User terryjreedy: My immediate question was "What could have happened that this should suddenly fail?"  The answer is "Nothing!"  For each CI run, reference_find is called 1000 times with random input.  There is obviously a non-zero probabilty of generating this corner case for which the test as a whole is wrong.  Hence we get an occasional bogus failure.  Some possible fixes:
* Do not generate this corner case; I have no idea how right now.
* Do not test this easy-to-detect case: `if not p or not text: continue` after `text` is generated, or perhaps `if not p: continue` after `p` is generated.
* Change reference_find to return 0 for this case, *making sure to not introduce another bug*.

The test was added in #86138, with PR by Dennis Sweeny and Tim Peters.

---------END---------
28
User asottile: I'm testing out the 3.12 beta and I find the warning from [PEP 706](https://peps.python.org/pep-0706/) to be unactionable for most uses

I'm ok accepting the default, and I'm ok accepting the new default in 3.14 -- but my assertion is that most people don't care about explicitly setting this and that the 3.14 default will be ok.  the DeprecationWarning as it is right now will generate a lot of (imo unnecessary) noise about the new feature

in my case I'm extracting tar files that I both completely trust and also would be extracted identically in the `filter='data'` case (which will be the default in 3.14).  I really would only care about the DeprecationWarning if I were doing exceptional things or dealing with exceptional tar files

ideally it should only fire if it would alter the output or error -- otherwise it is just noise
User zware: IIUC, the solution is for you to set `TarFile.extraction_filter` as you like.  The point of the warning is to *be* noisy, to force you the user into acknowledging the danger of the default.
User asottile: warnings for the sake of noise seems a bad stance to take.  it's why people ignore them and don't fix things.  warnings should be actionable
User zware: These warnings are actionable: set `TarFile.extraction_filter`.
User asottile: they aren't -- the default for 3.14 works fine for me and does not change behaviour.  if I do nothing the warning resolves itself.  in the interim it is forcing me to make a change for no benefit other than to silence a warning
User encukou: I would like to do it that way, but I didn't find a reliable, cross-platform way to check whether the end result would be different without a filter.
Granted, this is mostly for changes are for details that probably wouldn't matter to you, like user and mode. But where would you draw the line?
---------END---------
29
User asottile: # Feature or enhancement

I would like to access `parse_template` to be able to eagerly validate that a regex replacement template is valid.

I'm [using this in my python-based text editor to do find and replace](https://github.com/asottile/babi/blob/f0f4a91e4e4ab2a01e58b57fa44d058961422e3e/babi/screen.py#L503-L508).  Currently I am reaching into `sre_parse` to get `parse_template` (which used to be a public-named module but was deprecated in 3.11)

I suspect this could be as simple as exposing `parse_template` -- my code doesn't actually care about the return value, simply whether it raises an exception or not
User sobolevn: Refs https://github.com/python/cpython/issues/105456
User AlexWaygood: Cc. @serhiy-storchaka
User raghunandanbhat: I started working on this and added `parse_template()` in  `re.__init__.py`

`
Considering the very limited amount of documentation about this function, I would appreciate some help in testing it thoroughly and writing detailed documentation.
User serhiy-storchaka: Is regex replacement template validation only what you need? Or do you need more information from the result?

Over a year ago, I wrote an implementation to compile a regex replacement template. I expected that this could be used to speed up critical code, like `re.compile()`, but I didn't see much difference compared to implicitly caching the compiled regex replacement templates. So I excluded it from #91524.
User asottile: personally I only care about validation
---------END---------
30
User sobolevn: See my reasoning here: https://github.com/python/cpython/pull/104248#issuecomment-1585579473

<!-- gh-linked-prs -->
### Linked PRs
* gh-105628
<!-- /gh-linked-prs -->

User sunmy2019: Indeed, `None` better represents what we have here.
---------END---------
31
User zhatt: The fix to #89564 produced a significant performance degradation when logs are on an NFS filesystem.  The fix for #89564  was  for a bug found with the TimedRotatingFileHandler but an fix was added to the sized base file rotator too.

Here is the RotatingFileHandler.shouldRollover() function.   The os.path.exists() and os.path.isfile() are very slow on NFS filesystems.  On Linux systems, if os.path.isfile() returns False, the self.stream.seek(0,2) will always return 0 because the file is not seekable so non-isfile() files will never be rotated unless the rotation size is smaller than the message size.

Since, the rollover could be triggered with a very large msg and small maxBytes.  Moving the exists() and isfile() check to inside the `if self.stream.tell()...` would cover that case and not run the expensive status operations with a correctly configured logger except when the rotation is needed.  That is similar to the fix made to the TimedRotatingFileHandler in #96159.




<!-- gh-linked-prs -->
### Linked PRs
* gh-105887
<!-- /gh-linked-prs -->

User arhadthedev: cc @vstinner as an author of the linked issue gh-89564 and @vsajip as an author of its associated PR.
User vstinner: Do you want to propose a fix? (Pull request)
User zhatt: @vstinner I will create a PR.
---------END---------
32
User michaelfm1211: To parse HTTP headers, the `http.client` module uses the `email.parser.Parser` class, as headers are in the same format. The relevant function for this is [`_parse_header_lines`](https://github.com/python/cpython/blob/main/Lib/http/client.py#L224-L236). Because `http.server` uses `http.client` to parse its headers, this function is also responsible for parsing headers on the server side.

However, `_parse_header_lines` doesn't pass an [email policy](https://docs.python.org/3/library/email.policy.html) to `email.parser.Parser`. The documentation mentions that [a policy should always be provided, and that the default policy will change in the future](https://docs.python.org/3/library/email.parser.html#email.parser.BytesFeedParser), so it's probably a good idea to change the code to use a specific policy.

The current default policy for `email.parser.Parser` is [`email.policy.compat32`](https://docs.python.org/3/library/email.policy.html#email.policy.compat32), which is a bit outdated but provides complete backwards compatibility. Because the documentation says that this won't be the case forever, I suggest that we change `_parse_header_lines` to use [`email.policy.default`](https://docs.python.org/3/library/email.policy.html#email.policy.default) instead. While this is a breaking change, it would greatly extend support for the many extra RFCs that `email.policy.default` already supports.

One of these RFCs in particular that this change would start supporting is RFC 2047, which includes receiving encoded-word header values and would resolve part of issue #105530.

Note: [this is also on discuss.python.org](https://discuss.python.org/t/upgrading-http-client-to-use-default-email-policy-for-parsing/27632).

<!-- gh-linked-prs -->
### Linked PRs
* gh-105918
<!-- /gh-linked-prs -->

---------END---------
33
User nullstd: <!--
  If you're new to Python and you're not sure whether what you're experiencing is a bug, the CPython issue tracker is not
  the right place to seek help. Consider the following options instead:

  - reading the Python tutorial: https://docs.python.org/3/tutorial/
  - posting in the "Users" category on discuss.python.org: https://discuss.python.org/c/users/7
  - emailing the Python-list mailing list: https://mail.python.org/mailman/listinfo/python-list
  - searching our issue tracker (https://github.com/python/cpython/issues) to see if
    your problem has already been reported
-->

# Bug report


When running the following code, I ran into some unresolved future error message as shown below:


Output



Because the child process exited so early, a `BrokenPipeError` was raised, and the event loop tries to disconnect relevant internal pipes, but one future `_stdin_closed` ( in [subprocess.py](https://github.com/python/cpython/pull/13098/files#diff-84f561d3c1c4ea1724bb6fb3b3ba10b2a3c81c011014432e50ba34fa81845915) ) is not properly handled. It's introduced in https://github.com/python/cpython/pull/13098

I also tested on Python 3.7.17 there's no such error message.

# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- CPython versions tested on: Python 3.11.3, 3.7.17 
- Operating system and architecture: macOS Ventura 13.3

<!--
You can freely edit this text. Remove any lines you believe are unnecessary.
-->

Some links I found helpful:

https://stackoverflow.com/questions/14207708/ioerror-errno-32-broken-pipe-when-piping-prog-py-othercmd

https://stackoverflow.com/questions/23688492/oserror-errno-22-invalid-argument-in-subprocess#28020096

Could you help take a look?  @asvetlov   

cc @1st1 



---------END---------
34
User brandtbucher: The generated code in `Python-ast.c` is missing error checks following the construction of C-level `alias`, `arg`, `comprehension`, `keyword`, `match_item`, and `withitem` nodes from their Python object counterparts. This means it's possible to crash the interpreter by attempting to compile an AST where a required member of these nodes is replaced with `None`:



I'll have a PR up in a minute with the one-line fix.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105589
* gh-105838
* gh-105839
<!-- /gh-linked-prs -->

---------END---------
35
User AlexWaygood: # Feature or enhancement

We should discourage and deprecate `typing.AnyStr`.

# Pitch

`typing.AnyStr` is bad for many reasons:

1. The name implies that it has something to do with the type `Any`. It has nothing to do with the type `Any`.
2. The name implies that it means "any string". It does not mean "any string".
3. `AnyStr` is a `TypeVar`, but the name does not follow the common naming convention for TypeVars (using a "T" suffix). Many users appear to think that it is equivalent to `str | bytes`, which is incorrect.
4. `AnyStr` is the only type variable that is publicly exported from the `typing` module. Unusually, it is a constrained type variable. Constrained type variables are usually not what users want for modern APIs. Bound type variables, in general, have more intuitive semantics than constrained type variables.
5. One of the motivations for PEP-695 (accepted by the Steering Council, and now implemented) was the fact that reusable type variables can be confusing in terms of their scope. In general, I believe the consensus of the typing community is that using PEP-695 syntax for creating type variables clarifies the scope of type variables and makes them more intuitive for users. As such, we should discourage using reusable TypeVars such as `AnyStr`.

For all of these reasons, `AnyStr` is _very_ commonly misused, especially by typing beginners. We get many PRs at typeshed that misuse `AnyStr`, and it can often be hard to catch these misuses in CI (careful manual review is required).

Therefore, we should discourage and deprecate `typing.AnyStr`. Unfortunately, it is very widely used, so the deprecation period will have to be a long one.

I propose the following plan:

1. Clarify the docs for `typing.AnyStr`. Explain more clearly the differences between `AnyStr` and a union; give examples of uses of `AnyStr` that would be invalid. This docs clarification can be backported to 3.12 and 3.11.
2. In Python 3.13, state in the docs that using `AnyStr` is deprecated and that users are encouraged to use PEP-695 syntax wherever possible.
3. In Python 3.16, remove `AnyStr` from `typing.__all__`, and start emitting a `DeprecationWarning` if a user does `from typing import AnyStr` or accesses `typing.AnyStr`.

   Removing it from `__all__` will be a breaking change, but it's the only way to emit a `DeprecationWarning` for `typing.AnyStr` before removing it unless we're okay with emitting a `DeprecationWarning` any time a user does `from typing import *` (and I'm not).
4. In Python 3.18, remove `AnyStr` from the `typing` module.

Thoughts?
User hauntsaninja: I think maybe still too aggressive... if we did deprecation warnings in the first version of Python released after 3.11 end of life, users could respond to the warning by using the recommended alternative PEP 695 syntax.

edit: a slower timeline was edited into the original post, so this comment no longer applies
User AlexWaygood: > I think maybe still too aggressive... if we did deprecation warnings in the first version of Python released after 3.11 end of life, users could respond to the warning by using the recommended alternative PEP 695 syntax.

Yes, that makes sense. Python 3.11 will be end-of-life in October 2027, and Python 3.16 will be released in October 2027. So, introduce the deprecation warnings in Python 3.16? Or do you think Python 3.17?
User JelleZijlstra: We can perhaps afford to be more aggressive here because users who want to avoid the deprecation warning have a simple workaround that works on all versions: they can write `AnyStr = TypeVar("AnyStr", str, bytes)` themselves.
User hauntsaninja: Probably 3.16, unless enough people actually start testing alphas and betas by then that we want to reduce friction ;-)

Yeah, I guess so. But I think I'd prefer users make one change instead of two, and if users just inline AnyStr they still have a misleadingly named object and reusable type variables. I'd also want the warning to clearly suggest the alternative and having two alternatives depending on what you support muddies the message a little.
User AlexWaygood: I think I agree with @hauntsaninja. If people just replace the really-badly-named stdlib type variable with an identical really-badly-named type variable in their own code, that kinda defeats the point :)

So let's go with introducing deprecation warnings in Python 3.16, and removing it from Python in 3.18. (I've edited my original post to reflect that.)
User sobolevn: Two extra points:
1. Replace our own usages of `AnyStr`
2. I also propose adding one (or more) `@overload` example to show how to replace `AnyStr` usage in `def some(x: AnyStr) -> AnyStr:` to keep the same type-checking behaviour

---------END---------
36
User pganssle: # Documentation

At the PyCon US 2023 sprints, I spent some time with @glyph, @hauntsaninja, @AlexWaygood and others trying to put together a plan to make the type stubs for `datetime` distinguish between na√Øve and aware datetimes (even if the runtime types don't reflect this). Unfortunately, we came to the conclusion that there's no way to do it in a backwards-compatible way that accounts for subclasses without the use of higher-kinded types (this is not the first time we came to that conclusion, and unfortunately in neither case did we write down the exact reason this is a problem as far as I know, so, uh... let's remember to do that next time we tilt at this particular windmill).

Some time ago, however, @glyph did come up with a set of `Protocol` classes that basically solves the problem for anyone who is willing to opt in to it, since that frees him from certain backwards compatibility concerns. The upshot of the conversation was that the best we can do from the CPython side is probably to add a link to [`DateType`](https://github.com/glyph/DateType) into the [`datetime`](https://docs.python.org/3/library/datetime.html) documentation, presumably in the "See also" infobox, along with a recommendation that it can be used to improve type checking of na√Øve vs. aware datetimes.

I realized that we didn't get around to this in the sprints and we never made an issue to track the progress, so when this recently came up in [this issue](https://github.com/python/cpython/issues/105544), it reminded me to open this issue.

CC: @abalkin 

<!-- gh-linked-prs -->
### Linked PRs
* gh-105946
<!-- /gh-linked-prs -->

---------END---------
37
User vstinner: Currently, when an unittest.TestCase has two test methods with the same name, only the second one is run: the first one is silently ignored. In the past, the Python test suite had many bugs like that: see issue #60283. I just found a new bug today in test_sqlite3: issue #105557.

It would be nice to automatically detect when a test method is redefined. The problem is to avoid false alarms on method redefined on purpose: right now, I don't know if there are legit use cases like that.

Here is a prototype:



Output:



It only emits a warning on the second Tests.test_bug() method definition. It doesn't emit a warning when RealTests redefines test_bug(): is it a legit use case to override an "abstract" test method of a "base" test case class? This case can also be detected if needed.
User vstinner: If someone wants to propose a PR based on my proof-of-concept, please go ahead. I'm not really interested to implement a fix for this issue right now. I just report the issue :-)

cc @erlend-aasland @serhiy-storchaka 
User serhiy-storchaka: Yes, this problem is known and was automatically detect was discussed before, but AFAIK it was not materialized in a PR.

The proposed solution would have false positive report for the following example:


Also, it only works if tests with the same name are defined in the same class inherited from TestCase. It does not help if a test is overridden in a subclass or defined in a mixin class (or different mixin classes), which may be more common type of errors. But a solution which checks that the test does not override functions in superclasses would fail for many legitimate use cases.

User sobolevn: This solution will also add a potential meta-class conflict for tools that already use metaclasses, like test plugins, custom loaders, etc.
User erlend-aasland: It seems like this is not an easy feature to add :( Suggesting to close this issue.
User vstinner: I understand that unittest.TestCase class namespace cannot be customed, because unittest is written to be extended, and metaclasses and other hacks are common.

If it's not possible to change unittest.TestCase class, would it make sense to have a different solution to at least detect duplicate method names in the Python test suite? Maybe using a custom Python source code parser?

Demo using ``ast.NodeVisitor``:



This Proof-Of-Concept doesn't understand decorators or how to build test case cases using mixin classes and class inheritance. But it may be able to catch some obvious bugs. If such tool detects false positives later, it can be enhanced later, no?
User erlend-aasland: Perhaps it can be made as a tool that is run by patchcheck?
User sobolevn: `pyflakes` can do it:
<img width="788" alt="–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2023-06-21 –≤ 09 32 13" src="https://github.com/python/cpython/assets/4660275/ee1eee6b-9dce-4192-9f32-d2d087a4aa8c">

Or even better `flake8` (it has more configuration options and uses `pyflakes` inside):
<img width="788" alt="–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2023-06-21 –≤ 09 33 15" src="https://github.com/python/cpython/assets/4660275/e4ceb980-7657-4941-9c68-c44594c3badf">

User erlend-aasland: @sobolevn, is it possible to configure **flake8** to only detect duplicate test method names? If so, we could simply add that to the CI, with no changes needed in the code base.
User sobolevn: > to only detect duplicate test method names?

No, it will detect all duplicated functions and methods and some similar things: duplicated classes, imports, etc.

<img width="788" alt="–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2023-06-21 –≤ 10 18 47" src="https://github.com/python/cpython/assets/4660275/c8da8594-8c38-459f-b36e-17c4275cd6c4">

I think that all of these checks make sense in our case.
User erlend-aasland: Related:

- gh-60283

(Oh, Victor already referenced this issue; sorry 'bout the noise üòÑ )
User vstinner: > pyflakes can do it

That's exactly how I found #105557 and why I created this issue... Maybe we can just use pyflakes in a CI job, only looking for this specific kind of issue?
User erlend-aasland: > Maybe we can just use pyflakes in a CI job, only looking for this specific kind of issue?

+1

I created a follow-up issue in the core workflow repo. Let's close this and #60283 as superseded.
User sobolevn: I can take this on, I don't have any active issues at the moment to work on.

If anyone else wants it, I am totally fine :)
User erlend-aasland: Superseded by python/core-workflow#505
---------END---------
38
User vstinner: Example with ``Lib/encodings/mac_arabic.py`` generated from ``VENDORS/APPLE/ARABIC.TXT``:



pyflakes generates warnings like:


---------END---------
39
User mborus: # Documentation

When installing a new Python version on Windows, the py launcher automatically selects this for the

    py

and 

    py -3 

command.

When you are testing the newest Python beta you normally don't want this as a default.

The customization instructions are not clear enough on that.

https://docs.python.org/3/using/windows.html#customization

When you save a `py.ini` file in `%LOCALAPPDATA%` with this content,

    [defaults]
    python=3.11.4

starting 

    py 

will launch version 3.11.4, but starting 

    py -3

will launch version 3.12.02b.

You need to change the `py.ini` to 

    [defaults]
    python=3.11.4
    python3=3.11.4

to make the "-3" switch work.

The documentation in 4.8.4.1. / 4.8.4.2.  could hint this, because it's really difficult to find out why `py -3` is not working when you're used to always define if you want version `-2` or `-3`.

(Haven't used `-2` in a long time but still use the `-3` to be safe)

Testcase: Install the current Python3.12 beta on a Windows 10 machine with Python 3.11. Save the `py.ini` file in  `%LOCALAPPDATA%`, then from a newly opened `cmd.exe` run `py` and `py -3`
User eryksun: The usage of `PY_PYTHON`, `PY_PYTHON3`, and `PY_PYTHON2` is discussed in "Customizing default Python versions" (4.8.4.2). I would prefer for the examples to also include "py" commands such as "py", "py -2", "py -3", "py -3.7", and "py -3.7-32", in addition to the shebang commands. I would also prefer for "Customization via INI files" (4.8.4.1) to come after the detailed explanation of the environment variables. It should be merged with the paragraph that begins with "[i]n addition to environment variables, the same settings can be configured in the .INI ...", just before the examples for the "[defaults]" section in "py.ini".

---------END---------
40
User gvanrossum: There are some tests for the cases generator in Tools/cases_generator/test_generator.py, but several things are wrong with these:

- [x] They are currently broken
- [ ] They depend on pytest
- [ ] They aren't run in CI
- [ ] They are incomplete

Let's fix that (in that order).

<!-- gh-linked-prs -->
### Linked PRs
* gh-105707
<!-- /gh-linked-prs -->

---------END---------
41
User erlend-aasland: _See https://github.com/python/cpython/issues/103837#issuecomment-1541808307 and https://github.com/python/cpython/issues/103837#issuecomment-1543936079_:

EAA:
> I still think we should consider changing the underlying API so we can emit a resource warning if close failed.

VS:
> IMO sqlite3 should be modified to emit a ResourceWarning.
>
> Currently, no ResourceWarning is emitted when a connection is not closed explicitly:
---------END---------
42
User mzhu22: # Documentation

#28907 fixes enum creation taking quadratic time relative to the number of members, but _only_ for members whose values are hashable. If not fixed, ideally this would be documented somewhere as it could potentially be a big performance trap

---------END---------
43
User michaelfm1211: <!--
  If you're new to Python and you're not sure whether what you're experiencing is a bug, the CPython issue tracker is not
  the right place to seek help. Consider the following options instead:

  - reading the Python tutorial: https://docs.python.org/3/tutorial/
  - posting in the "Users" category on discuss.python.org: https://discuss.python.org/c/users/7
  - emailing the Python-list mailing list: https://mail.python.org/mailman/listinfo/python-list
  - searching our issue tracker (https://github.com/python/cpython/issues) to see if
    your problem has already been reported
-->

# Bug report

When receiving HTTP headers in MIME encoded-word format (per RFC 2047), the `http` module does not decode the header's value out of encoded-word. For example:


Additionally, when setting a header to a string containing a non-ISO-8859-1 character, a `UnicodeEncodeError` exception is thrown, however, this could be solved by just using MIME encoded-word. For example:


# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- CPython versions tested on: 3.11.3
- Operating system and architecture: macOS 11.7.4, x86_64

<!--
You can freely edit this text. Remove any lines you believe are unnecessary.
-->


<!-- gh-linked-prs -->
### Linked PRs
* gh-105531
* gh-105621
<!-- /gh-linked-prs -->

User michaelfm1211: After the full test suite failed on my first PR for this issue (#105531), I looked into this a bit more. I think the change would be better as two PRs:
1. #105621 is the sending portion. This part should not have any breaking changes and should be relatively straightforward. It just handles the potential `UnicodeEncodeError` by falling back to RFC 2047 encoded-word.
2. The other part will be the receiving portion. So far I've thought of two ways to do this: either upgrade `http.client` to parse headers using the default email policy rather than `email.policy.compat32` (which is described in more depth in issue #105622), or do it as a standalone change.
User davidism: This does not seem correct. Can you point to the modern standard from https://httpwg.org/specs/ (or even an old standard) that says that _HTTP_ clients should encode headers like this, or that servers should decode them automatically?

HTTP headers have a few different "common" formats, but each HTTP/1.1 header really needs to be treated on a case-by-case basis as many have their own quirks. The only common encoding format I've seen and implemented for Werkzeug's header parsing is for dict-like headers: `Header: key1*=UTF-8''%ab, key2*=...`. I would be very surprised if `http.server` suddenly started returning pre-decoded UTF-8 data, especially for an old email format instead of what's commonly used in HTTP.
---------END---------
44
User stefanv: I define the following UserDict, that implements nested lookups (`d['key.subkey']`):



I expect the following to work as it did in Python 3.11, but it does not:



I do not see any release notes indicating that `__getitem__` is no longer the correct method to override.

User Eclips4: This also reproduced on the current main.
Seems that in 3.12 and newer versions `__getitem__` isn't called in `.get` 
User Eclips4: Also, if you define a `__contains__` with the same behavior, it will work on 3.12.

Introduced in #17910
After some research of this issue, I came to the conclusion that the best solution here would be to add documentation of that.
User stefanv: IUUC, the proposed fix is to change the implementation to the following:



Not ideal, but it works.

---------END---------
45
User sobolevn: Right now there are places in `test_enum` that for some reason contain code like https://github.com/python/cpython/blob/4ff5690e591b7d11cf11e34bf61004e2ea58ab3c/Lib/test/test_enum.py#L53-L76

For some reason we try to catch exceptions that won't ever happen (at least, they never *should* happen).

But, there are also regular `Enum`s that are defined without any exceptions: https://github.com/python/cpython/blob/4ff5690e591b7d11cf11e34bf61004e2ea58ab3c/Lib/test/test_enum.py#L78-L101

This is an artifact of 10+ age: https://github.com/python/cpython/commit/6b3d64ab5d7a448535a10811234d4ef99ddb54b0#diff-8467f9fbbff81abf26d87a8dbbf0e0c866157971948010e48cc73539251a9e4cR14

I propose to remove this and backport this PR to all supported Python versions.
PR will be available quite soon :)

<!-- gh-linked-prs -->
### Linked PRs
* gh-105523
<!-- /gh-linked-prs -->

User ethanfurman: @sobolevn You are correct that they never _should_ happen, but if they do none of the tests in the file will run, so there will be no information on which tests did pass.

Rather than get rid of the existing exception handling, please update the global enums that do not have it (and their tests, of course).
User sobolevn: Ok, if that's easier for you to work on / debug `enum.py`. I will update the PR shortly.
User sobolevn: I finally had time to look at this. Here are my thoughts.

## Raising an error for "protected" Enum

I've added these lines to `EnumType.__prepare__`:



Here are the results:



I've noticed that around 10 tests do not reraise errors if `*Stooges` is an exception instance.

## Raising an error for an "unprotected" Enum

I went with `IntFlagStoogesWithZero`, so the whole suite failed with an error.



## Conclusion

Indeed, the first case is much more informative and helpful. But, I need to:
1. Update all "unprotected" enum definions
2. Reraise exception whenever any of these enums are used in tests
---------END---------
46
User AlexWaygood: # Feature or enhancement

`typing.Annotated` is currently implemented as a class, but doesn't need to be. All other objects like it in the `typing` module are implemented as instances of `typing._SpecialForm`, and we can do the same here. This simplifies the code a lot (making it easier to maintain in the future), and should also be _slightly_ more performant.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105510
<!-- /gh-linked-prs -->

---------END---------
47
User MathieuB1: # Bug report

Hi everyone,

When passing headers that contain utf-8 characters (in my case a Cookie), it failed to encode the correct charset because stuck on latin-1 encoding.



Can you please fix this one and set the default encoding as utf-8? I don't see why the 'latin1' charset have been set. It should be related to this change https://github.com/python/cpython/commit/8f36af7a4c9409a673412e4bdfbad76d700abc3a

Many thanks in advance

# Your environment

python3.9

User michaelfm1211: I'm not so sure about this. First of all, [RFC 9112](https://www.rfc-editor.org/rfc/rfc9112#name-field-syntax) explicitly warns about using Unicode characters as a potential security risk (however, I don't know enough about Python's implementation to know if it really would, but it's generally better to be safe). Additionally [this Stack Overflow answer](https://stackoverflow.com/a/4410331/13376511) (although ten years old) says `latin-1` (aka: ISO-8859-1) is the largest encoding that's ever been supported, so all of UTF-8 is definitely not supported by the standard.

If we really *did* want to allow Unicode characters, then I think it'd be better to automatically MIME encode/decode (RFC 2047) with UTF-8 ([here's a quick summary from Wikipedia](https://en.wikipedia.org/wiki/MIME#Encoded-Word) if you're not familiar with this).

In your case of using cookies, you could also alternatively just [URL encode](https://en.wikipedia.org/wiki/URL_encoding) the cookies and get rid of this error (using [`urllib.parse.quote`](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.quote) or [`urllib.parse.urlencode`](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlencode)). By doing some [research](https://stackoverflow.com/a/28508471/13376511) on Stack Overflow, this looks like the general consensus on how to solve the issue you have. I'm not aware of any language/framework that URL encodes cookies for you, so I'm not sure it'd be particularly useful for Python to do it either.
User MathieuB1: Thanks for the investigation, after reading ([RFC 2047](https://datatracker.ietf.org/doc/html/rfc2047)) you mentionned,
I see that it could be possible to allow utf-8 encoding inside of headers 
by using =?utf-8? (encoded-words) syntax. But I can't see this implementation inside of http/client.py as its a thing dedicated to mail.

In my opinion the default encoding set at iso-8859-1 (Latin-1 see [RFC 5997](https://datatracker.ietf.org/doc/html/rfc5987) ) should be reviewed as 
there is not only European caracters that are used by people in headers. For a security perspective yes it allows more characters to be decoded, but on Devs side we will have this error when trying to be full utf-8 (headers included) on a request:



In fact its not really what is expected when you have your OS, Server and your Proxy all set to UTF-8 charset.
The cookie seems more like an exception for supporting only ISO-8859-1 and ASCII.
Also I would be curious to see the behavior with Java and C++ servers handling headers in UTF-8.
User michaelfm1211: I agree, it would make sense to have the `http` module support Unicode via encoded-word, considering Unicode is the default throughout most of the standard library. I actually realized `http.client` doesn't handle encoded-word at all, even if it's receiving those headers from the server. Getting encoded-word in a header might be surprising and confusing to a beginner, so at the very least, we should handle it when parsing headers in responses.

Yesterday I opened a separate issue on this encoded-word bug alone ([gh-105530](https://github.com/python/cpython/issues/105530)), and wrote [this PR](https://github.com/python/cpython/pull/105531) to attempt to fix the issue. It ended up failing some tests beyond the `http` module (not sure why, but I'll try to fix it up later today), but that's one possible resolution to this issue.

Lastly, I agree that gauging how other languages/servers handle Unicode in servers and clients would be helpful to determine the best course of action here in Python.
---------END---------
48
User JelleZijlstra: Currently, unions created through `typing.Union[A, B]` and through the PEP-604 syntax `A | B` are at runtime instances of completely different types, and they differ in exactly what elements they accept. This is confusing and makes it harder for users to detect unions at runtime.

I propose to proceed in two steps:

1. Make `typing.Union` an alias for `types.UnionType` and make it so `types.UnionType[A, B]` works, accepting the same types `Union` accepts now.
2. Loosen the rules for what the `|` operator accepts to accept more types that are commonly used in unions.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105511
<!-- /gh-linked-prs -->

User JelleZijlstra: I put up a first implementation at #105511. Some thoughts on the details:

- I implemented it by making `typing.Union` just a re-export of `types.UnionType`, but I feel it might actually be more intuitive if `typing.Union` was the canonical name of the object, and `types.UnionType` was an alias. We could change the implementation to set the module and name differently.
- The repr() of all unions changes to use the `|` syntax.
- I added dummy `__name__`, `__qualname__`, and `__origin__` fields to `types.UnionType` to satisfy some `test_typing` tests.
- We no longer support writing to a union's `__args__` attribute. A test relied on this, but nobody should have been assigning to `__args__` anyway, so I'm fine with this change.
- There's a couple of behavior changes around `issubclass()` and subclassing because `types.UnionType` is (unlike `typing.Union`) an actual type.
User tibbe: This broken an AST walker I had written for Python 3.9, after I upgraded all my `Union[X, Y]` to `X | Y`, something is expected to be purely syntactical change.
---------END---------
49
User The-Compiler: With code such as:



Python 3.10.11 prints `Flag.B`, and so does Python 3.11.3. However, with Python 3.11.4, this happens instead:



As a workaround, a detour via `.value` works in this case:



This causes issues with PyQt, which has the following flags ([as bindings from C++](https://github.com/qt/qtbase/blob/v6.5.1/src/corelib/global/qnamespace.h#L1047-L1057)):



(Output from Python 3.10 - with Python 3.11, `KeyboardModifierMask` goes missing in the output, and so does `Flag.Mask` above, but that seems like a different issue?)

With my project, I'm [trying to remove a modifier](https://github.com/qutebrowser/qutebrowser/issues/7735) from the given flags. With Python 3.10.11 and 3.11.3:



But with Python 3.11.4, same issue as above:



As a culprit, I suspect:

- #103365
- #103494
- #103513

cc @ethanfurman @benburrill

<!-- gh-linked-prs -->
### Linked PRs
* gh-105542
* gh-105571
* gh-105572
<!-- /gh-linked-prs -->

User The-Compiler: Thanks @ethanfurman for the quick fix! I can confirm this fixes things for my project (qutebrowser) as well. I suppose this can be closed then?
User benburrill: May be worth noting that there are still a few subtle differences between the new Flag behavior and the behavior from 3.10.

In Python 3.10, `~Flag.Mask == Flag(0)`, and `Flag(252)` would have raised a ValueError.
In Python 3.11.3, `~Flag.Mask == Flag(0) == Flag(252)` due to the behavior of CONFORM
In Python 3.11.4, `~Flag.Mask == Flag(0) != Flag(252)` (STRICT is still not strict enough to restore 3.10 behavior)
With @ethanfurman's fix, `~Flag.Mask == Flag(252) != Flag(0)`.  

I think this change is probably fine for most cases, but it would be a good idea to add some more test cases.
User ethanfurman: Good catch, @benburrill -- I definitely want `~Flag.Mask == Flag(0)`.
---------END---------
50
User sobolevn: Repro:



We can access all other attributes, but not `__copy__` and `__deepcopy__`:



I am not quite sure what is the right thing to do here ü§î 

<!-- gh-linked-prs -->
### Linked PRs
* gh-105488
<!-- /gh-linked-prs -->

---------END---------
51
User sobolevn: The simplest repro:



I think that it should be:



We already had similar issues in the past: https://github.com/python/cpython/pull/102637

This happens somewhere in `ga_repr`, I will have a look.
---------END---------
52
User iritkatriel: 
We would ideally have bytecodes.c as the single source of truth about opcodes. So opcode.py and the code generated from it should be replaced by alternatives from the cases_generator, if we can.




<!-- gh-linked-prs -->
### Linked PRs
* gh-105482
* gh-105506
* gh-105788
* gh-105791
* gh-105865
* gh-105913
* gh-105950
<!-- /gh-linked-prs -->

User iritkatriel: @gvanrossum The old macros HAS_ARG and HAS_CONST are exposed in Include/opcode.h (and have been for a long time). Does this mean that before we can remove them, we need to expose the opcode_metadata.h file there as well? We should also mark all of this as unstable API (how do we do that?)
User gvanrossum: Neither `HAS_ARG` nor `HAS_CONST` is mentioned in the docs at all. I am of the opinion that in this case this implies they are not public, and anyone using them should not be surprised if they disappear. (It would be more of an issue if they turned into lies.)

opcode.h is mentioned only once in the docs, as the source of truth for dis.py. (It is also mentioned twice in Misc/NEWS.d/, but that's really just a changelog.)

I do think we ought to provide similar functionality to debuggers.

Info about declaring unstable APIs comes from PEP 649, which links to https://devguide.python.org/developer-workflow/c-api/#c-api. It looks like you must use the `PyUnstable_` prefix (e.g. `PyUnstable_BytecodeHasArg`), and it ought to be in a .h file under Include/cpython/.
User iritkatriel: > I do think we ought to provide similar functionality to debuggers.

Do you mean to expose the stuff in opcode_metadata.h file through some unstable c api? 
User gvanrossum: Yeah, to the extent that it‚Äôs useful, in function form (so the data structure is not public).

Or we can wait until someone asks.
User iritkatriel: There is a usage of HAS_ARG in Modules/_opcode.c.    We could remove it (send oparg -1 if None was given and do this check inside Python/ code). Otherwise we need to move opcode_metadata.h from Python/ to Include/internal.
User gvanrossum: I would just set it to 0 when it's None or missing. Honestly it's always bothered me that you must know ahead of time whether an arg is needed or not.
---------END---------
53
User barneygale: https://github.com/python/cpython/blob/264a0110ffa4e08b0c7b1023e67a6bd7cb9617c6/Lib/pathlib.py#L692

[Per](https://github.com/python/cpython/pull/104999#pullrequestreview-1468672383) @eryksun:

> Note that this check is wrong for incomplete UNC paths such as "//", "//spam", "//?/UNC/", and "//?/UNC/spam". Such paths lack a root and don't even have an implicit root. However, they're still absolute because they don't depend on the current working directory. I don't know how or whether you want to handle this edge case.
> 
> `ntpath.isabs()` gets this case right, but it's still wrong for rooted paths such as "/spam".
---------END---------
54
User kekekekule: Hi!
I got an issue using Python 3.9.16 and 3.9.17

Launching this code (inspired by #90622)



Causes a deadlock sometimes.

The faulthandler dumps the traceback below on timeout:


The pstree shows the following:


Running under docker launched in QEMU on Mac OS M1:


Tried to analyze under strace and suspect that the problem, here's the last output, after that it simply hangs:


Having child processes with pids 304 and 305, on interrupting my script I got from strace that it hung on wait4 of pid = 305.


304 exits normally after SIGINT, 305 not, still hangs.


Did not check versions upper than 3.9.x. However, I do not exclude that the problem can be reproducible on higher versions of Python.

UPD. Reproduced the same on Python 3.10.10:


UPD2. Just investigated, that when issue happens, one of the children isn't even introduced in strace except wait4.
UPD3. Some logging from multiprocessing:

User kekekekule: @AlexWaygood see you added this issue on a board. However, could you validate please whether this is a real bug that was not fixes by some multiprocessing/threading/concurrent patches in 3.9
User AlexWaygood: Hi, I was just doing some drive-by triaging. I'm a core dev, but I don't primarily work on multiprocessing issues in CPython. I could investigate further, but I'm not a multiprocessing expert, so I'm probably not best placed to do so. I also have a number of other projects on the go at the moment. I added it to the board so that members of the team who _are_ experts on multiprocessing will be able to find it and work on it more easily.

However, please try to see if the problem reproduces on Python 3.11 or later. Python 3.9 and 3.10 are now in "security-only" mode ‚Äî we won't fix bugs in those versions of Python unless they relate to a security vulnerability.
User kekekekule: @AlexWaygood Could reproduce on Python 3.11.2.

So, maybe it's worth labeling the issue for python 3.9, 3.10, 3.11.

User AlexWaygood: > So, maybe it's worth labeling the issue for python 3.9, 3.10, 3.11.

Unless you can persuasively argue it's a security issue, we won't be backporting a fix to 3.10 or older, so I won't add the 3.10 or 3.9 label for now.
User kekekekule: A new thing I noticed: could reproduce this deadlock even on Python 3.8. Just highlighting for easier debugging.

User YvesDup: Hi @kekekekule , I am just wondering if bug is still present when running ProcessPoolExecuter with a mp_context as `spawn` or `forkserver` ?
User kekekekule: Spawn/forkserver -- no.
From my repro it follows that I was using fork. I know spawn/forkserver is safer, but this does not cancel the issue :)
User YvesDup: > Spawn/forkserver -- no

Sorry, I don't understand if you ran your script with other context than 'fork' just to check if there is still something wrong.
---------END---------
55
User sobolevn: # Feature or enhancement

`sre_*` modules like `sre_constants`, `sre_compile`, and `sre_parse` were deprecated in `3.11` in https://github.com/python/cpython/commit/1be3260a90f16aae334d993aecf7b70426f98013

Our regular deprecation policy is that the deprecated things can be removed in N + 2 release, which is `3.13`.

# Pitch

Let's remove them if there are no objections.

I guess it is safe to remove them for several reasons:
1. https://bugs.python.org/issue47152 clearly states that they were undocumented
2. There are now `re._parser`, `re._constants`, and `re._compiler` modules that are used instead
3. They were listed as deprecated in the "what's new" and the warning was pretty clear

The only argument agaist removing them:
1. The deprecation warning never says when they will be removed:



I will send a PR once we settle it:
- Either with a better deprecation message
- Or with these modules removed

@vstinner @serhiy-storchaka what's your opinion?
User hugovk: Do they have much use in the top 5k PyPI packages? 

For example: https://dev.to/hugovk/how-to-search-5000-python-projects-31gk
User sobolevn: It is still used, yes.
But, many results here are from mypy / typeshed / jedi usages, which are safe.
There are some string usages, like in `isort`, `pipreqs`, `ruff`, and in one `pytest` plugin, which are also safe: for example, `pytest` does not recognise `sre_*` modules as stdlib ones.

Notice, that some results are from vendored dependencies that would be quite hard to update.

Full results of `cpython/search_pypi_top.py -q . "sre_(compile|constants|parse)" > results.txt`
[results.txt](https://github.com/python/cpython/files/11679910/results.txt)

User vstinner: It's interesting that exrex already uses ``re._parser``. This project is: "Irregular methods for regular expressions.".



Another project is using the private ``re._constants`` sub-module (people love to abuse the private API instead of asking to make what they need public):



Another example:



pyparsing was using sre_constants, it's no longer the case:



There are vendored copies of pyparsing which still use sre_constants.

coverage test suite explicitly ignores the deprecation, [tests/conftest.py](https://github.com/nedbat/coveragepy/blob/2e09055737aaa7a4c3d61bd1cb700ef528827036/tests/conftest.py#L51):



Similar example:



or:


User vstinner: I would prefer that people don't use private APIs. But well, as soon as it's technically possible, people just continue to do that.

> Do they have much use in the top 5k PyPI packages?

At the least, it's unclear to me if it's ok or not to remove these modules right now.  I would say that it's ok since we respected PEP 387 deprecation period. But @serhiy-storchaka may have a different opinion.
User serhiy-storchaka: Initially I was going to not keep old modules. But it was shown that they are used in some third-party projects, so it would be nicer from us to keep them for a time. Now we can remove them at any time. But it would be even more nicer if we first add public API for things used in third-party code. It is not always feasible, for example `re._constants` is inherently implementation detail. But I was going to add public API for compiling replacement strings.
---------END---------
56
User sobolevn: (or at least I cannot find these tests)

I think that testing the pickle result of functions, classes, and type aliases created with the new syntax is very important.

I propose to add these tests, PR is in the works.


<!-- gh-linked-prs -->
### Linked PRs
* gh-105443
* gh-105845
<!-- /gh-linked-prs -->

User sunmy2019: Sounds reasonable.
User JelleZijlstra: Thanks Nikita!
---------END---------
57
User mdruiter: I [have](https://stackoverflow.com/q/76408051/357313) files with a small header (8 bytes, say `zrxxxxxx`), followed by a gzipped stream of data. Reading such files works fine most of the time. However in very specific cases, seeking backwards fails. This is a simple way to reproduce:

Unfortunately I cannot share my files, but it looks like _any_ similar file will do.

Debugging the situation, I noticed that DecompressReader.seek (in Lib/_compression.py) sometimes rewinds the original _file_, which I suspect causes the issue:


Apparently GzipFile does want to support seeking regular files, even though it isn't fast. I think it can, except that it should rewind the file to the starting position, _not_ 0!
---------END---------
58
User lazka: # Bug report

This is a bit of a long shot, as I'm out of ideas, please stay with me :)

The MSYS2 project has been hunted by random Python crashes over the last year in CI (GitHub and GitLab), in combination with [meson](https://github.com/mesonbuild/meson) and I finally got some time to reproduce/bisect it, still only in CI:

https://github.com/lazka/python-crash-test (see the workflow file there for the setup)

This sets the error mode to SEM_NOGPFAULTERRORBOX and runs ninja which repeatedly calls python, until python crashes.

Python crashes with STATUS_ACCESS_VIOLATION, only on CI, and only if just-in-time debugging is disabled via SEM_NOGPFAULTERRORBOX, which makes it quite hard to pin down/debug.

I've now taken the time to git bisect it via GHA CI and come to the following result:

* The first commit to introduce the crashes is https://github.com/python/cpython/commit/41010184880151d6ae02a226dbacc796e5c90d11 (this was during [v3.10.0a3...v3.10.0a4](https://github.com/python/cpython/compare/v3.10.0a3...v3.10.0a4))
* The first commit to make the crashes go away again is https://github.com/python/cpython/commit/ed57b36c32e521162dbb97199e64a340d3bff827 (this was during [v3.11.0a3...v3.11.0a4](https://github.com/python/cpython/compare/v3.11.0a3...v3.11.0a4))

I'd welcome any ideas on why those commits could have an effect and what I could try to further pin this down.

My initial goal was to revert or backport commits after the bisect, but those commits have so many conflicts and touch so many things that I gave up.

# Your environment

- CPython versions tested on: all versions from 3.10.0a1 up to 3.11.3
- Operating system and architecture: on all of Windows Server 2019/2022 (via GHA) Windows Server 2016 (via GitLab CI)

---------END---------
59
User 1mikegrn: # Feature or enhancement

`match_class` in `ceval.c` should default to match the positional arguments in the `.args` attribute for Exceptions

# Pitch

By default, the BaseException object in Python defines an args attribute which is assigned the tuple of the arguments given to Exception objects at instantiation. 



Matching this object in structural pattern matching (SPM) requires knowledge of this implementation detail and results in the match statement being overly verbose for developers who prefer `return`-ing errors instead of `raise`-ing them.



the `match_class` function in `ceval.c` allows for passing positional arguments to the match for classes. The function uses a `__match_args__` class attribute to compare positional values against attributes of the instance. These attributes are packed into a `PyList` object, and the values are then compared against for SPM.

The proposal here is to, for Exception objects, unpack the `.args` attribute by default into the list of values used for SPM. The result would be a cleaner syntax more aligned with how exception objects are constructed.



A proof-of-concept implementation can be found here:
https://github.com/1mikegrn/cpython

# Previous discussion

https://discuss.python.org/t/add-structural-pattern-matching-to-exception-objects/27389/22
https://www.reddit.com/r/Python/comments/13zq790/add_match_args_to_baseexception/

Originally this idea was framed as adding a `__match_args__ = ("args", )` class attribute to the BaseException class. The downside there is that you are then required to pass a tuple of args to the `case`. After further investigation and discussion, changing `match_class` to unpack the `args` attribute for Exception objects seems the better approach.

The purpose of this change is to better support the notion of errors as values in Python. Keeping errors out of the global error state and in the local stack provides for more granular control flow and explicitness in error handling. Static type checkers like mypy can validate type safety more effectively for errors that are returned instead of raised. It's an approach that has been seen in multiple other languages as a first-class citizen, and major projects in languages with `try/catch` functionality have been known to emulate the approach so as to derive its benefits.
User sobolevn: Type-checkers (at least mypy) need a well definied tuple of `__match_args__` to work correctly. How can this solution work for this use-case?
User 1mikegrn: Presumably static type checkers would need a corresponding update to similarly check by default the types of the `.args` attr on exception objects, since there would be no `__match_args__` to read

As mentioned another approach would be to make a default `__match_args__=("args", )` on the base exception class, but the resulting syntax isn't as elegant, and it doesn't match the objects construction so you still need knowledge of the implementation details


User sobolevn: But, `ValueError` accepts `*args`, so we can do something like this:



User 1mikegrn: presumably you'd also want corresponding type support (not unprecedented, basically treat it to be the same as `tuple[str, int]`)


User brandtbucher: Thanks for taking the time to research and implement this!

While this does seem sort of "cool", I'm not sure this is an idea worth pursuing right now. To me, it feels too much like a special case (one that will probably slow down *all* class matches by a tiny bit). Plus, as you note, it's already possible to match on the value of `args` using the class pattern syntax.

We also haven't pushed pattern matching on exceptions in general, since that's completely different from the built-in exception handling model and encourages people to use the payload of an exception for error handling rather than the class hierarchy. I'm pretty sure that as soon as we introduce this feature, people will use it to start matching on the exact text of error strings, which is not at all what we want.

However, a more general solution for "view patterns" was discussed at the [Language Summit](https://pyfound.blogspot.com/2023/05/the-python-language-summit-2023-pattern.html) this year. If that idea moves forward, it *may* make sense to add `__match__` method to classes (like `BaseException`) that collect their constructor arguments into a single attribute. But we can cross that bridge when we get there.
---------END---------
60
User torokati44: # Feature or enhancement

To resolve the problem in the issue title:

 - Either: Never implicitly prepend a `/` to the name passed as parameter to `SharedMemory` (before passing it on to `shm_open`), only require it from the user to add it themselves.
 - Or: Always prepend it, even on non-POSIX platforms (Windows), where it's not necessary (but doesn't hurt either).

In both cases:
- Document whether the `name` field does or does not contain this implicitly prepended `/`, preferably in a platform-independent way.
- Also consider disallowing more than one leading `/` in the final name, due to #105380.

# Pitch

Since `SharedMemory` is a wrapper for a system-wide resource, it can also be used to communicate with non-Python processes, or even just other Python processes using different wrappers to the underlying system API (such as the `posix_ipc` package).
The current platform-dependent, hidden alteration of the `name` parameter makes this a lot more cumbersome, because correctly accessing the same resource via other wrappers on all platforms requires some care.

# Previous discussion

None that I'm aware of.
User eryksun: Reliable use of POSIX [`shm_open()`](https://pubs.opengroup.org/onlinepubs/9699919799/functions/shm_open.html) requires the name to begin with a slash:

> The `name` argument points to a string naming a shared memory object. It is unspecified whether the name appears in the file system and is visible to other functions that take pathnames as arguments. The `name` argument conforms to the construction rules for a pathname, except that the interpretation of \<slash\> characters other than the leading <slash> character in `name` is implementation-defined, and that the length limits for the `name` argument are implementation-defined and need not be the same as the pathname limits {PATH\_MAX} and {NAME\_MAX}. If `name` begins with the \<slash\> character, then processes calling `shm_open()` with the same value of `name` refer to the same shared memory object, as long as that name has not been removed. If `name` does not begin with the \<slash\> character, the effect is implementation-defined.

Prepending a forward slash to the name on Windows would break using session explicit names of the form "Local\\{name}" (i.e. the current session), "Global\\{name}" (i.e. session 0), and "Session\\{session number}\\{name}".

User torokati44: > Reliable use of POSIX [shm_open()](https://pubs.opengroup.org/onlinepubs/9699919799/functions/shm_open.html) requires the name to begin with a slash:

Yes, I was aware of this, which is why I'm not proposing the slash never to be prepended.

> Prepending a forward slash to the name on Windows would break using session explicit names of the form "Local\{name}" (i.e. the current session), "Global\{name}" (i.e. session 0), and "Session\{session number}\{name}".

However, I wasn't aware of these... :cry:
Then I have no real solution to propose I'm afraid.

User torokati44: Wait, actually, what am I talking about... The first option of never modifying the name (only throwing if it doesn't start with `/` on POSIX) is still viable IMHO.
Then also `name` and `_name` would always be equal.
---------END---------
61
User torokati44: # Bug report

Shared memory names act as "file paths" on Linux, therefore no matter how many `/` characters they start with, they count as only one to the system. But `resource_tracker` doesn't account for this.

See this repro involving two interpreters running at the same time:

| Process 1 | Process 2 | `/dev/shm/abc` exists |
|-----------|-----------|-----------------------|
| `from multiprocessing.shared_memory import SharedMemory` | `from multiprocessing.shared_memory import SharedMemory` | no |
| `shm = SharedMemory("abc", True, 10)` | | yes |
| | `shm = SharedMemory("/abc", False)` | yes |
| `shm.buf[0] = 42` | | yes |
| | `assert(shm.buf[0] == 42)` | yes |
| `shm.unlink()` | | no |
| `assert(shm.buf[0] == 42)` | `assert(shm.buf[0] == 42)` | no |
| `exit()` | | no |
| | `shm.unlink()` # raises exception | no |
| | `exit()` # prints warning | no |

At this point, calling `shm.unlink()` in Process 2 throws an exception, which may be unexpected.

And regardless if it is called or not, Process 2 prints `UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown` at exit, even though nothing got leaked.

Note that `/abc` could also have been `//abc`, `///abc`, and so on. On macOS and Windows however, all of these names belong to distinct objects.

# Your environment

- CPython versions tested on: 3.11.3
- Operating system and architecture: Fedora Linux 38, x86_64
---------END---------
62
User vstinner: There are multiple Python functions marked as "deprecated", usually only in the documentation, or sometimes in comments, whereas the feature is there for a long time and there is no *need* to remove them, or the rationale changed.

For example, the ``array.array('u')`` format was deprecated since it used the ``Py_UNICODE`` C type (which is deprecated), but the code was updated to use the ``wchar_t`` C type which is fine and not depreated.

Another example is passing *NULL* as *value* to ``PyObject_SetAttr(obj, name, value)`` to delete an object attribute is deprecated... but this feature exists since the birth of Python. I don't think that it's really useful to deprecate this behavior: it's fine to keep it. The ``PyObject_DelAttr()`` function is just implemented as a macro passing *NULL*:



<!-- gh-linked-prs -->
### Linked PRs
* gh-105374
* gh-105394
* gh-105395
<!-- /gh-linked-prs -->

User hugovk: See also https://github.com/python/cpython/issues/80480 which links to a merged PR to add `array.array('w')` and with an open PR to deprecate `array.array('u')`, cc @methane.
User vstinner: I wanted to propose the keep the deprecated logging.Logger.warn() method (remove its deprecation)... but instead I proposed to remove it :-) see issue #105376.
User arhadthedev: > For example, the `array.array('u')` format was deprecated since it used the `Py_UNICODE` C type (which is deprecated), but the code was updated to use the `wchar_t` C type which is fine and not depreated.

- gh-95760 was merged few hours ago. Is the cited point relevant then?
User methane: See https://github.com/python/cpython/issues/80480#issuecomment-1541327809

There are few situations where platform- or compiler-dependent `wchar_t` arrays are useful.
Many users use `array('u')` as a mutable Unicode buffer, and `array('w')` is recommended for that use.
---------END---------
63
User serhiy-storchaka: Boolean expressions produce larger bytecode and are slower in 3.12 than in 3.11.

In 3.11:

In 3.12:


In 3.11:

In 3.12:

Furthermore, adding parenthesis affects the generated code in 3.12, unlike to 3.11. Peephole optimization does not work anymore.

User CCLDArjun: Looks like 3468c768ce5e467799758ec70b840da08c3c1da9 introduced the regression
User CCLDArjun: > Furthermore, adding parenthesis affects the generated code in 3.12, unlike to 3.11. Peephole optimization does not work anymore

I don't think there can be a peephole optimization here since `JUMP_IF_FALSE_OR_POP`and `JUMP_IF_TRUE_OR_POP`aren't there anymore
User markshannon: Removing `JUMP_IF_FALSE_OR_POP` was quite a useful simplification, as it helps branch optimization and wasn't much used.

Is this a performance problem in real code?

We could improve the version using local variables, as `LOAD_FAST` is side-effect free, so we can change:

to

But it's not a priority, as doesn't seem to make much difference in practice.

---------END---------
64
User jaraco: In Python 3.11, inside a comprehension, `locals()` was bound to the scope of the comprehension:



But since 3.12b1, `locals()` is `globals()`:



May be related to #105256.

Discovered in [this job](https://github.com/jaraco/jaraco.clipboard/actions/runs/5107203578/jobs/9179898494), triggered by [this code](https://github.com/jaraco/jaraco.clipboard/blob/e81d86fb98c7e0d42fb160960cdd696b07b88952/conftest.py#L5-L9) (which I intend to update not to use `locals()`).

Is this change intentional?

<!-- gh-linked-prs -->
### Linked PRs
* gh-105715
<!-- /gh-linked-prs -->

User JelleZijlstra: Yes, this is intentional and called out in PEP-709. cc @carljm
User sunmy2019: 

With PEP-709, list comprehensions do not have their own stack. This affects top-level sentences.

User jaraco: Thanks for the reference. I do think the current behavior violates the PEP, which indicates that "locals() includes outer variables", suggesting that locals() would continue to include inner variables, and confirmed by the example where "under this PEP", `x` is found in locals().

In my example above, `name` does not appear in `locals()`, which is a departure from the prior behavior and of the PEP.
User sunmy2019: The correct thing seems to be something like


User Yhg1s: `name` doesn't appear in locals() at the global level because the generated bytecode uses fast locals for the name even at the global scope:


This seems to me to be more correct than the alternative (replacing the existing name for the duration of the list comprehension) in particular because globals are visible to outside code. Is there a practical problem with not including the loop variable in the locals() at the global level?
User jaraco: > Is there a practical problem with not including the loop variable in the locals() at the global level?

I'm not sure I understand the question or if it was even directed at me.

The problem I see is that it's no longer possible to reference `locals()['name']` inside the loop.



Any code that depends on locals actually representing the local variables will fail. I linked to the code that started failing as a result of this change... but as I mentioned, that code used a legacy construct that's superseded by f-strings. I'm not yet aware of any other practical uses. In general, this change feels like a breaking change deserving of a deprecation period.
User JelleZijlstra: I do agree that this is a behavior change that wasn't called out in PEP 709 and that we didn't realize would be an issue before. PEP 709 says that locals() now includes the variables from the outer frame. That's true in functions, where locals() now includes both outer and comprehension-local variables, but in module and class scopes, it now includes *only* the outer variables.



And that can indeed break code like the code @jaraco linked above.

To fix it, I wonder if inside the implementation of `locals()` we can detect that we're within a class- or module-scoped comprehension, and if so, add the comprehension locals to the returned dictionary.
User carljm: Yes, the lack of `name` key in `locals()` in the given example is a problem. Thanks @jaraco for the report.

I'm traveling this week but will be able to look into possible fixes here more closely next week. It's tricky because we can't actually add the key to the module globals (or class "locals") dictionary without potentially stomping on an existing outer-scoped variable, which would be much worse. There may be potential band-aids where `locals()` returns a different dictionary within a comprehension, with the extra key added. The long term solution to this is something like PEP 667, which would allow fixing this much more cleanly.
---------END---------
65
User Guy-Markman: # Feature or enhancement

This is a discussion about a feature I have, before I try to implement it.

I would be glad if there was an option to overwrite old files when extracting new files with the same name at the same location.

It can be a new parameter, which will be defaulted to False, which if true, will check if there is a file / directory with the same path. If it doesn't have, it will delete the existing file /directory before writing the new ones.

# Pitch

I'm writing a program which is extracting a archive to a location the user want me to extract it.
When extracting with CLI tools, if there is a file that will have the same path as existing file, I will get a prompt which will ask me if I want to replace the old file with the new one.

But I'm Python I have to check if there is a file with the same path of the file I'm going to extract, otherwise I will get a file exists exception.

Why not make it built in? 

Also, by adding it as an optional Parameter, we won't break backwards compatibility.

# Previous discussion

<!--
  New features to Python should first be discussed elsewhere before creating issues on GitHub,
  for example in the "ideas" category (https://discuss.python.org/c/ideas/6) of discuss.python.org,
  or the python-ideas mailing list (https://mail.python.org/mailman3/lists/python-ideas.python.org/).
  Use this space to post links to the places where you have already discussed this feature proposal:
-->

https://discuss.python.org/t/overwrite-existing-files-when-extracting-a-file/27311

<!--
You can freely edit this text. Remove any lines you believe are unnecessary.
-->

User sunmy2019: What exact function do you want to modify?
User Guy-Markman: > What exact function do you want to modify?

In ZipFile: extract, extractall and _extract_member

In TarFile: extractall, extract, _extract_one and_extract_member 
---------END---------
66
User guillaumematheron: # Bug report

Minimal reproductible example:



Expected output: Either all UUIDs are different, or u.is_safe is not `safe`

Actual output:


If the initial call to uuid1() is omitted, or if using spawn instead of fork, then the bug is not triggered.

By following the code, the underlying function called seems to be [uuid_generate_time_safe](https://github.com/util-linux/util-linux/blob/master/libuuid/src/gen_uuid.c). I was not able to reproduce the bug outside of python by calling the C method directly, so I assume python sets a global state the first time `uuid1()` is called, and this state is copied when the fork occurs.

This issue was originally discovered in [clickhouse-connect](https://github.com/ClickHouse/clickhouse-connect/issues/194) then discussed in the context of [multiprocessing](https://github.com/python/cpython/issues/84559).


# Documentation issue

This may require a separate issue, but the documentation of `uuid1()` states:



This seems to be false on my platform since only a counter seems to be incremented between two calls to `uuid1()`.


# Environment

- Python 3.10.6
- Ubuntu 22.04 LTS 5.15.0-58-generic x86_64
---------END---------
67
User mkdruss: [input.txt](https://github.com/python/cpython/files/11655564/input.txt)



**Actual result:**
diff -Nau input.txt output.txt 
--- input.txt	2023-06-05 20:24:52.403660457 +0200
+++ output.txt	2023-06-05 20:25:44.562612636 +0200
@@ -29,6 +29,7 @@
 </html>
 
 --------------l1sFYP2m750zRB2cqq59goTA--
+
 --------------tuwh9BRnBMPVHY5fHxB3sZQ1
 Content-Type: image/jpeg; name="t.jpg"
 Content-Disposition: attachment; filename="t.jpg"

**Expected result:**
zero return code of `diff input.txt output.txt`

input is simple html message with attachment from thunderbird
This behavior is unpleasantly if you want dkim.sign(m.as_bytes(),.....) , this produce invalid DKIM signature.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105485
<!-- /gh-linked-prs -->

User gaogaotiantian: The direct reason for this is Python always add a new line after the closing boundary(which is not required by RFC1341 from what I read) and a new line before the encapsulation boundary(which is required).

As the code parsed the email content, it was reconstructured and output with Python formatting, which ended up with an extra blank line between the two delimiters.

I agree this is not the ideal behavior, but this is probably not a bug because I found this:

https://github.com/python/cpython/blob/f04c16875b649e2c2b420eb146308d0206c7e527/Lib/email/generator.py#L315C5-L318



This indicates that `email` module does not guarantee to preserve the format of the raw content.

This would be a feature request which is nice to have, but the it's not a trivial fix based on the current code - it would probably break some backward compatibilities on the way there.

User p-m: Furthermore, trailing space after a semicolon is removed.
Please see here for further information:
https://lists.mailman3.org/archives/list/mailman-users@mailman3.org/thread/4EYRCOV576SW4N5LVJSH2QGZLQXJ7PDL/

I would really like to see a fix, because the current behaviour breaks DKIM signatures and probably other types of signatures for the body.

TIA and kind regards, Peter
User gaogaotiantian: It looks like this was intentionally implemented this way: https://bugs.python.org/issue14983

It was discussed thoroughly and the implementation has been like this for 10 years. The original consideration was also about signing. As it was already discussed and it's been working since then, I don't think this will be changed. Maybe thunderbird has different implementations than other email clients?
User p-m: > It looks like this was intentionally implemented

That is about "email.generator". Here, the message is already generated.

There should be at least an operation mode like "do not modify any single byte of the original message".
Broken signatures are unexpected behaviour.
User gaogaotiantian: The message is reconstructed(as Python objects) when you do `email.message_from_bytes()`. It shares code path when serializing(dumping to bytes/string) with creating messages with fields. If you want "do not modify any single byte of the original message", maybe just don't parse it?

There are two problems mentioned in this issue:
1. The extra blank line after ending delimiter - this will NOT be fixed. This is intentional as I mentioned above, please read through the discussion, it's for signing as well and it has been working since then. Changing that behavior could potentially break other clients.
2. The missing space after `;`. This could probably be fixed, I have a patch that has a very minimal impact on existing code and I'll submit it soon.
User p-m: > maybe just don't parse it?

I guess, that this is not possible in mailman, or any other application, that uses this module just for modifying (or adding) particular headers of an existing message.

> 1. The extra blank line after ending delimiter - this will NOT be fixed.

IMHO there should be at least an option, to keep the body unmodified.

> it has been working since then.

It depends on the use cases. It works for generating new messages, but not for forwarding messages (with minor modifications), that have been received from elsewhere. In the past, there was probably no problem, but nowadays the spam filters are more and more aggressive and block messages with invalid DKIM more often.

> Changing that behavior could potentially break other clients.

Ok. Then, what do you think about a simple option, like "keep-body-unmodified"?

>  2. The missing space after `;`. This could probably be fixed, I have a patch that has a very minimal impact on existing code and I'll submit it soon.

Thanks!
User gaogaotiantian: If you take a look at the parsing code, you'll realize it's a gigantic monster. You can't achieve "keep-body-unmodified". Like I said, when you parse the raw string(bytes) to an email object, you build a Python structure, and all the raw data is discarded. When you do ``as_bytes()``, you reconstruct the string(bytes) based on the Message object. There is just no path to "keep the body".

Also, the difference for 1. is on the ending delimiter, which is considered as structual skeleton, not actual data. When you parse the raw string, there's no space for that blank line - it was interpreted as Python object structures.

If you want to give it a try, go for it. If your implementation does not break the existing use cases, it would be accepted. But maybe you'll realize it's not feasible when you start working on it.
User p-m: On Wed, Jun 07 2023, Tian Gao wrote:

> you'll realize it's a gigantic monster.

Ok... :(


> You can't achieve "keep-body-unmodified".

I thought of something like a function (for example
"message_from_bytes_b", that parses the headers and puts the whole body
in a single array without further parsing.


> If you want to give it a try, go for it.

I can't do it myself, but I could offer a bounty for it. Where would be
a good place for such an offer please?

TIA,
-- 
           Peter

User p-m: Gmail does not send an extra blank line between the two delimiters.
Does that mean, that Gmail does not respect rfc2046#section-5.1.1?
And that Google should fix that?
User gaogaotiantian: Unfortunately I'm not an email expert. From Python's side - not adding the extra blank line is easy - it is an intentional feature which was added in https://bugs.python.org/issue14983 and can be easily reverted.

I'm not in the position to say whether Google should fix their email clients behavior, but the current behavior (which is deliberate) of Python has been there for > 10 years, and changing it back has a very high risk to break backward-compatibility.

If the original commiter @bitdancer could share some thoughts on this, that would be great. But David is probably not super active on CPython repo now.

For this feature specifically, it's an A or B choice. If the current behavior is "bad", there would be a lot of complaints. So, I think at least in many cases, the current behavior works. At least for Evolution and KMail, which was mentioned in the original issue 10 years ago, this is the favored behavior.

It's possible that after 10 years, things have changed and (most of) the modern clients changed their format which Python should follow, but I'm afraid you need to either prove it, or have a field expert in Python team to approve it.

For your information, I'm not even in a position to make this decision - I'm not a core dev. For this kind of backward incompatible change, you'll need some support from core dev team. (As I mentioned above, having a switching option is almost infeasible, but changing the behavior is trivial)

I'm not familiar with the core dev team enough to know who is the expert in the field, we can see if anyone is interested in this.
User p-m: On Thu, Jun 08 2023, Tian Gao wrote:

> not adding the extra blank line is easy

That is not really the question (blank line or not). It's rather: How to
generate the same as the original message?

Perhaps it could be possible, to let "message_from_bytes" store some
flag in the message object, that permits the generator to produce the
same output later on?


> For this feature specifically, it's an A or B choice.

IMHO the current behaviour is correct for newly created messages.
And for messages read in by message_from_bytes, A/B should depend on a
flag (or similar) stored in the message object.


> For this kind of backward incompatible change,

The suggested change would not be backward incompatible.


> having a switching option is almost infeasible

I can hardly believe that. I guess, that it's more a question of "how
much effort".

-- 
           Peter

User gaogaotiantian: > That is not really the question (blank line or not). It's rather: How to
generate the same as the original message?
> Perhaps it could be possible, to let "message_from_bytes" store some
flag in the message object, that permits the generator to produce the
same output later on?

If you want the exact same message, why do you need to parse it? Just store it as bytes and use that later. If you can change the code calling the function, you can do that. The problem to store and output the same bytes in `Message` object is - what if the user changes it through the APIs? What would you output? That would simply make `Message` object a holder for bytes - which is what normal bytes do.

> I can hardly believe that. I guess, that it's more a question of "how
much effort".

That's why I said "almost", and why I used "infeasible" instead of "impossible". Yes you can rewrite the whole email module and make it work, that's not "feasible".

Sometimes features are easier to imagine than implement, that's why PRs are always welcome - the Python maintainers may be used to a specific pattern and ignoring some smart implementations.

The reason I came up with my conclusion is that I've already spent hours into this trying to figure out a way to make it work, but failed. As of now, none of the core devs have shown interest in this issue, that's why I'm a bit pessimistic about this being addressed.

I'm not saying your request is invalid or wrong, but Python devs do have to make tradeoffs and set priorities because they have limited time and the time is a very important resource.
User p-m: On Thu, Jun 08 2023, Tian Gao wrote:

> If you want the exact same message, why do you need to parse it?

I guess, that mailman needs it for modifying (or adding) particular
headers of an existing message.


> they have limited time and the time is a very important resource.

I understand that very well. That's why I suggested a bounty.

-- 
           Peter

User gaogaotiantian: If you need to modify/add particular headers, that means you can't output the "exact" message. The code needs to read in and parse the full message and reconstruct the message after the modification. During this process, the information of whether an extra blank line after ending delimiter would be missing(at least on the current implementation).

Maybe someone would do this for the bounty, it would be a pretty difficult job and require a very thorough review. Not sure where you would find the best person for the job though.
---------END---------
68
User corona10: At macOS 12.5.1 with Apple clang version 14.0.0 (clang-1400.0.29.202)

`

<!-- gh-linked-prs -->
### Linked PRs
* gh-106284
<!-- /gh-linked-prs -->

User corona10: Header from apple Xcode.

`
---------END---------
69
User darrikonn: ## Actual
`info.context.data_loader["organization"]` yields `MagicMock`

## Expected
`info.context.data_loader["organization"]` yields `AsyncMock`

## Test case


User tirkarthi: This is an intentional behavior where child mock is MagicMock/Mock if the method accessed is synchronous in this case `__getitem__`

Relevant commit : 3667e1ee6c90e6d3b6a745cd590ece87118f81ad

https://github.com/python/cpython/blob/d3af83b9342457d8b24476baeb799f7506ff04f3/Lib/unittest/mock.py#L1074-L1080
User darrikonn: Mmm interesting, thanks for clarifying.
I'll need to use a custom AsyncMock class for my use-case.
---------END---------
70
User kunom: This is a western Europe Windows 11 machine:



As you can see, there is codepage confusion. You don't get back what you wrote out.

Windows has different codepage settings applied, depending on context. File encoding (also called _ANSI codepage_) is not necessarily identical with console encoding (also called _OEM codepage_), see https://stackoverflow.com/a/43194047. The OEM codepage contains legacy graphical symbols like "‚ï£" or "‚ñí".

On my machine:


The character "√∂" has codepoint 0x94 in [CP850](https://de.wikipedia.org/wiki/Codepage_850) (see table there). In [CP1252](https://de.wikipedia.org/wiki/Windows-1252), this codepoint maps to "‚Äù".

The suggestion here is that `subprocess` related things should not pass the choice of the default encoding to `io.TextWrapper` (which is documented to take `locale.getencoding()`), but should instead default to the value returned by `GetConsoleCP()`. 

This would be exactly the same as [the GO people decided to do](https://go-review.googlesource.com/c/go/+/27575).

User terryjreedy: I get the same in main with US machine.
User eryksun: If Python is attached to a console session, the console's current input code page is `os.device_encoding(0)`, and the current output code page is `os.device_encoding(1)`. 

The CMD shell's internal commands such as `echo` use the current output code page. For example:



Alternatively, since the console defaults to using the system OEM code page, you can use `encoding='oem'` if you haven't otherwise changed it via the console API, "chcp.com", or the "CodePage" setting of a named console session under the registry key "HKCU\\Console\\\<title\>". 

---

There is no universal I/O encoding or API query. For example, "sort.exe" use the process OEM code page instead of the console output code page. In the following example, I've set the console code pages to 850, and I chose the character "¬¢" because it's encoded differently in each of the code pages: 437 (process OEM), 1252 (process ANSI), and 850.



For another example, "attrib.exe" uses the process ANSI code page.



The list of mutually inconsistent examples could go on. There is no standard. Common choices are the process ANSI code page, process OEM code page, UTF-16, UTF-8, or the current input code page or current output code page of a console session.

The current console code page in general has nothing to do with the user locale (e.g. day/month names, number/currency symbols) or the user's preferred UI language (text resources, messages). It's a bad choice for the locale encoding, unless it's UTF-8. The best choice in general is the ANSI code page of the user locale, unless the process ANSI code page is UTF-8. Next best is the process ANSI code page, which is normally based on the system locale and commonly matches the user locale. Python uses the process ANSI code page as the locale encoding, unless it's overridden by UTF-8 mode.

---

> This would be exactly the same as [the GO people decided to do](https://go-review.googlesource.com/c/go/+/27575).

That revision changed their `readConsole()` function, which reads encoded bytes from console files via `ReadFile()` and then decodes to text using the console input code page from `GetConsoleCP()`.

Since 3.6, Python's I/O stack uses the `io._WindowsConsoleIO` class that's based on the wide-character console API functions `ReadConsoleW()` and `WriteConsoleW()`. So console code pages aren't directly relevant to Python, except for the low-level `os.read()` and `os.write()` functions.

User kunom: This is great detail and information about the behaviour of Windows. Thanks, @eryksun ! 

Still, is the expectation that any user of `subprocess.run(shell=True, ...)` knows of that? They are just calling the default command interpreter triggered by `shell=True`. For that one, shouldn't we know how it behaves, and shouldn't Python be able to adapt to that?

So, while I agree with you that text encoding clearly needs to be configurable because there is not clear standard, I get back to my initial impression that the default is wrongly chosen.

FWIW (probably not too much): the OEM codepage is also used by any dotnet executable (no need to include it's source code here):


---------END---------
71
User sunmy2019: Currently, there are some tests with intentional leaks or some other tests that do not support rerun.

Adding this decorator will help to resolve this edge case.

inspired by https://github.com/python/cpython/pull/105122

<!-- gh-linked-prs -->
### Linked PRs
* gh-105311
<!-- /gh-linked-prs -->

User Eclips4: Interesting. Can be #105273 resolved if we decorate `test_capi/test_misc/TestOptimizerAPI.test_counter_optimizer`?
User erlend-aasland: After thinking about this for some weeks, I'm -1 on the idea. I don't think we should make it easier to skip reruns. Unfortunately, adding machinery to `test.support` will exactly do that; hence my -1.

I'm sorry to have urged you to open this issue in the first place. However, I changed my mind.
User sunmy2019: That's fine. I think we can leave this issue open and close the PR.
User erlend-aasland: Let's not encourage making it easier to skip reruns; let's close the issue.
---------END---------
72
User grapland0: 
Expected: abc.bz2 contains compressed data from subprocess.
Actual: abc.bz2 contains **uncompressed** data from subprocess.

cpython 3.11.3, linux 5.4.0-1109-azure

User pandaninjas: This seems to be because subprocess gets the file descriptor of the file (https://github.com/python/cpython/blob/main/Lib/subprocess.py#L1710-L1723), which would bypass the processing that bz2.open does.
---------END---------
73
User Die4Ever: # Feature or enhancement

configparser doesn't support multiple entries with the same name (acting like an array), which meant I had to use regex instead.

# Pitch

For example, the game Deus Ex uses ini files like this



https://github.com/Die4Ever/deus-ex-randomizer/blob/develop/installer/Configs/DXRandoDefault.ini#L42-L58

User terryjreedy: Closing because : 
1. No prior discussion.  The new feature template suggests: " New features to Python should first be discussed elsewhere before creating issues on GitHub,  for example in the "ideas" category (https://discuss.python.org/c/ideas/6) of discuss.python.org.
2. No specification.  There is a statement, but no specific proposal to implement.
3. I believe the implied proposal is outside the current scope of the module, to provide "a basic configuration language which provides a structure similar to what‚Äôs found in Microsoft Windows INI files. "  https://docs.python.org/3/library/configparser.html#module-configparser  The doc goes on to note that the extended syntax used in Windows Registry is not supported and to list alternatives that support structured data.
---------END---------
74
User grantramsay: When initialising an SSLContext there is a call to `SSL_CTX_set_session_id_context()`:

    #define SID_CTX "Python"
        SSL_CTX_set_session_id_context(self->ctx, (const unsigned char *) SID_CTX,
                                       sizeof(SID_CTX));
    #undef SID_CTX

The openssl man pages state that `SSL_CTX_set_session_id_context` is a "server side only" operation:
https://www.openssl.org/docs/man1.0.2/man3/SSL_CTX_set_session_id_context.html

> SSL_CTX_set_session_id_context, SSL_set_session_id_context - set context within which session can be reused (server side only)

> The session id context becomes part of the session. The session id context is set by the SSL/TLS server. The SSL_CTX_set_session_id_context() and SSL_set_session_id_context() functions are therefore only useful on the server side.

In some circumstances, calling this on a client side socket can result in unexpected behavior. For example TLSv1.3 PSK: https://github.com/python/cpython/pull/103181#issuecomment-1493611344

The fix for this was originally part of another PR (https://github.com/python/cpython/pull/103181) @gpshead recommended creating a separate issue/PR

<!-- gh-linked-prs -->
### Linked PRs
* gh-105295
<!-- /gh-linked-prs -->

---------END---------
75
User hauntsaninja: I think it would be useful to link directly to https://peps.python.org/pep-0632/#migration-advice in 3.12's What's New, since that section has more specific advice.
---------END---------
76
User keuin: `asyncio.wait`accepts a `Iterable[Awaitable[_T]]` as its first parameter. However it will hang forever if we passes an invalid `Awaitable` to it and there are subprocesses wrapped in `asyncio.Task` running.

To sum up, we have to meet these conditions at once:

1. There are tasks for subprocesses managed by `asyncio.subprocess`. It does not matter if we `asyncio.wait` for it or not.
2. The program attempts to perform `asyncio.wait` on iterators that can yield non-awaitable objects (such as integers).

Here is a litmus program that reproduces this problem:



When running it on Linux, this program is expected to terminate immediately after throwing an exception. Because we are attempting to wait for `0`, which does not implement `Awaitable[_T]`. However the program will barely output:



and hangs forever. Hitting `Ctrl-C` does terminate it, but the stack trace points to somewhere far away from the real cause:



It does not matter if we wait for `[0, t1]`, or `[0, t2]`. As long as `t2` (task of the subprocess) is created, the program will hang when awaiting on that list.

If we run it on previous versions of Python (such as the latest branch `3.10`, `2c9b0f30`), it will cause an exception thrown in the standard library, though the exact behavior seems to be undefined, since I have observed many different locations that throws some kind of exceptions like `AttributeError`  in different locations, when bisecting using git. The correct output looks like:



My test environment is Intel 12th gen core with the latest Archlinux & zen kernel (`6.3.5-zen1-1-zen`). I have rerun the test on an updated Debian 11 VM and got the same result. So I don't think it is related to distros and kernel versions.

Bisecting shows this issue was directly introduced in commit `7015e137`: `gh-88050: Fix asyncio subprocess to kill process cleanly when process is blocked (#32073)`, which attempted to fix a previous `asyncio` issue. After investigating the source code, I believe this was caused by this commit, which has two major problems:

1. It doesn't validate the object before accessing its attribute `pipe`. If we look at method `_try_finish`, we find that method has validated the same objects, so this should be fixed. This causes an unrelated exception and interrupts the clean-up process `_process_exited`. However this piece of code has been removed in a subsequent commit. So it is not relevant today.
2. It doesn't wake up waiters in `self._exit_waiters` after subprocess exited. This directly causes the program hang forever.

I will post my fix and detailed explanation in a PR shortly. Please let me know if there are problems.



<!-- gh-linked-prs -->
### Linked PRs
* gh-105289
<!-- /gh-linked-prs -->

User kumaraditya303: Why are you passing a non awaitable object in the first place?
User keuin: > Why are you passing a non awaitable object in the first place?

As you can see, that's a misuse. It reveals two problems:

1. The implementation is not exception safe. When processing invalid input, those waiters waiting for a background subprocess are not triggered correctly, which leads to an inconsistent state.
2. As a standard library, the implementation should fail immediately against an invalid input, or define what should happen, rather than breaking previous behaviors.
User kumaraditya303: The issue here isn't because of `asyncio.wait` but rather a background fire and forget task which hangs if not cancelled correctly. Simplified code:


As seen  in the code adding a sleep(0) which is essentially cycle event loop once before tearing it apart fixes it.
User keuin: > As seen in the code adding a sleep(0) which is essentially cycle event loop once before tearing it apart fixes it.

Thanks for your detailed explanation. But I think we are a little off-topic. Do you mean that I should "fix" the litmus program rather than fix `asyncio`? I don't think that makes sense because the litmus program is to show the difference behavior of `asyncio`, which I think is a bug of the cleaning-up process. The litmus program is not to be used or debugged. Its purpose is to show the internal state of the runtime.

Let go back to the track by printing the lifetime of the litmus program and compare before/after the commit `7015e137`: `gh-88050: Fix asyncio subprocess to kill process cleanly when process is blocked (#32073)` has been made.

Let's a line of code `print('Handle:', handle, file=sys.stderr)` in `base_events.py:1937`, just at the line after `handle = self._ready.popleft()`:



And add another line `print('Waking up exit waiters...', file=sys.stderr)`, in file `base_subprocess.py`, just one line after the comment `# wake up futures waiting for wait()`, which looks like:



To synchronize all outputs, we also add `file=sys.stderr` in all `print` calls from the litmus program.

# Current behavior

When running without the PR, it prints:



It's clear that the main program hangs, while the subprocess has exited.

# The last "good" revision, before `7015e137`

Now let's do the same experiment on commit `0e72606d`, which is the last revision on which the litmus program won't hang:



The program terminates successfully without any interference. It's also clear that those waiters has been woke up after the process exits. So I think the real problem is: previous fix `7015e137`  breaks this cleanup process. And we have to fix that.

# The fix made by myself (PR)

After applying the fix PR, it prints:



Now the litmus program behaves exactly the same before commit `7015e137`. I think we should treat it seriously since Python is such a popular language. This flaw could also be triggered in other places, though I have not found out.

BTW, the CI is failing because I am experimenting on how to write a test case that works as expected in my branch. The test works on my Linux machine, but I doesn't have Windows or MacOS testing environments, so I have to use the public CI. The frozen event loop makes testing much harder, and I am still finding out how to mark those exceptions as expected.

Please consider twice before closing that PR. Or maybe we need another developer to review that?

@kumaraditya303 @gvanrossum
User kumaraditya303: > Please consider twice before closing that PR. Or maybe we need another developer to review that?

I have reopened the PR (I meant to request changes but somehow it got closed and I didn't look again sorry for that ) though I wasn't referring to the test failure added by your test but rather other tests. I referred to the fix as wrong as it basically reverts my fix.  That change isn't straight forward to understand, see my explanation of it in https://github.com/python/cpython/issues/88050#issuecomment-1268011757
User kumaraditya303: For your reference, the test failure I was referring to were from https://github.com/python/cpython/actions/runs/5212830335/jobs/9406983274, see the warnings in other tests not added by your PR.
---------END---------
77
User dougmccasland: Python  3.10.6
module email ‚Äî An email and MIME handling package v3.11.3

Consider this simple message:

Notice the o-umlaut in the word hotel, this is encoded in utf8.  I put this in a file called msg.eml.  Then run this:

The output:

I expect the output to have valid utf8 since the CTE is 8bit.  This problem also hhappens with the older get_payload() and with any of the "_from" methods, such as email.message_from_bytes().  




<!-- gh-linked-prs -->
### Linked PRs
* gh-105306
<!-- /gh-linked-prs -->

User michaelfm1211: I tested this on the current `main` branch and got the same thing. However, I did not got an issue when using `msg.get_payload()`.

The `.get_content()` method is determined by `msg`'s policy's content manager. For the default policy, that's [`email.content_manager.raw_data_manager`](https://github.com/python/cpython/blob/3.11/Lib/email/contentmanager.py#LL61C1-L61C17). For MIME types starting with `text`, `raw_data_manager` uses [`email.content_manager.get_text_content`](https://github.com/python/cpython/blob/3.11/Lib/email/contentmanager.py#L64). `get_text_content()` calls `msg.get_payload(decode=True)` (which decodes the payload according to the CTE header), then decodes the result with the email's charset. Here's the source for `get_text_content()` ([link](https://github.com/python/cpython/blob/3.11/Lib/email/contentmanager.py#L64-L68)):


I don't know too much about Unicode, but I think the issue is that if `decode=True` and the payload is a string (which it is in your case), then `msg.get_payload()` will first try to encode the payload with `'ascii'`, then fall back to `raw-unicode-escape`. Of course, the umlaut cannot be encoded with ASCII, so `msg.get_payload()` returns the payload encoded with `raw-unicode-escape`. Here's the source code for where that happens ([link](https://github.com/python/cpython/blob/3.11/Lib/email/message.py#L301-L308)): 


So when `msg.get_payload(decode=True)` returns something encoded with `'raw-unicode-escape'`, then `get_text_content` tries to decode it with a charset that's not `'raw-unicode-escape'` (UTF-8 in your case), you get a `UnicodeDecodeError`, which becomes those weird symbols because `errors='replace'`.

To fix this, I think `msg.get_payload(decode=True)` should first try encoding with UTF-8 if the CTE is `8bit`, then fall back to ASCII for any other CTE. Then if those fail it can fallback to raw Unicode escape.
User dougmccasland: Very interesting.

Quite right, get_payload() (with no args) with the above input works.  With get_payload(), I had previously thought of using the errors='replace' kw arg because some incoming messages had (apparently) malformed utf8 causing the python script to terminate early.  

I forget the sequence, but I later came across some input that contained a 3-byte utf8 character ‚Äô (Right Single Quotation Mark, U+2019); the sender's software incorrectly used that in the word "I'm" instead of a simple ASCII apostrophe.   With errors=replace, I was now seeing \u2019 in the output (see below) -- I suppose that is raw-unicode-escape ?  (The umlaut is 2 bytes.)  I thought I could prevent such problems by going to newer get_content(), and the coding is simpler.  Plus the documentation says:

https://docs.python.org/3/library/email.compat32-message.html?highlight=get_payload#email.message.Message.get_payload
This is a legacy method. On the EmailMessage class its functionality is replaced by get_content() and iter_parts().  

When I added that \ui2019 character to the input, here is what I got, with various methods:


So I will try reverting to plain get_payload() and see how that works with future real-world email messages.  Of course, there are countless ways an email message can be badly encoded by the MUA or by text that is pasted into the MUA from another program.  (I've sent my share of malformed messages. :-)  )

Also interesting:
https://docs.python.org/3/whatsnew/3.2.html?highlight=get_payload
Given bytes input to the model, get_payload() will by default decode a message body that has a Content-Transfer-Encoding of 8bit using the charset specified in the MIME headers and return the resulting string.

I like your idea about how to fix it, although I don't know how to do it.

User michaelfm1211: Yep, it looks like that odd behavior with the right single quotation mark character is being caused by encoding with raw-unicode-escape. Just to confirm I ran it through `.encode()` manually and got the same result as you:

Regarding the documentation's note on `get_content()` being a legacy method: `get_content()` just calls `get_payload()` under the hood when using the `raw_data_manager` (which is used by the `default` policy), so I guess the "legacy method" note is only for end users. `get_payload()` is still very much involved in the process.

Regarding the What's New in Python 3.2's description of `get_payload()` you linked, I find it odd that the docs say it will decode the message with a CTE of 8bit, but in the code, it will only attempt to use the ASCII codec (which should only be used for 7bit and other CTEs that only use ASCII, such as base64) before falling back to raw-unicode-escape. I implemented my fix idea in [this PR](https://github.com/python/cpython/pull/105306), and it handles the right single quotation mark character as expected:

User dougmccasland: Thanks Michael.

Adding some more:
When I use get_payload() (no args) and the message has CTE quoted-printable, the payload is not QP-decoded; but it's correctly QP-decoded with get_content():

Input:

Results:

So as a work-around, I am using get_payload() (no args) for CTE 8bit, and get_content() (no args) for CTE QP (or anything besides 8bit).  Tested with CTE base64 and get_content() works for that.

--Doug

---------END---------
78
User Eclips4: Tried on current main:
Fails on test_capi/test_misc/TestOptimizerAPI.test_counter_optimizer

User Eclips4: Seems to be introduced in #105100 
cc @markshannon 
User AlexWaygood: Fixed by https://github.com/python/cpython/pull/105683
User sunmy2019: I am testing heads/main: 6a80664ef12 and the issue happens again.


User markshannon: > Fixed by #105683

Unfortunately not.
User gvanrossum: Just got hit by this (it makes the refleak buildbots fail in #105924).

I've got a hunch the problem is in deopt_code() (in codeobject.c), but I can't quite nail it. I put a printf() there when opcode == ENTER_EXECUTOR, and in the loop in the test which executes five times, it prints only four times?! Then the second time the whole test is re-run, it prints never at all (but the test fails after the first time through the loop). It appears the *first* iteration (out of five in the test -- `for repeat in range(5)`) doesn't cause any executor to be cleared when clear_executors() sets `func.__code__ = func.__code__.replace()`. But the first iteration clearly _has_ an executor, because the assert passes (the first time the whole test is run).

Not sure where this really points, but the logic in deopt_code() is subtle (it gets the opcode from `_Py_GetBaseOpcode(code, i)`, then patches `instructions[i]`). I get lost when I try to understand `_Py_GetBaseOpcode()`.
User gvanrossum: This was now fixed by @vstinner in #106171. His fix is to create a whole new function object (by using `exec`). This is needed because the original code object is kept alive by the parent code object -- and the only way to clear a code object's `co_executors` array is to cause the code object to be finalized.

(I tried putting the assignment to `func.__code__` in front of the `yield`, but it didn't seem to work.)

Arguably we ought to have an explicit API for this, but that can wait.
---------END---------
79
User brettcannon: This would make cross-builds easier since an in-place build is the default outcome when building CPython.
---------END---------
80
User carljm: It is already the case in all recent Python versions that the lazy-update behavior of the dictionary returned from `locals()` causes weird frame-introspection-sensitive semantics around lazy iteration of `locals()`. E.g. this code will work fine:



But run this under a tracing function that simply accesses `frame.f_locals`:



And suddenly you instead get `RuntimeError: dictionary changed size during iteration`.

The reason is because accessing `frame.f_locals` triggers a lazy update of the cached locals dictionary on the frame from the fast locals array. And this is the same dictionary returned by `locals()` (it doesn't create a copy), and so the tracing function has the side effect of adding the keys `k` and `v` to the locals dictionary, while it is still being lazily iterated over. Without the access of `frame.f_locals`, nothing triggers this locals-dict lazy update after the initial call to `locals()`, so the iteration is fine.

This is a pre-existing problem with the semantics of `locals()` generally, and there are proposals to fix it, e.g. [PEP 667](https://peps.python.org/pep-0667/).

What is new in Python 3.12 is that PEP 709 means comprehensions can newly be exposed to this same issue. Consider this version of the above function:



Under PEP 709, `k` and `v` are now part of the locals of `f` (albeit isolated and only existing during the execution of the comprehension), which means this version of `f` is now subject to the same issue as the previous for-loop version, if run under a tracing func that accesses `f_locals`.
User carljm: I recently discovered code of this form in `numpy`. The fix is to eagerly iterate `locals()` and extract the data you need, and avoid running any additional code during iteration of `locals()`:



Given the inherently unstable nature of the `locals()` dictionary under current `locals()` semantics, I think this approach is generally advisable when iterating over `locals()`. I already submitted this fix to numpy and it was merged: https://github.com/numpy/numpy/pull/23855
User carljm: My current plan to address this is to a) document it in the What's New for Python 3.12, and b) run a search over popular PyPI projects to look for comprehensions that iterate over `locals()` and submit the same fix to them.

I think longer-term the right fix for this issue is something like PEP 667, so that `locals()` has more predictable behavior.
User gaogaotiantian: Just to add that `locals()` has been a painful issue for `pdb` for a long time. `pdb` had to create a cache for `f_locals` to make most of the code execution work. However, any read attempt to the `locals()` would clear the code execution result (the variables are temporarily stored in `locals()` dict).

We have fixed a bug where `ll` will clear the variable changes in https://github.com/python/cpython/issues/101673, we have an open issue where `up` and `down` will clear the variable changes https://github.com/python/cpython/issues/102864 which is difficult to fix because the code is in `bdb`. We have to work around the `locals()` issue - but the user can always execute `locals()` to check all the local variables and clear the variable changes.

This problem has been there since at least 3.8(that's what I checked, I assume it would be even earlier). I strongly agree that we should do something about `locals()` to make it work more reliably.
---------END---------
81
User brettcannon: With `Tools/wasm`, the tooling is there for building WASI, but the overall expected development flow could probably stand to be written out explicitly.
User brettcannon: From @mdboom via [a comment](https://github.com/python/cpython/pull/105251#discussion_r1214784539):


1.    Run build_wasi.sh
2.    Hack on code
3.    Rebuild by cd'ing to builddir/wasi and running make
4.    Repeat steps 2-3
5.    ?
6.    Profit!

User brettcannon: https://github.com/python/cpython/issues/105261 would probably simplify things if it were possible.
User brettcannon: We should probably consider creating a separate WASI dev container which does the actual build of WASI as part of the pre-build. That way the main dev container can be a bit faster and smaller to get up and going, while providing a pre-build that has WASI already built and ready to go.
---------END---------
82
User jamesmurphy-mc: # Bug report

When pickling and unpickling an instance of an instance of a metaclass, `_pickle` reaches into the class object directly and calls `tp_new` from the C side, whereas `pickle` uses `cls.__new__`, which triggers a custom `__getattribute__` if defined. Whether this is a bug or just an intentional optimization I'm not sure but at the very least one there is an observable difference in behavior in that unpickling from a distribution where `_pickle` is available does not call `__getattribute__` but unpickling from a distribution where `_pickle` is not available does call `__getattribute__`.

# MWE

(Works on all available CPythons on Compiler Explorer, 3.5 - 3.11)
https://godbolt.org/z/x76bs8dzv

To simulate different distributions having `_pickle` available or not, we can use a meta path entry to cause it to fail to import, causing `pickle` to fall back to the pure Python version.



Output with `hide_pickle = True`


Output with `hide_pickle = False`

---------END---------
83
User bacher09: # Bug report

This bug happens in [Objects/stringlib/fastsearch.h:589](https://github.com/python/cpython/blob/v3.11.3/Objects/stringlib/fastsearch.h#L589) during matching the last symbol. In some cases, it causes crashes, but it's a bit hard to reproduce since in order this to happen, the last symbol should be the last in this particular memory page and the next page should not be read accessible or have a different non-contiguous address with the previous one. 

The simplest script that reproduces the bug for me is:



But since the result of this script depends on a file system,  kernel, and perhaps even a moon phase :smile: , here's a much more reliable way to reproduce it: 


This causes the bug across all Linux environments that I've tried. It uses a trick with inaccessible memory region to increase the chances of this bug happening and no files, to speed it up.
Here's some extra info from GDB:



# Your environment

- CPython 3.11.3
- OS: Linux 6.1 (but it should be OS independent)

I've also tried a bit modified version of a script on OS X, and it crashes there as well.


cc @sweeneyde (since you are the author of d01dceb88b2ca6def8a2284e4c90f89a4a27823f and 6ddb09f35b922a3bbb59e408a3ca7636a6938468).



<!-- gh-linked-prs -->
### Linked PRs
* gh-105252
<!-- /gh-linked-prs -->

User bacher09: Also wanted to add that this bug happens not only with `mmap`, it's very easy to see if you add a few extra `assert` statements into `fastsearch.h`.
User bacher09: The simplest fix would be just doing bounds checks, but this could eat away some performance improvements from a fastsearch and perhaps someone who is more familiar with the intricacies of this search algorithm would be able to come up with something better.


User sweeneyde: Thanks for the report. We might be able to keep fastsearch.h the way it is (reading one past the end of the haystack, which is safe for null-terminated strings including python bytes objects), and instead alter the call for the mmap module to add a special-case to check for `haystack[-len(needle):] == needle` with one memcmp. I can look into this today.
---------END---------
84
User ronaldoussoren: PyObjC contains some functionality that needs to walk the entire MRO of classes and access the ``tp_dict`` slot. As of Python 3.12 beta 1 this code crashes due to the slot being ``NULL`` on static builtin types.

The code in PyObjC walks the MRO to implement ``tp_getattro`` for the proxy of Objective-C classes to replicate ``PyObject_GenericGetAttr`` with some customisations, as wel as in the implementation of an alternative to ``super`` (again with customisations in the attribute resolution path). All code paths only need read-only access to the ``tp_dict`` of static builtin types, although some code needs to be able to update the ``tp_dict`` of classes created by PyObjC.

There is currently no documented alternative for directly accessing ``tp_dict``, and because of this I've changed PyObjC to use the private API ``_PyType_GetDict``.

Some alternatives I've looked into:

* Use ``PyObject_GetAttrString(someType, "__dict__")``: This increases the complexity of the PyObjC code bases and requires changes to long stable code:
  
  * Needs to introduce a different code path for accessing the ``__dict__`` slot of non-PyObjC subclasses
  
  * Needs to switch to the abstract Mapping API instead of the PyDict API, which likely has a performance impact
  
  * Changes to reference counting invariants in the code

* Use ``_PyType_GetDict`` on Python 3.12 or later: This works, but introduces a reliance on a private CPython API that might disappear at any moment (even micro releases), for example by no longer exporting the function from shared libraries.

Proposal:  Rename ``_PyType_GetDict`` to either ``PyUnstable_Type_GetDict`` or ``PyType_GetDict``, while keeping the current interface (e.g. returning a borrowed reference and never raising an exception).

This is a follow-up from #105020 

@ericsnowcurrently 

<!-- gh-linked-prs -->
### Linked PRs
* gh-105747
<!-- /gh-linked-prs -->

User ronaldoussoren: @Yhg1s : Would it be acceptable to introduce such a new API in an upcoming 3.12 beta release?
User ronaldoussoren: @Yhg1s, I've added the deferred blocker label because this issue is a seemingly intentional regression from 3.11 without a documented replacement.

I'm currently using a private API and that doesn't feel right.
User ericsnowcurrently: FWIW, I'm in favor of adding `PyType_GetDict()`.  I've put up a PR: gh-105747.
User ronaldoussoren: I agree that its is better to return a new reference instead of a borrowed reference in APIs like this, but that can make life harder for anyone transitioning to the new way of working.  In my particular case it does make transitioning harder because I'll have to add refcount updates where I didn't have to do before.  That said, this is not a big problem.
---------END---------
85
User JelleZijlstra: The oparg for MAKE_FUNCTION is a mask of various flags (e.g. 0x8 means it has a closure). These are currently hardcoded as integers; let's use named constants instead.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105215
<!-- /gh-linked-prs -->

---------END---------
86
User The-Compiler: # Bug report

When a dataclass with `__module__ = "builtins"` is defined (which seems somewhat common for custom exception classes), this seems to result in a `__builtins__` attribute to `__builtins__` (which recursively points to the builtins module):



results in:



This in turn results in an `import astroid.nodes` causing an endless recursion, in the original issues due to [how schemathesis uses `dataclasses`](https://github.com/schemathesis/schemathesis/pull/1736/files#diff-c0a7dd254259e4e0371c4ba7e54bc3fda03f62e751e977f22e06879b91234aca):

Downstream reports:

- https://github.com/pylint-dev/astroid/issues/2191
- https://github.com/pytest-dev/pytest/issues/11053
- https://github.com/schemathesis/schemathesis/issues/1745

# Your environment

- CPython versions tested on: 3.7.16, 3.8.16, 3.9.16, 3.10.11, 3.11.3, 3.12.0b1
- Operating system and architecture: Archlinux 64-bit
User terryjreedy: It is normal for modules created from Python to have attribute '__builtins__'.  I am not surprised that one is added here, so I doubt that this is a bug.

My recursive search of Lib/*.py failed to find `__module__ = "builtins"` or `__module__ = 'builtins'`.  Dataclasses might never be used in the stdlib for a custom exception.  Normal is `class CustomError(Exception): pass` or similar.

@ericvsmith Is there any point to using dataclasses for custom exceptions?
User The-Compiler: > It is normal for modules created from Python to have attribute '**builtins**'. I am not surprised that one is added here, so I doubt that this is a bug.

While that's true, it does seem very surprising to me that merely declaring a dataclass ends up dynamically adding a new attribute to an existing module, which wasn't there before.
User The-Compiler: From a (very) quick reading of the dataclasses source, it looks like this is actually happening because `exec()` gets called with the `__module__` as `globals`:



And this behavior actually [seems to be documented](https://docs.python.org/3/library/functions.html#exec):

> If the *globals* dictionary does not contain a value for the key `__builtins__`, a reference to the dictionary of the built-in module [`builtins`](https://github.com/python/cpython/issues/builtins.html#module-builtins) is inserted under that key. That way you can control what builtins are available to the executed code by inserting your own `__builtins__` dictionary into globals before passing it to [`exec()`](https://github.com/python/cpython/issues/105203#exec).

While that still seems rather surprising, I suppose that's working as documented then?
User ericvsmith: Yes, this is because of the `exec()` behavior. I'm curious why `__module__ = "builtins"` is being used.
User The-Compiler: From [what I can gather](https://github.com/schemathesis/schemathesis/commit/a5432405f6342477809c917e0bd1f4fcd9b7dd20), it's a hack to see `InvalidSchema` in an error message, rather than `schemathesis.exceptions.InvalidSchema`.

I have a feeling I've seen this hack used elsewhere before as well, but I can't remember where exactly.

edit: Might have been [this bit in pytest](https://github.com/pytest-dev/pytest/blob/24534cdd29d74e302e86db1a719c99e024d7903e/src/_pytest/outcomes.py#L54-L57) I'm thinking of.
User terryjreedy: I see no CPython issue here.
User ericvsmith: > From [what I can gather](https://github.com/schemathesis/schemathesis/commit/a5432405f6342477809c917e0bd1f4fcd9b7dd20), it's a hack to see `InvalidSchema` in an error message, rather than `schemathesis.exceptions.InvalidSchema`.

Irrespective of this particular issue, it seems like a bad idea to hide the type of an exception.


User Stranger6667: @The-Compiler 

You're right about the reason it was implemented this way. I found this hack exactly in the place in `pytest` you've mentioned and used it in Schemathesis.
---------END---------
87
User iritkatriel: As discussed in https://github.com/capi-workgroup/problems/issues/1,  we have some C API functions that have ambiguous return values, requiring the caller to query ``PyErr_Occurred()`` to find out whether there was an error.

We will try to move away from those APIs to alternative ones whose return values non-ambiguously indicate whether there has been an error, without requiring the user to call ``PyErr_Occurred()``.

In this issue we will discuss the iterator API. ``PyIter_Next`` return NULL for both error and for the iterator being exhausted. ``PyErr_Occurred()`` distinguishes between the cases.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105202
<!-- /gh-linked-prs -->

User erlend-aasland: Before we rush to introduce new APIs, can we clarify the following:

> We will try to move away from those APIs to alternative ones whose return values non-ambiguously indicate whether there has been an error.

I interpret this as:

- for function retuning int, 0 is a successful call, -1 is an error
- for function returning PyObject pointer, NULL is an error, anything else is ok

Error means a raised exception.
User iritkatriel: I don't think it's a good API for an iterator if you need to check 2 things just to know whether iteration completed, and then after the loop you need to check again why it was completed.
User erlend-aasland: Perhaps iterator APIs should get their own issue, since they require ambiguous return values? It seems to me they don't fit in the premises established by the OP.
User iritkatriel: Sure, let make this issue about iterators.
User iritkatriel: My proposal is to add a new ``PyObject *PyIter_NextItem(PyObject* iter, int *err)`` which returns the same as ``PyIter_Next``, but also sets ``*err`` to 0 in success and -1 on error.

The reason to do this and not ``int PyIter_NextItem(PyObject* iter, PyObject **item)`` is because of how this function is used while looping over an iterator: for each iteration we want to know whether we got another value or not. Only after (when we got a NULL) we want to check the error value to see how the iteration exited.

So I am proposing:



rather than

  
User iritkatriel: CC @encukou 
User erlend-aasland: I still fail to see how the problem "API A has ambiguous return value" is solved by introducing API B which also has an ambiguous return value.
User iritkatriel: I‚Äôm giving up on incrementally fixing this in the current c api and will turn my attention to the new c api work. Those who warned me were right, unfortunately.
User iritkatriel: (See discussion on the attached PR for the full picture).
User gvanrossum: > I‚Äôm giving up on incrementally fixing this in the current c api and will turn my attention to the new c api work. Those who warned me were right, unfortunately.

Isn't this overreacting? Ignoring Raymond's knee-jerk reaction, Erlend's feedback points to a real issue with how we've been talking about this, but is not a reason to give up on incremental fixes. The way I read Erlend's feedback is that the problem isn't that the result or return value is ambiguous, but that it requires calling `PyErr_Occurred()` to get the full picture. There are a few ways we can design the replacement for `PyIter_Next()` to avoid `PyErr_Occurred()`, and we simply have to look at the ergonomics to choose one. That seems a rational choice we can make.
User iritkatriel: > Isn't this overreacting? [...] s not a reason to give up on incremental fixes. 

I figured with two -1s and no other responses this would die due to "no consensus", and in general I think "is it worth it" discussions for incremental changes are always going to be opinionated/political, which is not really how I want to spend my time.

But I'll reopen the issue and PR so we can continue.

User erlend-aasland: FWIW, here's my thoughts about this: I really liked an early variant of your PR where you had an `int` *return value* and an `PyObject *` item *output param*. IMO, that was an API that is less likely to be misused/misunderstood; the *return value* aligns with a well established C idiom[^1]. I don't see it as a problem that looping will involva a few more lines of code. As I see it, the resulting "explicit code" is easier to read and reason about. If I can chose between an API designed to save lines in a `while` loop and an API designed to be as unambiguous as possible, I'll go for the latter any day. But API design is subjective, even though there certainly are aspects which can be discussed objectively.

Also, I appreciate that there is a dedicated place to record API problems and discuss them; I really appreciate Irit's hard work with managing all of this. API design is hard, and I think it is important to discuss proposed APIs thoroughly before landing a design[^2], rather than rushing that process and landing possibly premature solutions. I also understand that long discussions about new APIs can be discouraging.

[^1]: I believe the use of well established C idioms should be weighted in API design, as I believe it makes for APIs that are less likely to be misused. Maybe I'm wrong.
[^2]: Take a peak at the HPy repo and see how carefully they design every single API! Personally, I find it very inspiring to follow their discussions.
User markshannon: We should decide in general whether functions that effectively return a `(error, value)` pair should have the signature
`int foo(..., PyObject **value)` or `PyObject *foo(..., int *err)`.

I strongly prefer the former: I think it makes for nicer flowing code, makes it harder to miss the error, and is (IMO) more idiomatic C.

As to what the return values should be:
* -1 for an error. As we use that convention *everywhere*
* 0 for a "lesser" result, which in this case is termination (for `PyDict_GetItem`, the case where the key is absent).
* 1 for the "greater" result, the presence of a value (for `PyDict_GetItem`, the case where the key is present).

What is the "lesser" or "greater" result is somewhat subjective, but I think it makes for the least surprising API.

There are three cases to deal with, the return code should reflect that. Any code using the function will need to handle all three cases, with two tests, but should be able to test for the most common case with a single test. The trio of values, `-1, 0, -1` allows that.

The version returning an `int` code can be used just as efficiently as the value returning a `PyObject *` value.

E.g. with `int PyIter_NextItem(PyObject *iter, PyObject **next)`


This sort of three value return is relatively common, so we should have a consistent, documented pattern for it.

User erlend-aasland: I agree with Mark on all points. I also think we should agree on and document the new API guidelines _before_ solving any existing issues; that way, there will hopefully be fewer long and exhausting discussions for each new API.
User iritkatriel: @markshannon That's not a bad option, I tried it in an earlier iteration and I'm not sure I landed on anything better.   How would you fix ``PyUnicode_Compare``? It currently returns -1, 0 1 and we need to add a fourth value for error.

@erlend-aasland I appreciate your remarks, but let's not refer to what happened on that PR as "discussion". Children of an impressionable age have access to GitHub and they might be reading this. We should teach them better than that.

User markshannon: > How would you fix PyUnicode_Compare?

`PyUnicode_Compare` can only fail because it has the wrong type signature.
Change it from `int PyUnicode_Compare(PyObject *left, PyObject *right)` to 
`int PyUnicode_Compare(PyUnicodeObject *left, PyUnicodeObject *right)` and it becomes infallible.

User markshannon: Assuming you want to keep the more general signature, then yes it does need more return values.
There are in fact five possible return values: `error, less, equal, more, unordered`. The last for sets, floats, etc.
In which case I would use the following enum:

See https://github.com/python/cpython/blob/main/Include/internal/pycore_code.h#L469 for why this seemingly odd choice makes sense.
User encukou: > As to what the return values should be:
> * -1 for an error. As we use that convention everywhere
> * 0 for a "lesser" result, which in this case is termination (for PyDict_GetItem, the case where the key is absent).
> * 1 for the "greater" result, the presence of a value (for PyDict_GetItem, the case where the key is present).

That sounds like a good direction.
A general guideline should also suggest when `*result` should be set. For the ‚Äúlesser‚Äù case, do we leave it untouched, set it to `NULL`, or say that users shouldn't use it? What about the ‚Äúerror‚Äù case?

For `PyDict_GetItem`, it seems ergonomic enough to do:

* -1 for an error, everywhere. `*result` is undefined, may be garbage, don't touch it.
* 0 for success
   * result is NULL, where the key is absent
   * result is non-NULL if found

In a [`PyWeakref_GetRef`](https://github.com/python/cpython/blob/2de389766342ee18907130debf63a0336b0bbb45/Objects/weakrefobject.c#L898-L911)-style function, determining whether to return 1 or 0 is a (admittedly tiny) bit of extra work that most callers could ignore. Is it worth the consistency? Or should `__next__`-style function be the odd ones that return `1` for convenience in a `while` loop?
User markshannon: For `PyDict_GetItem`, I'd prefer to return `0` for not found, and `1` for found.
This seems more consistent; not found is the "lesser" result.
It also reduces memory traffic a bit, as `*result` is only touched if the value is found.

For example, implementing a version of `dict.get`, where the key is expected to be missing:
 

User markshannon: `PyWeakref_GetRef` does a lot of work to check that the weakref is not NULL and is a weak ref.
See https://github.com/capi-workgroup/problems/issues/31
User erlend-aasland: Can we please open a devguide issue for establishing and documenting these API guidelines, and then move the general discussion we're now having over there? When that discussion has landed, and guidelines are documented, we can ~~start~~ continue fixing existing problematic APIs by adding new APIs according to the established guidelines.
---------END---------
88
User iritkatriel: The last two lines in this snippet should be indented further than the line above them, they are more specific.
(Alternatively, they can be removed because they don't add much).

<img width="401" alt="image" src="https://github.com/python/cpython/assets/1055913/5bee1ab5-4a6f-45a7-b0fb-de2710002c10">


<!-- gh-linked-prs -->
### Linked PRs
* gh-105672
* gh-105782
* gh-105786
<!-- /gh-linked-prs -->

User rhettinger: I would just fix the indentation on the last two.  Then, I would check to make sure the link targets are unchanged so that external links into our docs aren't broken.
User iritkatriel: I think Ellipsis should be indented under slice as well:

<img width="194" alt="image" src="https://github.com/python/cpython/assets/1055913/ca8f0f8d-f323-4b97-b93c-47989cf787bc">

User iritkatriel: And internal frames under frame objects:

<img width="225" alt="image" src="https://github.com/python/cpython/assets/1055913/1e047383-34fd-4c68-8049-d8ca0ddf3585">

User safwansamsudeen: Hey, I'm new here, would love to contribute to Python!

It's not much of a fix, but can I work on this?

If so, which file is this?
User roy9495: Hi, I made the indentation changes in these two files Doc/build/html/contents.html, Doc/build/html/c-api/concrete.html respectively but these files are untracked so when I am trying to commit the commit is becoming too large. 
Any guidance you can give me will be helpful.
User safwansamsudeen: @roy9495, were you assigned to this? 
User roy9495: @safwansamsudeen No, I was not assigned. Sorry, I should have asked first.
User iritkatriel: We don‚Äôt assign issues to contributors. 

The way it goes is that if you want to work on an issue you leave a comment about that, and if there is such a comment by someone else, and it‚Äôs a fairly recent comment, you let them work on it. If it‚Äôs an older comment you should ask them if they are still working on it, and if they don‚Äôt reply within a reasonable amount of time you can leave a comment to indicate that you are working on it.

So basically, we communicate our intentions on the issue.


User roy9495: @safwansamsudeen Are you still working on this?
User safwansamsudeen: @roy9495 I didn't start it. If you're working on it, that's cool! If you're busy, I'll take it up.
User safwansamsudeen: > @safwansamsudeen No, I was not assigned. Sorry, I should have asked first.

Ahh @roy9495 I'm sorry, I misread the conversation. My first comment on this issue indicated that I wanted to work on this, and I interpreted your comment to be disregarding mine and working on it instead... which is why I asked if you were assigned to it.

I now see you were responding to my request of help, ugh. Thanks, and do accept my apologies for my coldness in my second comment! And great work on the PR!
User roy9495: @safwansamsudeen Thank you, Please don't apologise. 
User CAM-Gerlach: PR and all backports merged, so closing this now. Thanks @roy9495 and @iritkatriel!
---------END---------
89
User graingert: # Documentation

https://docs.python.org/3.12/library/importlib.resources.abc.html#importlib.resources.abc.TraversableResources says it is deprecated in favour of itself

it should be documented here https://docs.python.org/3.12/library/importlib.html#module-importlib.abc

introduced in https://github.com/python/cpython/commit/10b12dd92ab6e29b573706dd6adb3d8eb63ae81b#r115987571

cc @jaraco @hugovk 

<!-- gh-linked-prs -->
### Linked PRs
* gh-105232
<!-- /gh-linked-prs -->

User FFY00: Yep, the warning should be in `importlib.abc` instead of `importlib.resources.abc`.
---------END---------
90
User iritkatriel: As discussed in https://github.com/capi-workgroup/problems/issues/20, there are functions in the C API that do not have a way to report error (void return type).

Some of them actually set the error indicator in some cases, and the only way to know it to check ``PyErr_Occurred()``. In other cases the functions currently do not have error cases, but the lack of return value is still a problem in case we want to evolve them in the future and need to report errors. 

We will use this issue to fix the straightforward ones, while more complicated ones may spawn their own issues.



<!-- gh-linked-prs -->
### Linked PRs
* gh-105185
* gh-105218
* gh-105219
* gh-105220
* gh-105221
* gh-105222
* gh-105223
* gh-105233
<!-- /gh-linked-prs -->

---------END---------
91
User martindemello: # Bug report

Compiling the following code:



generates



There is no `SETUP_ANNOTATIONS` opcode generated despite the `__annotations__` access later on. (Found via https://github.com/google/pytype/issues/1435 - for some reason this does not generate a runtime error, but it does cause issues for tools like pytype.)

# Your environment

cpython 3.10

<!-- gh-linked-prs -->
### Linked PRs
* gh-105177
* gh-105313
* gh-105314
<!-- /gh-linked-prs -->

User yilei: FWIW, it's a runtime error when the module is imported, not run as main:


User brandtbucher: I think `find_ann` in `compile.c` needs to be taught about `Match_kind`.

Looks like @JelleZijlstra picked this up already. Feel free to ping me for review!
---------END---------
92
User graingert: <!--
  If you're new to Python and you're not sure whether what you're experiencing is a bug, the CPython issue tracker is not
  the right place to seek help. Consider the following options instead:

  - reading the Python tutorial: https://docs.python.org/3/tutorial/
  - posting in the "Users" category on discuss.python.org: https://discuss.python.org/c/users/7
  - emailing the Python-list mailing list: https://mail.python.org/mailman/listinfo/python-list
  - searching our issue tracker (https://github.com/python/cpython/issues) to see if
    your problem has already been reported
-->

# Bug report
With the following demo code, a subgenerator `yield from` from a parent generator main(), that catches exceptions and returns "good":
 


when run with `python -m trace`, the exception is delivered to the outer generator main() instead of being suppressed


the problem also occurs with the equivalent generator syntax, eg:



# Your environment

<!-- Include as many relevant details as possible about the environment you experienced the bug in -->

- CPython versions tested on: c05c31db8c9dfd708b9857bb57f8e5f3ce40266d 3.12b1
- Operating system and architecture: Ubuntu 22.04 x86_64

see also

https://github.com/urllib3/urllib3/issues/3049#issuecomment-1570452951
https://github.com/nedbat/coveragepy/issues/1635


<!-- gh-linked-prs -->
### Linked PRs
* gh-105187
* gh-105378
<!-- /gh-linked-prs -->

User graingert: @markshannon bisected to 411b1692811b2ecac59cb0df0f920861c7cf179a
User brandtbucher: It might be related to the fact that `YIELD_VALUE` cannot raise...

https://github.com/python/cpython/blob/a99b9d911e0f8cb11b3436bdd8eb649b15d01a50/Python/bytecodes.c#L929-L932

...but `INSTRUMENTED_YIELD_VALUE` can:

https://github.com/python/cpython/blob/a99b9d911e0f8cb11b3436bdd8eb649b15d01a50/Python/bytecodes.c#L910-L918

That seems incorrect.
User brandtbucher: This is a little easier for me to follow:





User gaogaotiantian: Will the actual exception in the trace function be thrown away if we simply remove the error check in instrumented version?
User brandtbucher: > Will the actual exception in the trace function be thrown away if we simply remove the error check in instrumented version?

No, we would need to explicitly clear it.
User Yhg1s: Is there anything left to do for this issue?
User brandtbucher: Nope!
---------END---------
93
User AlexWaygood: On `main`, an `isinstance()` check against `Sized` works just the same as it does on Python 3.11:



However! If you first subclass `Sized` like this, `TypeError` is raised on that `isinstance()` check!



This was originally reported by @vnmabus in https://github.com/python/typing_extensions/issues/207.

Note that (because of the `abc`-module cache), this _doesn't_ reproduce if you do an `isinstance()` check _before_ subclassing `typing.Sized`:



<!-- gh-linked-prs -->
### Linked PRs
* gh-105152
* gh-105159
* gh-105160
<!-- /gh-linked-prs -->

User AlexWaygood: This bisects to https://github.com/python/cpython/commit/b27fe67f3c643e174c3619b669228ef34b6d87ee
User AlexWaygood: Also reproduces if you use `collections.abc.Sized` rather than `typing.Sized`:


User JelleZijlstra: With some debugging we found that this is due to code in _abc.c that iterates over all the `__subclasses__` of the ABC:

https://github.com/python/cpython/blob/c05c31db8c9dfd708b9857bb57f8e5f3ce40266d/Modules/_abc.c#L768

Alex's PR #105152 works around this problem by re-arranging the code in typing.py. In #105159 I propose a fix on the ABC side instead. We should consider merging that PR too to prevent similar problems with other ABC subclasses.
User AlexWaygood: This issue is fixed, and Jelle's decided not to pursue #105159, so I'll close this 
---------END---------
94
User hmc-cs-mdrissi: # Bug report
I'm unsure if this is more tensorflow bug or python bug but it does trace to this [pr](https://github.com/python/cpython/pull/103034) and error message traces to cpython source. I've also commented on issue in tensorflow side [here](https://github.com/tensorflow/tensorflow/issues/60687).



With typing extensions an error message is produced of,


# Your environment

I've tested on cpython 3.9.16 and 3.10.10 where using typing works.  I'd suspect 3.11 will work as well. Using typing_extensions 4.6 triggers error with inspect.getattr_static usage. Reported here instead of typing extensions as adding getattr static there was backport from here. I'd expect this to reproduce on 3.12 although installing tf on 3.12 is likely tricky and easier to test with typing extensions. The _DictWrapper class is [here](https://github.com/tensorflow/tensorflow/blob/f8066222ad6b2edbf1ee359fda66f2b43473dbe7/tensorflow/python/trackable/data_structures.py#L782) and is like a subclass of `__dict__` with a custom getattribute to do some extra tracking of elements.

I used latest version of tensorflow (2.12). The underlying error message comes from this [line](https://github.com/python/cpython/blob/f90d3f68db720bd6d0deda8cc0030339ccd43858/Objects/typeobject.c#L2871). As this can be triggered in attribute access unsure whether it should be TypeError vs AttributeError. If it was attribute error getattr static would catch it and be fine. If it needs to be type error then it's unclear to me whether tensorflow class is breaking some assumption or if getattr static should also catch type errors [here](https://github.com/python/cpython/blob/f90d3f68db720bd6d0deda8cc0030339ccd43858/Lib/inspect.py#L1794). In particular it's interesting that `object.__getattribute__` can raise TypeError and inspect.getattr_static can fail.

I'll try to look tomorrow if I can make a more self contained example that doesn't require tensorflow.
User AlexWaygood: Thanks! Looks like the root cause here is probably a bug in `inspect.getattr_static` rather than `typing`. That does pose interesting questions for the typing_extensions backport, though.
User hmc-cs-mdrissi: One thing I noticed from exploring DictWrapper is it strangely satisfies 



This looks to come from library [wrapt](https://github.com/GrahamDumpleton/wrapt) being used to make DictWrapper behave like a dict. It relies mainly on [ObjectProxy](https://github.com/GrahamDumpleton/wrapt/blob/980e4b451c2b3cdef1d32c9ae10d97894dd3735e/src/wrapt/wrappers.py#L73) for this. My rough gut is there's likely a reproducing example only using wrapt/ObjectProxy without needing tensorflow. It feels like it partially proxies dict but not enough here.
User AlexWaygood: Yes, so here's a repro that doesn't involve `typing`, just `inspect.getattr_static` (using Python 3.11, since `tensorflow` doesn't install using CPython `main`):



I think there's probably bugs in CPython _and_ [either tensorflow or wrapt] here. The `_DictWrapper` class must be _pretty_ weird if `object.__getattribute__` raises `TypeError`. But also, `inspect.getattr_static` should be resilient to this kind of edge case: it should probably never raise `TypeError`.
User AlexWaygood: Interesting! It looks like `inspect.getattr_static` is buggy on <=3.11, but not on 3.12+. Here's a repro on 3.11 that just involves `wrapt` (no `tensorflow` code):



But on `main`, it works as expected:



Lots of changes have been made to `inspect.getattr_static` in Python 3.12 (in the name of improving performance -- see  #103193), so it's entirely possible that one of these changes "accidentally" fixed this bug.
User AlexWaygood: It reproduces with the `wrapt` C extension, but not without it. Which might be why it reproduces on 3.11, but not `main`...
User AlexWaygood: Here's a repro that doesn't involve `tensorflow` or `wrapt`: https://github.com/AlexWaygood/getattr-static-repro.

Unfortunately it involves a 3,000-line C extension (I've just vendored `wrapt`'s C extension to create the reproducible example)! I doubt I'll be the best person to help narrow this down further, since I'm no good with C. Maybe @carljm or @JelleZijlstra would be able to help narrow this down further?
User AlexWaygood: This issue looks pretty related:

- https://github.com/GrahamDumpleton/wrapt/issues/231
User AlexWaygood: The `TypeError` here is because `inspect.getattr_static` is doing `object.__getattribute__(x, '__dict__')`, and where `x` is an instance of `wrapt.ObjectProxy`, that can raise `TypeError` _if_ the `wrapt` C extension is installed.

I _think_ it's only possible for `object.__getattribute__(x, '__dict__')` to raise `TypeError` if `x` has been created via a custom C extension. And I think _that_ means that `inspect.getattr_static` is doing nothing wrong here, and there is only a bug in `wrapt`. But, very interested to hear if @carljm has any thoughts on that!
User JelleZijlstra: Haven't had a chance to investigate much, but here's the C stack trace (on main) for where the `this __dict__ descriptor does not support 'Bar' objects` error is raised:


User carljm: I have this on my list to take a look at, but it may be some time next week before I can get to it.
User carljm: I looked into this a bit. Here's what's happening:

1. `Foo` is a normal Python type, which has a `__dict__` descriptor (it only inherits `object`, whose `tp_dictoffset` is `0`, so it gets `Py_TPFLAGS_MANAGED_DICT` and a `tp_dictoffset` of `-1`, so its `tp_getset` includes `__dict__`, which means that its own type dict gets populated with the default descriptor for `__dict__` by `type_add_getset`).
2. `Bar`, on the other hand, inherits the builtin type `WraptObjectProxy_Type` (it also inherits `Foo`, but `WraptObjectProxy_Type` is the nearest base defining a C instance layout -- see `best_base` -- so it is used to define the layout for `Bar`), whose `tp_dictoffset` is non-zero, therefore `Bar` doesn't get `Py_TPFLAGS_MANAGED_DICT` and its own `tp_dictoffset` is left at `0`, therefore (see `type_new_set_slots`) it does not get a `__dict__` descriptor in its type dict.
3. The MRO of `Bar` is `(Bar, Foo, WraptObjectProxy_Type, object)`.
4. So (in `inspect.getattr_static`) when we look up `__dict__` in the type dictionary of `Bar`, we don't find it in `Bar.__dict__`, we go up the MRO to `Foo`, and we do find the default `__dict__` descriptor in `Foo.__dict__`.
5. When we actually try to run this descriptor's getter, `get_builtin_base_with_dict` finds `WraptObjectProxy_Type` as the nearest builtin base type with a dict, and so we try to use its `__dict__` descriptor. But it doesn't have one! (No entry for `__dict__` in `WraptObjectProxy_getset`.) Thus we get the error.

I'm not 100% sure about the logic behind `get_builtin_base_with_dict`; it seems a bit ad-hoc. But it's also been there a long time and would be very disruptive to change, so I'm not going to think too hard about changing it. If we accept its implementation as given, that implies that this is a bug in `WraptObjectProxy`: C extension types that define `tp_dictoffset` really need to also define a `__dict__` descriptor in order to avoid this problem.

Given all that, the question remains about what `inspect.getattr_static` should do when it encounters a type that is broken in this way. On one hand, the whole point of `inspect.getattr_static` is to check for existence of an attribute "safely." But on the other hand, I think this particular TypeError pretty much always indicates a bug in the C extension type, and I don't like papering over bugs and allowing them to persist longer undetected. In this case, if we catch the `TypeError` the result would be `inspect.getattr_static` would always fail to find instance attributes on instances of such broken types; this seems even more confusing than raising the `TypeError`. And I think the proper definition of "safely" for `inspect.getattr_static` is not "can never raise" but rather "doesn't execute arbitrary Python code," and no arbitrary Python code is being executed here. Raising in case of a badly-defined type seems to me still the best option for `inspect.getattr_static`.

So my recommendation would be to close this issue as not a bug in Python.
User AlexWaygood: Thanks so much for the in-depth investigation @carljm! This confirms my suspicions, and I agree with your conclusion.
---------END---------
95
User theGOTOguy: The [documented behavior](https://docs.python.org/3/library/csv.html#csv.QUOTE_MINIMAL) of `csv.QUOTE_MINIMAL` is that fields containing the quote character should be quoted.

However, running:




My expectation is that this field would have been quoted (e.g., `"\""`), as it contains the quote character (escaped).  If this is intended behavior, then the documentation should be clarified, or if it's not, this is a bug.

# Your environment

- Operating system and architecture:  Ubuntu 22.04

---------END---------
96
User Sh3idan: # Bug report

The classes [ctypes.BigEndianUnion](https://docs.python.org/3/library/ctypes.html#ctypes.BigEndianUnion) and [ctypes.LittleEndianUnion](https://docs.python.org/3/library/ctypes.html#ctypes.LittleEndianUnion) was added in Python 3.11. In the case of the use of an `Union` in a `Structure` class that match the opposite system endianness, the `Union` class can't be used as shown below:



The following traceback is given by the interpretor:




# Your environment

- CPython versions tested on: Interpreter compiled from the main branch in debug mode and also Python 3.11 (coming from the official release)
- Operating system and architecture: Windows 10 x64



<!-- gh-linked-prs -->
### Linked PRs
* gh-105106
<!-- /gh-linked-prs -->

---------END---------
97
User RobBotic1: # Documentation

The documentation states "The SSL context created above will only allow TLSv1.2 and later..." However, the example code sets the minimum version to TLS 1.3 (not 1.2 as stated).

I believe [this line](https://github.com/python/cpython/blob/c39500db5254c9efee9937f951f58d87ee14cf2f/Doc/library/ssl.rst?plain=1#LL2687C20-L2687C20) should be changed to:
`   >>> client_context.minimum_version = ssl.TLSVersion.TLSv1_2`

I am happy to create a pull request, if that is helpful.

<!-- gh-linked-prs -->
### Linked PRs
* gh-105404
<!-- /gh-linked-prs -->

User sunmy2019: > I believe [this line](https://github.com/python/cpython/blob/c39500db5254c9efee9937f951f58d87ee14cf2f/Doc/library/ssl.rst?plain=1#LL2687C20-L2687C20) should be changed to:
> ` >>> client_context.minimum_version = ssl.TLSVersion.TLSv1_2`

This part was changed in response to the deprecation of TLSv1_2. https://github.com/python/cpython/commit/2875c603b2a7691b55c2046aca54831c91efda8e

You should change the description text below instead.

https://github.com/python/cpython/blob/c39500db5254c9efee9937f951f58d87ee14cf2f/Doc/library/ssl.rst#L2691
---------END---------
98
User ayappanec: test_create_directory_with_write in test_zipfile fails in AIX with the below message

======================================================================
FAIL: test_create_directory_with_write (test.test_zipfile.test_core.TestWithDirectory.test_create_directory_with_write)

Traceback (most recent call last):
  File "/cpython/Lib/test/test_zipfile/test_core.py", line 2879, in test_create_directory_with_write
    self.assertEqual(zinfo.external_attr, (mode << 16) | 0x10)
AssertionError: 1106051088 != 5401018384

----------------------------------------------------------------------

The reason being AIX stat call st_mode returns 240700 (in octal) for a directory with 700 mode. In linux, it returns 40700. So there is extra  ‚Äú2‚Äù which seems to be related to journaled filesystem. So the logic requires that the mode should go a bitwise AND operation with 0xFFFF ,  like how it is done in **test_write_dir** in **test_core.py** to be in sync with **zinfo.external_attr**

<!-- gh-linked-prs -->
### Linked PRs
* gh-105228
<!-- /gh-linked-prs -->

---------END---------
99
User hmc-cs-mdrissi: I'm unsure if this is really a bug, but it's behavior that surprised me upgrading a typed dict to use Required/NotRequired 655 style syntax so would help to clarify.

# Bug report



vs 



is documented in the pep as more readable replacement. There is one way though that new syntax behaves differently then old that caused some tests I had to fail. (edited typo here) For`Movie1.__total__` is false. For `Movie2.__total__` is true. This is surprising when expecting `__total__` to mean all keys required. The current documentation does note [though](https://docs.python.org/3/library/typing.html#typing.TypedDict.__total__) `Point2D.__total__ gives the value of the total argument. Example:`. That documentation does exactly match implementation [here](https://github.com/python/cpython/blob/d593074494b39a3d51e67a4e57189c530867a7ff/Lib/typing.py#L2808) which looks to be true for years.

# Your environment

Tested on python 3.9 (using typing-extensions TypedDict/NotRequired), but reading main total argument is used for `__total__` without considering optional keys so should apply to main as well. The relevant line hasn't changed in ~4 years either so I expect it to reproduce for all versions since NotRequired (3.11+).
User JelleZijlstra: > For`Movie1.__total__` is true. For `Movie2.__total__` is false. 

I think you mean the opposite.

> This is surprising when expecting `__total__` to mean all keys required.

That expectation is wrong, as the `Point3D` example in the docs makes clear.

I'm not sure what change you are asking for, but it's probably worth making the docs for `__total__` more explicit that the value isn't all that meaningful in the presence of inheritance and `Required`/`NotRequired`: you should use `__required_keys__` and `__optional_keys__` instead.
User hmc-cs-mdrissi: Yes I got Movie1/Movie2 backwards writing that up.

I think a recommendation of prefer `__required_keys__`/`__optional_keys__` makes sense and is how I fixed my bug. Original code was written in 3.8 before them, but for any typed dict where Required may exist `total` now seems of low value. 

edit: The proposed change in mind if discrepancy was a bug would be if `__optional_keys__` is non empty then set `__total__` to false. But I'm fine changing the request to be documentation only change and will just prefer `__required_keys__` in future.
User sobolevn: I agree with @JelleZijlstra, `__total__` has a documented behaviour: 

> `__total__` gives the value of the total argument.

I also agree that the docs can be improved:
1. I think we can add an explicit example with `total=True` and `NotRequired` to show that they are not related.
2. I think we can rename the first class here: 
<img width="748" alt="–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2023-06-01 –≤ 13 57 58" src="https://github.com/python/cpython/assets/4660275/3b9f558f-5002-4e57-8673-3c1711d6a50b">

Right now the second class shadows the first one for no reason.
---------END---------
100
User Eclips4: Tried on curent main branch
<details>
<summary>Traceback</summary>


</details>

<!-- gh-linked-prs -->
### Linked PRs
* gh-106024
<!-- /gh-linked-prs -->

User Eclips4: seems to be introduced in #104798 
cc @lysnikolaou 
User Eclips4: I think, this traceback more helpful:
<details>
<summary>Traceback</summary>


</details>
User erlend-aasland: Thanks, Lysandros is already working on this since yesterday. BTW, the failure is not Windows specific: https://buildbot.python.org/all/#/builders/320/builds/730/steps/5/logs/stdio
User erlend-aasland: > seems to be introduced in #104798

FTR, it is verified that #104798 introduced this.
User Eclips4: > Thanks, Lysandros is already working on this since yesterday. BTW, the failure is not Windows specific: https://buildbot.python.org/all/#/builders/320/builds/730/steps/5/logs/stdio

Indeed. ~~I couldn't reproduce it on my WSL (Ubuntu 20.04), and thought that it's related to Windows system.~~
So, thanks for this point and removing os-windows label.
UPD: I was running test as `./python -m test -v test_peg_generator` on Linux, which doesn't cause test fail.
User Eclips4: I'm somewhat unsure about changing the issue title. Seems there two issues:
1 - which described in this issue - test fail (Windows issue)
2 - refleak test fail, which reproducible on Linux (but on Windows we cannot get refleak test results, because of test failing)
So, seems that there two problems, which probably will be solved by one PR
User erlend-aasland: Enclosing my response in `<details>` in order to not bring too much _clutter and noise_ into this issue:

<details>

Kirill, please see CI on `main`. You can see it using the GitHUb UI by clicking the "Code" tab. When in the "Code" tab, you can explore the CI run of the most recent commit:

<img width="933" alt="Screenshot 2023-05-29 at 23 25 50" src="https://github.com/python/cpython/assets/13780613/656463f1-3d13-457c-8ab7-2a737a08089d">

When you click the red cross or green check mark, you can inspect the CI run for the commit:

<img width="570" alt="Screenshot 2023-05-29 at 23 27 25" src="https://github.com/python/cpython/assets/13780613/e9e10636-5c30-488c-96aa-6bbed8da3bde">

As you can see, both the 32-bit and 64-bit Windows CI are completely green, which implies there is no failure on [current](39f6a0489fcc815a578d27dfee2feea003c896f8) `main`.

Perhaps the failed test you are seeing is because of an issue with your local setup? Did you try a fresh build? If no, please do so. In case of test failures, you should _always_ try `git clear -fdx` (in case of in-tree builds), or `rm -rf <builddir>` first.

However, what we _know_ from our CI, is that there's a _reference leak_.

</details>
User Eclips4: Thanks Erlend for your answer!
I know about it all.
After one situation with asyncio(eager-task merging) I've git clone and rebuilt interpreter every time when I find some fail.
So, there's a some proof of my words:
<details>


</details>

User erlend-aasland: @Eclips4, try with a UTF-8 setup.
User Eclips4: > @Eclips4, try with a UTF-8 setup.

Seems that with UTF-8 encoding test passes. But, cp1251 is a common encoding for Windows. 

User sunmy2019: (Copied from #105087)
Previously, test_peg_generator is skipped -- distutils has been removed in Python 3.12

Now it's added back in https://github.com/python/cpython/pull/104798 and is leaking references.

Recent builtbot results:
https://buildbot.python.org/all/#/builders/259/builds/775



---

Seems all ref leaks come from `Tools/peg_generator/pegen/build.py : compile_c_extension`

But no matter how many times we call this function, the leaks are the same as if it was called once.

This behavior looks like a cache was created and forgotten.
User lysnikolaou: FYI we still haven't been able to find a solution to this. We've bisected the problem down to a call to `setuptools.Distribution`. I've opened an issue on this on pypa/setuptools#3938 to figure out what the correct course of action is.
User sunmy2019: Hi, I tracked down part of the ref leaks to this cache:

https://github.com/python/cpython/blob/c6ca368867bd68d44f333df840aa85d425a51410/Lib/importlib/_functools.py#L6-L85

Used Here:
https://github.com/python/cpython/blob/c6ca368867bd68d44f333df840aa85d425a51410/Lib/importlib/metadata.py#L689-L691


Setting 

reduced the ref leaks.


User sunmy2019: ~~Part of the reference seems (not sure) to come from~~
~~https://github.com/python/cpython/blob/3fe7d7c020a8b2d395a58bfafbe689ee36f7fe30/Modules/_ctypes/callproc.c#L1877-L1926~~

~~string `LP_...` and string `_type_` seem to be leaked.~~
User sunmy2019: The rest ref leaks all came from here

https://github.com/pypa/setuptools/blob/b545fc778583f644d6c331773dbe0ea53bfa41af/setuptools/_distutils/dir_util.py#L12


That should be all for the ref leaks on Linux.
User sunmy2019: I will leave the fix to someone else. You can add me as the co-author of the fix if you wish to fix this.
User lysnikolaou: @sunmy2019 Thanks a lot for spending time on this! I'll work on opening separate issues if needed and fixing those leaks, probably early next week.
User erlend-aasland: @lysnikolaou, any updates on this? Perhaps we should skip rerunning these tests for now?
User lysnikolaou: Unfortunately I didn't have the time to look into this further. 

> Perhaps we should skip rerunning these tests for now?

Yup, that's probably a good idea.
---------END---------
101
User MarceColl: Fuck this
---------END---------
102
User mtrainham: Worst. App. Ever. Please make it not the worst app ever. Thanks.
User mtrainham: Fixed in #2, Please merge now.
User JohnPicchi: @mtrainham well since it is the default asp.net core app, take it up with microsoft
User mtrainham: Merge my PR damnit
---------END---------
103
User cborac: Amy I need an explanation. Wtf is this lol
User cborac: I NEED a RESPONSE
User cborac: AMY?
User amydevs: mmm?

On Mon, 25 May 2020 at 22:16, Sardonyx <notifications@github.com> wrote:

> AMY?
>
> ‚Äî
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/jy1263/impatient-eta/issues/1#issuecomment-633543321>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AMB5NUGTYC6AP5NL7KEIGCLRTJOQJANCNFSM4NJBNMXA>
> .
>

User cborac: ANSWER ME!
User amydevs: o its smth dumb i built lol
User cborac: oh lol
User cborac: uwu I'll close this bye
---------END---------
104
User elemm: This issue is a demo
User elemm: #5 
User elemm: #5 
User bchrobot: I'm still experiencing this issue.
---------END---------
105
User Haeyzl: 
User Lith1um: what kind of shit is this
---------END---------
106
User max11D: 
---------END---------
107
User abdallahAbdelhady: There is an small issue
I don't know what is it but there is an issue
User abdalla3yash: Dear mr.blela
      the really issue is this life, wanna die soon
---------END---------
108
User HunterAhlquist: I refuse to use any ++ applications until the "PLS FOLLOW ME!!!" and "SIGN UP FOR A ++ ACCOUNT!!!!" popups go away or can be disabled. My god it's annoying.
---------END---------
109
User CourtneyAhaus: Do we just use one color or use the red and the blue to show we are able to shuffle more than one deck?
User BHolmes117: I believe we just use one color of the back of the card
User nhabbott: I decided to say fuck this....
---------END---------
110
User EmilSoderlind: Dear Mr. Hedberg,

Could you perhaps please get your shit together and reincorporate the brilliant switch statement once again, bitch.


XoXo,
SkruvdragarN
User HedbergMartin: This is a stupid request that would ruin the code even more than your previous contribution. But not as bad as having the read and write in the same method. Therefore I am closing this issue.
---------END---------
113
User dclgamingtr: When the Arena also attacked someone. If you win the war, I get points. but the score does not decrease from the person who lost the attack.
User insthync: It's not a problem, I intently did it like that.

Why do you want to decrease the opponent's score? I think it's not a good mechanic.
User dclgamingtr: but if the score does not decrease, the score rises constantly and there is no fear of losing.

User dclgamingtr: When he says "give up" when he is going to lose, points do not decrease.
User insthync: As I try, when I lose the opponent my score decreases.
User dclgamingtr: my score is falling when i lose.
User dclgamingtr: I mean, if I win when I attack. The score does not drop from the loser.

User dclgamingtr: ![image](https://user-images.githubusercontent.com/61230304/75783160-26b05780-5d71-11ea-9df1-6528edf85140.png)


Since I made a leaderboard
User dclgamingtr: If I win when I attack. The score of the loser must drop.
User insthync: No, defenders score should not decrease. Because they are not intended to fight to increase their score.
User dclgamingtr: but it is so in all general games. must lose or everyone just rises. Is it possible for you to do this, there is no point in going online.
User insthync: Give me one example game
User dclgamingtr: Mobile legends adventure , ƒ±dle heroes , RAID shadow legeds
User dclgamingtr: Idle heroes have played for years.
User dclgamingtr: logically, I attacked, lost the war, points are dropped from me.
When I won the war. if i earn points. Why from the loser. points do not fall.
User insthync: Because that loser does not intend to fight with you.
User dclgamingtr: But I attack her, and I win.
User insthync: So you collect your score why you want to decrease other score?
User dclgamingtr: The loser has to drop the score. Otherwise only the score has to go forward.
User dclgamingtr: It is generally unreasonable. The scoring system is only for winning. However, it must fall when I attack. It is so in all the games I play.  This isn't just for this game. Every game with an arena system is like that.
User insthync: No score won't always go forward if the player starts the battle and lose.
User dclgamingtr: but if i win. loser person score does not decrease.
User insthync: Yes that is because they are not intended to fight 
User dclgamingtr: Since he doesn't want to attack. Why am I attacking? I earn.
User insthync: Score can be reset every week or month or season.
User insthync: Can you show me when opponents win your formation and you lose your score?
User dclgamingtr: 
Of course wait
User dclgamingtr: ![image](https://user-images.githubusercontent.com/61230304/75785792-55c8c800-5d75-11ea-9cbf-11bf0d6efe90.png)

I'm going to attack SASA now.
User insthync: No in other games
User dclgamingtr: 
wait I'm downloading
User dclgamingtr: I'm waiting for someone to attack me. I will throw a picture.
---------END---------
114
User ghost: I was banned for some reason and I don't know why.
Can I please get unbanned. 
[Ban ID: mtg-ban=**5eb2f944c761c1-08223996**]
all i said was hi and
if you can read this say 69420
my IGN is: WeDon'tNeedToKnowYourMCName^
if saying that is a ban offense, then I'm sorry.  I was not sure if this was a real chat. I am playing a large modpack and there are weird things in the pack. plus I thought you had to be on a server to chat with others. 
it also says, kicked, and rejoining, but I am not back on.  it is a ban or did I get kicked?
see image for more information
![Screenshot (100)](https://user-images.githubusercontent.com/64923623/81215278-1aca5700-8fa7-11ea-874e-cfd39ad563ed.png)

User Draylynn: Pretty sure you know what that number means, why you chose it and why you did it.
Your attempt to be 'smart' and 'funny', whilst 'trolling', was extremely bad taste and immediately noticed. It was sad, disrespectful and immoral: There are kids in this chatroom, it's a public chatroom.

You're banned for:
Inappropriate Conduct
Filter Evasion
Explicit Content
Wheaton's Law (Baiting 2 other users into blindly re-typing it)

"I don't know why."
"But it's just a random set of numbers I was testing."
"My sibling/cat/dog did it."
"I typo'd."
...Etc etc are not valid excuses.

If you want unbanned, I want you to recite the rules of MineTogether chat, entirely.
You should know them, as you're warned when you first join chat, and if you've lost them, well, good luck finding them, this is your quest now.
User ghost: --Inappropriate--
User ghost: --Inappropriate--
User ghost: --Inappropriate--
User Draylynn: This is exactly why you are banned:
You couldn't display any respect for the rules or the other users in chat, you obviously have little respect or dignity for yourself to act so feral in this issue tracker, you cannot behave and you can't be trusted to act properly in front of minors/general public. Absolutely appalling and disgraceful behaviour.

Learn from this, get yourself sorted out and re-try the appeal in a few months if you really care that much.

Quoting Terms of Service for a Virtual Private Server/Dedicated Server ordering system, is not the rules of MineTogether, and slandering our company will not be tolerated.
**All you had to say was "I forgot them", I would have reminded you with an unban**, all this for a single set of explicit numbers - Folks, this is why the rules are so harsh and why it's such a good effective filter.
---------END---------
115
User awjalan33371: my account has booked cuz i just sign up with your fuking tool 
 
User pashayogi: Hello, sorry for the inconvenience of using your tools, we are not
responsible for the use of our tools!

Pada tanggal Sab, 25 Apr 2020 07:12, awjalan33371 <notifications@github.com>
menulis:

> my account has booked cuz i just sign up with your fuking tool
>
> ‚Äî
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/pashayogi/SETAN-GOLD/issues/1>, or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AMDP5X36Z353RTANOCEY2FDROIMFXANCNFSM4MQPGEFA>
> .
>

---------END---------
117
User stephanthenoob: its shit
User stephanthenoob: its shit
User stephanthenoob: its shit
User stephanthenoob: its shit
User stephanthenoob: its shit
User stephanthenoob: its shit
User stephanthenoob: its shit
User stephanthenoob: its shit
User kuttim: retard
User IamM47Z: its baimless, what you were expecting lol
---------END---------
118
User jenniferz79: Steps to Reproduce
-----------------------
* Create a gauge dotnet project, netstandard 2.0
* Install Nuget package System.Data.SqlClient 
* Try to run the project.

Gauge version: 1.0.8
Commit Hash: 28617ea

Plugins
-------
csharp (0.10.6)
dotnet (0.1.3)
html-report (4.0.10)
java (0.7.4)
js (2.3.8)
reportportal (1.1.0)
screenshot (0.0.1)
xml-report (0.2.3)

Actual Behavior
---------------------
Exceptions thrown


Expected Behaviour
----------------------
As this is important package for sql database access, it should support without error.
User sriv: `Error: rpc error: code = Unknown desc = Exception was thrown by handler.` is a cryptic error message. It's thrown by the framework used. There's a PR #66 that is aimed at bringing out the underlying error.

Now, I suspect `System.Data.Sqlclient` (or one of it's dependency) did not load properly. In my machine it failed and the actual error was:



I am trying to see why this is the case and if it is caused by Gauge. 

If you can install gauge and gauge-dotnet from source, please try with these branches:

- gauge - https://github.com/getgauge/gauge/tree/grpc_error_reporting
- gauge-dotnet - https://github.com/getgauge/gauge-dotnet/tree/grpc_error_reporting
User jenniferz79: @sriv , thanks for reply.
I also failed at loading 'Microsoft.Win32.Registy'. But, I found if I lower the System.Data.Sqlclient nuget version to 4.4.0, I can pass above error. But, it failed at last when loading System.Text.Encoding.CodePages.dll.

I tried to build gauge-dotnet with the link, it failed.

I have not installed .net core 3.0 yet.
User jenniferz79: I compiled the latest one in master, it printed out error as below:

User sriv: Regarding `No such host is known.` error I see that nuget is hitting `http://dsbuild01:8300/` - perhaps you have a local nuget host setup? Would you be able to use `nuget.org` to build?

As for `System.IO.FileLoadException: Could not load file or assembly 'System.Text.Encoding.CodePages, Version=4.1.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a'.` - I am still uncertain if this has anything to do with gauge/gauge-dotnet? 
User jenniferz79: @sriv I found some nuget packages exist in 'Gauge.Dotnet.deps.json', but with lower version. When the same nuget package is referenced in the my gauge project, with higher version, it will throw exception when loading this .dll.
Currently, I am using a workaround: add System.Data.SqlClient and Microsoft.Win32.Registy nuget packages in gauge-dotnet, and re-build and re-package it. It works well. But it means I need to rebuild the gauge-dotnet plugin.
Strongly recommend gauge could add those nuget package by default. Normally System.Data.SqlClient is a must for DB connection.
User sriv: @jenniferz79 - thanks for the investigation. Would you be able to suggest which nuget packages are of lower versions in Gauge.Dotnet? If there isn't a good reason, we'll update the deps.

As for adding System.Data.SqlClient - I understand it is important for users, but I must admit that I am hesitant to add packages as dependencies without actually needing them in Gauge.Dotnet.
User jenniferz79: @sriv, System.Data.SqlClient has several dependence packages.
For example, System.Text.Encoding.CodePages, in dotnet, it is 4.3.0, which is too old. This will cause exception. There are several other dependence packages, they are all basic utilities, but in guage.dotnet, it is using quite old version.
I can understand adding some packages not used by gauge is quite strange. But as it includes many basic utilities packages which is being used by both gauge.dotnet and other projects, it would be better to add it. If not, user has to do the same thing as I am doing. Probably, need to be mentioned in gauge document, as I was stuck on this problem quite long time, and almost want to give up using gauge.
User zabil: > and almost want to give up using gauge.

In the interest of actually fixing this issue. I'd like to point out to our code of conduct. Especially around showing empathy.

https://github.com/getgauge/gauge-dotnet/blob/master/CODE_OF_CONDUCT.md#our-standards

There are two options here
* Help us fix the issue, either with constructive comments or sending a pull request.
* Choose another tool which will meet your project's requirement better.

But please understand that the contributors are trying their best.
User NivedhaSenthil: New version of dotnet (0.1.4) is released with updated references. Please do update and check.
---------END---------
119
User brapper69: Paladin seals still dispellable with the sanctified seals talent on the ret tree
User SUN-Leekie: Cannot reproduce.
We have an automated test passing and I even tried to do it manually.
User DollpartsFTP: I posted this on discord earlier after having my seal spam dispelled by a ganking shadow priest.

The talent should make you immune to seal being dispelled, currently it does nothing.
User SUN-Leekie: Can you try the following and report me the results:
- Having it dispelled alone
- Having it dispelled with multiple other buffs
User DollpartsFTP: Dispellable both ways, with other buffs, and without.
User SUN-Leekie: Closing until further proofs are provided.
User GigilPiq: No need to close, it does bug and the seals gets dispelled. Can confirm this happened multiple times.
Believe us, it's bugged.
User GigilPiq: There is your proof, just duel'd another pal, Used Seal of the Crusader and Judge'd it, then got him to dispell.

Here's combat log : 
![image](https://user-images.githubusercontent.com/59665141/76153714-09142280-60d0-11ea-823f-a961a419d1dd.png)

Here's the talent :

![image](https://user-images.githubusercontent.com/59665141/76153723-22b56a00-60d0-11ea-9bcd-a670a7beb988.png)

User SUN-Leekie: Your proof is a *judgement* dispel not a *seal*.
User GigilPiq: > Your proof is a _judgement_ dispel not a _seal_.

It seems like I have been acting like a retard. Sorry.
The Judgement can be dispelled but the seal on the Paladin should not be.
You are right
User blblblaz: what this talent does is make your Seals "undispellable" if you have 3 points in it, Judgments are not Seals, the debuff you put on your enemy has nothing to do with this talent
User GigilPiq: > what this talent does is make your Seals "undispellable" if you have 3 points in it, Judgments are not Seals, the debuff you put on your enemy has nothing to do with this talent

What the f are you talking about ? Judging SoC applies the debuff *Judgement of the Crusader*. 
User blblblaz: and Judgement of the Crusader is not Seal of the Crusader, this talent relates to the buff that you have before using Judgement
User GigilPiq: > and Judgement of the Crusader is not Seal of the Crusader, this talent relates to the buff that you have before using Judgement

Yes you are right, I went full retard on this one. Sorry.
I don't know why I was sure the Judgement debuff should not be able to be dispelled.
User blblblaz: no worries, most of the time the wording on these things is weird so it's easy to get confused by just reading it in a tooltip
User GigilPiq: I swear I could remember my SoC not being able to be dispelled. I am totally wrong here ?
---------END---------
121
User Sigri44: Is the network is dead ??

No peers and network confirmation down.. But token pump on exchange, WTF ?!

V
User marknl: The network is certainly NOT dead. There are over 3000 nodes, of which 800+ are ENABLED. So I suggest you look at the issue on a local scale.
---------END---------
122
User urherenow: You can see them if you look at the database using Notepad++ on Windows, but not in SQLiteStudio (for example). Provenance-emu was picking up those URLs for some reason, and failed to download box art.

Fix is to run vacuum on the database (to clean it up). I ran it using SQLiteStudio, then deleted cached stuff on my iPAD for Provenance, and the issue was resolved.
User clobber: 1. You all are probably not using the latest version of the database -- this is not my problem.
2. The database *does* get `VACUUM` after a large operation before I upload it.
3. Whether the database has been recently `VACUUM`'d *does not* affect the integrity of the data or SQL queries -- what you are suggesting is complete nonsense.
4. Expect a ban if you absolute idiots continue to spam me with "me toos" from issue https://github.com/Provenance-Emu/Provenance/issues/1314
---------END---------
123
User jakubgs: Colors are horrible for log aggregators, just look at this shit:

User zah: Wouldn't logging with JSON make more sense if the logs end up in ElasticSearch/Kibana?
User kdeme: You can already provide a flag at compile time to disable the colors: `NIM_PARAMS="-d:chronicles_colors:none"`
And we can also change the format to JSON as mentioned by zah. Could also log to file if that is more convenient.
User jakubgs: JSON is nice. Sure.
User zah: To enable JSON, compile the binaries with `NIM_PARAMS="-d:chronicles_sinks=json"`.
User jakubgs: It's nice as a temporary solution, but I wouldn't want to recompile a tool just to change log format.
User arnetheduck: if it helps, think of compiling as running a python script ;)
User jakubgs: It's not the same when i have to re-compile Nim compiler too :D.
User stefantalpalaru: > It's not the same when i have to re-compile Nim compiler too :D.

Use Docker's step-based caching to your advantage, by building the compiler in a separate step: https://github.com/status-im/nim-beacon-chain/blob/4b731b266d89e92d8a22c111f18de578dabcf1a0/docker/Dockerfile#L10
User zah: You are giving docker lessors to the wrong man, @stefantalpalaru :P
User jakubgs: Yeah, I saw that, it would help, I just really dislike the `clone` step there. Which files specifically does the Nim compilation require from the repo? Maybe we could `ADD` just those first for just that.
I'll probably add something like that in my next PR with some other fixes.
User kdeme: Closing this as it is possible at compile time with `make NIM_PARAMS="-d:chronicles_colors:none" wakunode`.
Run time option is rather an issue to be created at chronicles library repo.
User jakubgs: If so then we should use `-d:chronicles_colors:none` by default for Docker images: #26 
---------END---------
124
User sualko: https://github.com/jsxc/jsxc/blob/master/CODE_OF_CONDUCT.md
---------END---------
125
User grizzlyuser: #### Your system information:
TRIfA Version: 1.0.32

App from Google-Play / F-Droid / other: F-Droid

Android OS Version: Doesn't matter

Device: Doesn't matter

#### Please describe your issue in as much detail as possible:
As can be seen from https://github.com/zoff99/ToxAndroidRefImpl/blob/zoff99/dev003/android-refimpl-app/app/build.gradle#L233 , iOS emoji are included in TRIfA. AFAIK, these are proprietary, please see https://github.com/vanniktech/Emoji/issues/433 for details

#### Steps for reproducing this issue:
1. Open TRIfA.
2. Start a chat with any user.
3. Tap emoji icon near the text entry box.
4. Tap any emoji.
5. Send a message.

Expected results:
Emoji that are displayed on steps 3, 4, 5 are only freely licensed (libre) artwork. For example, https://github.com/googlefonts/noto-emoji or any other with similar licensing.

Actual results:
iOS (Apple) proprietary artwork is displayed.

![Screenshot_20200429-104232_TRIfA](https://user-images.githubusercontent.com/45486304/80572895-b720b680-8a07-11ea-8963-24df2eec189a.png)




User WorldCodeCentral: Twemoji by Twitter is released under BY-CA 4.0 (graphics) and MIT (code):
https://twemoji.twitter.com/
https://github.com/twitter/twemoji
preview: https://emojipedia.org/twitter/

Riot-web uses them. The only modern and modern looking emoji I could find with the most options (MIT/BY-CA 4.0). (Interested in alernatives, if anyone knows some.)
User Nokia808: This is really great issue that break the nature of Tox client & making it unsafe ! I think now TRIFA go in same path of Tok-Android !

It is the time to shift from TRIFA to other client, probably aTox ......
User therealmaxmoon: @Nokia808 How does it break the nature of TOX and how does it make it unsafe?

aTox doesn't support audio or video telephony. There are not many open source TOX clients for Android, to be specific... it's 2.
User Nokia808: @therealmaxmoon 
How you know what the CLOUSED source elements containing ??
I removed TRIFA from my device.
Moreover, TRIFA very frequently crash on my Android ! It only work for 1st time that I launch it, then persistently crash till I re-install it again !
Very annoying issue: TRIFA never restore backup of Tox ID created on other devices ! I need to import my already created Tox ID on my qTox but I can not at all !
Developer of TRIFA do not listen to users & their complains, just ignore them or look their issues !
From my side, this is the last replay from me on this repository & I will never open or read or install any thing from it. 

At least aTox developer hear to us & discuss us & he is on the road to add audio/video calls .....
---------END---------
126
User eliavmaman: 
User Poltergeisen: What's up with the hate? Why even bother posting this?
---------END---------
127
User luixiuno: yall scammers, this is not even a malware or jailbreak
User PsychoTea: comedic excellence
User Siguza: @luixiuno imma go out on a limb here and say you don't know enough to properly set this up.  
But if you're on 10.0-10.3.3, you can go to https://totally-not.spyware.lol and run it from there.
---------END---------
128
User ldunzweiler: Revenue.  Fuck you guys.  

---
#### Document Details

‚ö† *Do not edit this section. It is required for docs.microsoft.com ‚ûü GitHub issue linking.*

* ID: 77fb57a0-9af1-8ade-5846-4a44c6e1a702
* Version Independent ID: 24c49cab-e850-df1f-5050-bac2c37c96bd
* Content: [Microsoft Search in Bing and Office 365 ProPlus](https://docs.microsoft.com/en-us/deployoffice/microsoft-search-bing#feedback)
* Content Source: [DeployOffice/microsoft-search-bing.md](https://github.com/MicrosoftDocs/OfficeDocs-DeployOffice/blob/live/DeployOffice/microsoft-search-bing.md)
* Service: **o365-proplus-itpro**
* GitHub Login: @DHB-MSFT
* Microsoft Alias: **danbrown**
User DHB-MSFT: Comments are subject to the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). Locking this conversation because of inappropriate language.
---------END---------
129
User kulfoon: Did you miss [my comment](https://github.com/abp-filters/abp-filters-anti-cv/issues/372#issuecomment-624164660) or what?
User wizmak: @kulfoon You could have pinged me in the original issue, but anyways, this should be fixed, for now.

> Did you miss [my comment](https://github.com/abp-filters/abp-filters-anti-cv/issues/372#issuecomment-624164660) or what?

Well, No & yes. Your suggestion didn't really help, and I wanted to take a look to come up with a solution, had no time back then, then it slipped my mind, good?

Now, I'm not sure if this is the best way to contribute (assuming this is what you want to achieve here), and your expressing is somehow aggravating, please keep this in mind.
User kulfoon: > wizmak : @kulfoon You could have pinged me in the original issue

I did ping both of you, after 4 days, in the same comment, by editing it.

> wizmak : Well, No & yes. Your suggestion didn't really help, and I wanted to take a look to come up with a solution, had no time back then, then it slipped my mind, good?

No problem, I do appreciate, that's why I asked, in case you forgot or missed my comment, to save my time, I wrote a comment and waited 4 days, nothing, then pinged you and waited another day, still nothing, so I just asked what's going on with the issue, good? Just a question, I didn't mean to offend you in anyway, why then taking it so personally...seems you're overreacting. Also you could have leave a comment saying you're working on solution, otherwise how do I know/determine whether you have seen my comment and are working on a solution, or just have missed/forgot my comment.

>  wizmak  : Now, I'm not sure if this is the best way to contribute (assuming this is what you want to achieve here), and your expressing is somehow aggravating, please keep this in mind.

As explained above, I just asked, then you harassing me personally, for nothing, now, I'm not sure if this is the best way to contribute (assuming this is what you want to achieve here), and your expressing is somehow aggravating, please keep this in mind. See yourself got beaten by your own argumentation. Why even making a drama just because someone asked a question... So far, you haven't proven anything wrong with me in my commentary, you only throw general slander because you have imagined something. Can we stop it now? Coz if you're gonna continue your imagination, then I'll have to report you to the github administration for baseless harassment.

---------END---------
130
User grizzlyuser: #### Your system information:
TRIfA Version: 1.0.32

App from Google-Play / F-Droid / other: F-Droid

Android OS Version: Doesn't matter

Device: Doesn't matter

#### Please describe your issue in as much detail as possible:
As can be seen from https://github.com/zoff99/ToxAndroidRefImpl/blob/zoff99/dev003/android-refimpl-app/app/build.gradle#L233 , iOS emoji are included in TRIfA. AFAIK, these are proprietary, please see https://github.com/vanniktech/Emoji/issues/433 for details

#### Steps for reproducing this issue:
1. Open TRIfA.
2. Start a chat with any user.
3. Tap emoji icon near the text entry box.
4. Tap any emoji.
5. Send a message.

Expected results:
Emoji that are displayed on steps 3, 4, 5 are only freely licensed (libre) artwork. For example, https://github.com/googlefonts/noto-emoji or any other with similar licensing.

Actual results:
iOS (Apple) proprietary artwork is displayed.

![Screenshot_20200429-104232_TRIfA](https://user-images.githubusercontent.com/45486304/80572895-b720b680-8a07-11ea-8963-24df2eec189a.png)




User WorldCodeCentral: Twemoji by Twitter is released under BY-CA 4.0 (graphics) and MIT (code):
https://twemoji.twitter.com/
https://github.com/twitter/twemoji
preview: https://emojipedia.org/twitter/

Riot-web uses them. The only modern and modern looking emoji I could find with the most options (MIT/BY-CA 4.0). (Interested in alernatives, if anyone knows some.)
User Nokia808: This is really great issue that break the nature of Tox client & making it unsafe ! I think now TRIFA go in same path of Tok-Android !

It is the time to shift from TRIFA to other client, probably aTox ......
User therealmaxmoon: @Nokia808 How does it break the nature of TOX and how does it make it unsafe?

aTox doesn't support audio or video telephony. There are not many open source TOX clients for Android, to be specific... it's 2.
User Nokia808: @therealmaxmoon 
How you know what the CLOUSED source elements containing ??
I removed TRIFA from my device.
Moreover, TRIFA very frequently crash on my Android ! It only work for 1st time that I launch it, then persistently crash till I re-install it again !
Very annoying issue: TRIFA never restore backup of Tox ID created on other devices ! I need to import my already created Tox ID on my qTox but I can not at all !
Developer of TRIFA do not listen to users & their complains, just ignore them or look their issues !
From my side, this is the last replay from me on this repository & I will never open or read or install any thing from it. 

At least aTox developer hear to us & discuss us & he is on the road to add audio/video calls .....
---------END---------
131
---------END---------
132
User tripulse: The worst quality products are made with JavaScript. Don't ruin the GTK like this please!
User clayrisser: Let's use github issues for more objective comments. I can create a subreddit for react-gtk if you would like a spicier conversation.
User clayrisser: I just created the subreddit called [reactgtk](https://www.reddit.com/r/reactgtk) and created a post for this issue.

https://www.reddit.com/r/reactgtk/comments/grndhx/is_binding_javascript_and_react_to_gtk_ruining_gtk/
User tripulse: Well, this was not even a real issue. Just a bad joke.
User clayrisser: @tripulse glad it's just a joke :)
User Benjozork: programmerhumor discord is leaking
---------END---------
134
User Molikai: Regarding my background (It's aplpicable to understand my viewpoint) I've been in the BDSM world as a dominant for the better part of the last twenty years. Something we're big on? Consent.
And lo and behold - you're silly little waifu system doesn't care about consent.
And yes, I saw the warning on your help page that says it should not be taken as reality!
A) People don't read it.
B) You think that matters when an abusive asshole is messing with someone's head?
terms like 'gaslighting' are popular, but the correct one is psychological manipulation. 

Might I suggest, as a simple little fix - You either a) Make the supposed 'claim key' work (We tested: It doesn't) as a start: and as a fdinally, either an opt-out (That both blocks and removes all claims) or a simple way for the claimed to say 'no'. 
I've alreay decided that those 'silly little 'commands based dom/sub games that don't bother with consent make me never want to visit your server. 
But perhaps for everyone else, given it's freely available, and you don't know who else is using it?
Or under what circumstances?
I mean, Hyptohyetically, we're talking the grooming of underage girls here, if we want some hyperbole. 
Which might not be.
FIX IT.
User Kodehawa: profile claimlock

1. This is overblowing out of proportion something that CAN be disabled, and that you can ignore completely
2. The item description is old and hasn't been updated, once 5.5.5 is out it'll tell you to use profile claimlock instead, which is what locks you out of it.
3. I-I didn't even think BDSM here...
User MrLar: > I've alreay decided that those 'silly little 'commands based dom/sub games that don't bother with consent make me never want to visit your server.

Ah yes because the system that clearly states to not take it serious is a "silly command" "based" on dom/sub. And totally not a command that just exist for people to have fun with with 0 implicit. Quite frankly there is no real need for consent, especially given that you can easily opt-out and for that fact that it implies absolutely nothing whatsoever. It only ever gets meaning if you yourself apply meaning to it, at which point you should probably consider the fact that your own viewpoint might be a little wrong here not our system. 

> I mean, Hyptohyetically, we're talking the grooming of underage girls here, if we want some hyperbole.
Which might not be.

You literally called this an exaggeration yourself, if someone ever goes there it is quite frankly just stupid. Sorry for saying that bluntly. But by no means are we implying anything with this system. However anyone who feels disturbed by it can freely disable it or just stop using the bot. Please do note that I am 99% sure that in actual extreme situations (we are talking real harassment and what not here) we would almost certainly take actions and remove them for you and even elevate the issue to discord.

tl;dr This is just blow out of proportion as hell.
User Molikai:  

The fact you call the possibility 'stupid' just speaks to your innocence. 
Mostly, it suggests your lack of imagination. 
But thank you for answering my question regarding yon claim key!
I'll be sure to take note of this attitude in future.
     On Thursday, 21 May 2020, 20:52:22 BST, Lars <notifications@github.com> wrote:  
 
 



I've alreay decided that those 'silly little 'commands based dom/sub games that don't bother with consent make me never want to visit your server.


Ah yes because the system that clearly states to not take it serious is a "silly command" "based" on dom/sub. And totally not a command that just exist for people to have fun with with 0 implicit. Quite frankly there is no real need for consent, especially given that you can easily opt-out and for that fact that it implies absolutely nothing whatsoever. It only ever gets meaning if you yourself apply meaning to it, at which point you should probably consider the fact that your own viewpoint might be a little wrong here not our system.


I mean, Hyptohyetically, we're talking the grooming of underage girls here, if we want some hyperbole.
Which might not be.


You literally called this an exaggeration yourself, if someone ever goes there it is quite frankly just stupid. Sorry for saying that bluntly. But by no means are we implying anything with this system. However anyone who feels disturbed by it can freely disable it or just stop using the bot. Please do note that I am 99% sure that in actual extreme situations (we are talking real harassment and what not here) we would almost certainly take actions and remove them for you and even elevate the issue to discord.

tl;dr This is just blow out of proportion as hell.

‚Äî
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
  
User Kodehawa: "Mostly, it suggests your lack of imagination."

That sounds 20 shades of wrong, lol.

Anyway, there's an opt-out system in place for 5.5.5. I don't like the fact I *had* to add it, but it does exactly what you suggested, makes you unclaimeable and removes all of the claims on you.
User Kodehawa: Also this will mean there's now two ways to opt-out: A claim key applied to your profile with profile claimlock, which only locks you out of new claims and lets you use the waifu system as usual otherwise, and opting out completely, which removes all of the claims on you, but you can't use the system anymore.

Keep in mind that if someone is using the system in bad faith, we can also take action ourselves without you needing to opt-out.
User Devoxin: imagine claiming to be part of the bdsm world for at least 20 years and posting an issue like some snowflake

you spelled submissive wrong btw
User Molikai:  

That's good to know, thank you.

And that requires you to know someone is using it in bad faith, alas.

Regards,
Douglas
     On Thursday, 21 May 2020, 21:22:09 BST, Kodehawa <notifications@github.com> wrote:  
 
 


Also this will mean there's now two ways to opt-out: A claim key applied to your profile with profile claimlock, which only locks you out of new claims and lets you use the waifu system as usual otherwise, and opting out completely, which removes all of the claims on you, but you can't use the system anymore.

Keep in mind that if someone is using the system in bad faith, we can also take action ourselves without you needing to opt-out.

‚Äî
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
  
User Kodehawa: It just requires the person to report it accordingly. If they wanna do it without reporting, they can opt-out as said.
---------END---------
135
User 5HT2: The damage dealt by the sword is the same each time if you have delay mode in Aura turned on...
User 5HT2: Unless you mean changing the color of health in nametags based on health?
User 5HT2: Ah, well 'predicting' it based on what other players are doing is too unpredictable, ironically. I've tried doing this in the past and it wouldn't be feasible without a *huge* dataset and loads of computing power, which I don't think anyone would expend on a block game mod 
User 5HT2: Yes, the seeing how often they hit is extremely unpredictable without a large dataset.

User 5HT2: Tps Sync can't "check how often they hit you", and as I said reliably predicting when they're going to hit you is practically impossible in real time. If I added it people would just shit on it and call it a cheap gimmick, as yes, it wouldn't be reliable.
User 5HT2: As for how long actions take, you can just do `(20.0 / currentTpsInTicks) * timeToDoAction)`, that part isn't difficult and you shouldn't measure it manually.
---------END---------
136
User ghost: [Continuation of 862 because Adamantcheese keeps locking for no reason](https://github.com/Adamantcheese/Kuroba/issues/862)

> Removing a post removes it from display.

No shit. You still haven't explained why there's Hide AND Remove. They are both the exact same thing, the only difference is that two extra clicks are required to "unremove" a post. 

> Filter image hash isn't an option on posts without images. You must be on an older version.

I'm on the latest release build (v4.12.0). It is an option on posts without images on the latest version.

> Share is as @clawyf explained.

You still haven't explained why we can copy the image's link and not the post's link. If you want to play this game and persist, it's either both ways, or none. 

> Info provides additional info that isn't presented in the post cell. 

The only thing Info provides is the resolution of the picture inside a post and nothing else. The filename, size and time of date can already be seen from the post itself, which makes Info completely useless.

Just accept the truth, you have so much garbage in the context menu and it needs to get cleaned up. If you got rid of Remove and Info, nobody would bat an eye.
User Astridchan: >I'm on the latest release build (v4.12.0). It is an option on posts without images on the latest version.

It's fixed in the dev builds, see 3c79831.

>You still haven't explained why we can copy the image's link and not the post's link.

But you _can_ copy the post link, post menu -> share -> copy link.

>The only thing Info provides is the resolution of the picture inside a post and nothing else. 

The info menu is useful to view the exact post date 
and time when you have relative time (eg. "posted 1hr ago") enabled.
---------END---------
137
User Hivetyrant36: **File/Catalogue:** Warhammer 40000 1.4.19

**BattleScribe version:** 2.03.19

**Platform:** Windows/iOS/Android/Mac/Linux

**Dropbox:** No

**Description:** When I hover over the Household choices, Chapters, or hive fleet, the pop up only shows what the rule is called and that it was made with battle scribe. https://imgur.com/8zEyjbq

User alphalas: That‚Äôs likely because the flavor text was not added by the author. Read your codex.
User Hivetyrant36: That's why I posted here. Is this not the group that made the files for 40k? Seems a bit prissy.
User Hivetyrant36: Like just add the flavor text or show me how to or something. Don't just fucking close people's tickets they would like some help on. Why are all warhammer fans such unrelenting narcissists?
User Hivetyrant36: Like you can literally go fuck yourself
User Hivetyrant36: <!--
/* Font Definitions */
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:"Caslon Antique";
	panose-1:2 0 5 3 0 0 0 0 0 0;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
.MsoChpDefault
	{mso-style-type:export-only;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
-->Jon can go fuck himself, this isn‚Äôt resolved. At least say ‚Äúok yeah we missed that one.‚Äù Fuckers¬†Will ‚Äì HiveTyrant36¬†¬†From: Jon KissingerSent: Wednesday, May 13, 2020 8:20 PMTo: BSData/wh40kCc: Hivetyrant36; AuthorSubject: Re: [BSData/wh40k] Chapter/House/Swarm rules not displaying (#7120)¬†Closed #7120.‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.¬†
User alphalas: Because the data files are specifically made to supplement your codex, not replace it. We the data editors still expect you the user to have your books so you can reference them and understand the way your army works
User DrTobogganMD: @Hivetyrant36 your issue is best brought up to the developer, not the data team. The data is indeed in the file and you would see that once you choose to view roster. This thread is being locked because there is no further need for discussion. 
---------END---------
139
User scythe: - [x] I have read the [MiKTeX Contributing Guidelines](https://github.com/MiKTeX/miktex/blob/master/CONTRIBUTING.md)

(Windows 10, for reference, if it matters.)

I have a paper due tomorrow. 

I thought "hey, let's use the *American Physical Society* LaTeX Class!". I haven't used it before, but it's just a LaTeX class, what could possibly go wrong? So I loaded up MiKTeX and went to "Packages", searched for ReVTeX and installed it.

Oh, by the way, you have updates available, says the console. So I click on the "Updates" tab and try to install the updates. Some text appears in the pop-up window. After a while the window stops responding. Won't listen to clicks or the keyboard. Eventually I gave up and closed it.

Now the trouble begins. I go into TeXStudio and try to compile the template. Whoops, you're missing `textcase`. Odd. Installation failed.

I tried to compile a document I had compiled yesterday. It won't compile either.

I opened the MiKTeX console. "Check for updates" doesn't work: HTTP 401. No packages can be installed: HTTP 401. The error message isn't very helpful, aside from saying "HTTP 401 (Unauthorized)".

I have now spent *three hours* trying desperately to compile *anything*. Did I mention I have a paper due tomorrow?

So I gave up, uninstalled MiKTeX, and am now in the process of reinstalling it and -- *hopefully* -- actually being able to use LaTeX for this paper.

The purpose of this bug report is the following: **Please warn your users before you suggest they take actions that may break their system.** 

If I had known that running "Update All" might fail and *break absolutely everything*, I would never have tried to do it in the middle of writing a term paper. Obviously.

Which, as I mentioned, is due *tomorrow*.
---------END---------
141
User spirulin: <!--
By filing an Issue, you are expected to comply with the elementary code of conduct: https://elementary.io/code-of-conduct

Please note that this tracker is only for bugs and feature requests. Please try these locations if you have a question or comment:

  https://elementaryos.stackexchange.com/
  https://www.reddit.com/r/elementaryos/

Please read and follow these tips:
https://elementary.io/docs/code/reference#proposing-design-changes

Lastly, be sure to preview your issue before saving. Thanks!
-->

## Prerequisites
- [x] I have searched open and closed issues for duplicates.

## Feature
**Is your feature request related to a problem? Please describe.**

The problem is your team forcing us to use the OS the way you want us to use it although it makes it 1000000 times harder to use it your way, than what would be convenient for us.
Keyboard shortcuts are not faster and will never be faster than muscle memory and mouse! 
Try using keyboard shortcuts in the dark or when you are not over your keyboard and looking at the keys!
8 of 10 times you will press something else! So what's faster? Me pointing with the mouse and clicking the button, or you trying to find 2 or 3 keys?
Or me going with my mouse to the bottom, waiting for Plank to show up because it auto hides when there is an open window, clicking on the app/program and this way minimizing it?
Don't you really see how ridiculous is what's written in the Blog post "Why there is no minimize button".
Because ONE person thinks it's better doesn't give the right to make a thousand or more people using it that way, when there is a simple solution. Especially on Linux where customization is so easy to be added. 
Elementary tweak solves the problem but it's unsecure to add PPA that is not checked by Elementary team. The same reason you are showing the Warning window for Non-cureted apps/program in the App Center! So if you care about our security so much why 99% of your users have to add a PPA that one day can put them at security risk? Ship your OS the way you want by default but give us the option to make it more friendly for our use.

**Describe the solution you'd like**
In the setting simple option to turn on Minimize and Maximize buttons.

**Existing work**
Elementary tweaks makes it better but adding someone's PPA is dangerous even if it's on GitHub or whatever. You never know what actually is being installed or how it will change in the future.

**Describe alternatives you've considered**
Elementary tweaks included out of the box but it was denied here.

**Additional context**
For reference: https://www.reddit.com/r/elementaryos/comments/gdbo1k/tried_using_elementary_as_intended_without_any/
Most of the people who use your OS are installing tweaks to solve this problem. And most of them want it out of the box as an option. So maybe it's time to stop trying to be Apple or Windows and forcing people to use something the way YOU want it and give a simple option to make it the way people want it. It's not a major change or something. Just a simple solution.
User lewisgoddard: I guess the relevant blog links are:
- [What's Up With Window Controls?](https://blog.elementary.io/whats-up-with-window-controls/)
- [What's (Still) Up With Window Controls?](https://blog.elementary.io/what-still-up-with-window-controls/)
User spirulin: > I guess the relevant blog links are:
> 
> * [What's Up With Window Controls?](https://blog.elementary.io/whats-up-with-window-controls/)
> * [What's (Still) Up With Window Controls?](https://blog.elementary.io/what-still-up-with-window-controls/)

**Note: this post was written over two years ago. The information below may be outdated.**
On the top of the articles.
And actually more than 7 years ago. And the information was never relevant. I think Daniel was too young when he wrote it and thought he may be the next Inventor or something and changing people's lives and habits or whatever. Hope now when he is a grown-up person his attitude is different and he realized that it won't be the next big thing and it's better to make your users happy than being so self-absorbed and forcing them using it the way you want them to use it.
User cassidyjames: If you feel like you want/need minimize, there are still ways to do so on elementary OS today (clicking in the dock, using keyboard shortcuts, or using the context menu in the titlebar). But the ship has sailed on adding a "minimize" button to titlebars because apps designed for elementary OS don't need it, and we don't cater to non-native apps.
User ghost: > 
> 
> If you feel like you want/need minimize, there are still ways to do so on elementary OS today (clicking in the dock, using keyboard shortcuts, or using the context menu in the titlebar). But the ship has sailed on adding a "minimize" button to titlebars because apps designed for elementary OS don't need it, and we don't cater to non-native apps.

If you don't cater to non-native apps, you should atleast actually OFFER Apps that fit with your ideas
BUT YOU EFFING DONT.
Talk about being arrogant asshats.
Edit: I'm not deleting this. If you can't handle a few harsh comments, you should actually just cloister yourself. My comment has valid criticism you're choosing to ignore. Again.. Arrogant asshat.
User Julian-MJK: A majority of your users almost certainly rely heavily on "non-native" apps, this is Linux after all, and the native apps don't fill the needs of all users.

The addition of minimize button could significantly improve the workflow and ease-of-use for many users, is the fact, without at all diverging from the design principles & philosophies that make Elementary OS great.
User alextrayanov: > If you feel like you want/need minimize, there are still ways to do so on elementary OS today (clicking in the dock, using keyboard shortcuts, or using the context menu in the titlebar). But the ship has sailed on adding a "minimize" button to titlebars because apps designed for elementary OS don't need it, and we don't cater to non-native apps.

Most of the users install third party applications immediately. You native apps can't compare to VLC and real music player. None of them can. 
Not to mention you Browsers that sucks in every possible way and I would be surprised if even 1% of your users are using it.
So you are very wrong.
And as the OP described clicking on in the dock requires more steps and time than with Minimize button. Shortcuts too. So don't give such a dumb suggestions that don't work better than clicking dedicated button with a mouse.
Your and your team attitude is very arrogant.
But when asking for money to support you or your projects you are one of the nicest people.
"We want to improve people's experience".
What a bullshit. If you want to improve the experience you would add a SINGLE line of code and enable the Minimize and Maximize button even if it's in the Settings and your OS ships without them enabled which will change nothing for you. Whoever wants to use it the way you want them to use it will do so. For the rest of the people they will enable them without installing third-party tweaks (where most new to Linux will have no idea about) and enjoy the OS.
User jantari: If you don't cater to non-native apps why did you add flatpak-integration to the appcenter? lol, if you just personally hate minimize buttons so much you don't want them in the OS it's easy enough to admit - but that sounds like a weird excuse
User Gabriel-p: @cassidyjames you should probably just close this thread to comments. Nothing good will come out of this.
User alextrayanov: > @cassidyjames you should probably just close this thread to comments. Nothing good will come out of this.

Cut the free speech?
Maybe you should close your account?
Something good will come when they realize that their way is not the right way.
Nothing good comes from people like you who get what they give them and have no personal opinion or are scared to talk about things they don't like or agree with.
User cassidyjames: Thanks for the feedback, but this is an issue tracker‚Äînot a discussion forum. It wastes everyone's time to notify developers and designers of new comments, especially when they're extremely disruptive, abusive, and in violation of the [Code of Conduct](https://elementary.io/code-of-conduct). Since there is no action to be taken here, I'm locking this from further discussion. Feel free to discuss this in a more appropriate location.
User danirabbit: I'd like to be really clear that our policy is not to lock threads where there are dissenting opinions. Respectful debate is healthy and important. But when you start calling names and being disrespectful and disregarding the code of conduct, the discussion becomes unproductive. This kind of destructive behavior is not tolerated in our community. 
---------END---------
142
User BTCCRDZ: why use BSV...
User BTCCRDZ: Use BTC
User sketron1: BSV is total trash
User yakitorifoodie: developer received funds from BSV to take this step. 
User sketron1: Well I'll be switching to another wallet now...
User yakitorifoodie: @sketron1  you don't have to do that.  Theres a fork at https://github.com/spesmilo/electrumx
---------END---------
143
User ghost: I file an issue, maintainers close, reopen, again close - whilst ignoring the essence of the issue.

#2610

I insist: 

"multi-platform" contains macOS. If not, then you need to clarify.

i gave a wording example. one dev adopted it, two others felt the need to continue ignoring macOS existence (and mine btw., but that's not the main problem).




User jhand2: Open source software development is an inherently cooperative process. It is not enough to believe that your way is the right way, you must convince someone else that it is the right way.

I have heard your concern and I understand you have an opinion, but I don‚Äôt agree with the solution and so far nobody else has agreed with it either. I am happy that you want to contribute and I hope you continue to do so, but you should know that this is simply part of the process.

To address this specific issue, it is my opinion (shared by other commenters and contributors) that the documentation clearly states which operating systems the OE SDK supports. I‚Äôll leave this issue open for now for others to weigh in.
User ghost: > Open source software development is an inherently cooperative process. It is not enough to believe that your way is the right way, you must convince someone else that it is the right way.

Please spare this kind of talk for some juniors or internals.

>and so far nobody else has agreed with it either.

At least stop spreading misinformation.  My suggestion was adopted, until you intervened

https://github.com/openenclave/openenclave/issues/2610#issuecomment-594272005

>I understand you have an opinion

nah, you don't understand, thats the problem.

it is not my opinion, but a fact, but i'll not state it again.

> I‚Äôll leave this issue open for now for others to weigh in.

i don't care about this issue here, i care that you realize your rudeness, and reopen the issue in question, which is #2610.

listening to external observers prevents you from making insider errors. opinion? or again a simple fact?


User andschwa: Hi @lazaridiscom, as a member of Open Enclave's Community Governance Committee, I need to inform you that we have found your language and tone to be in violation of our stated [Code of Conduct](https://github.com/openenclave/openenclave/blob/master/docs/CodeOfConduct.md). Specifically, the above comment (among other interactions you have thus far made) is unwelcoming, insulting, and overall inappropriate for this community. This is an official warning to adhere to our Code of Conduct, and any further violations will result in escalated action, including but not limited to being banned from this project.

We would like to encourage all individuals to be able to participate in our community. To that end, our top priority is to create a positive environment. We encourage everyone to be respectful and use welcoming and inclusive language. We hope that you will realize the necessity of this, and would be happy to continue working with you if you can adjust your interactions appropriately.

Thank you,

Andy Schwartzmeyer on behalf of the Open Enclave Community Governance Committee
User ghost: ## Essence
"See, no CoC protects an freethinking-individual against mainstream masses, they stick always together and hide behind their (misinterpretd, by majority vote) CoCs."
source: https://github.com/confidential-computing/governance/issues/14#issuecomment-595817732

## The "CoC-Nanny" Intervenes
> and any further violations will result in escalated action, including but not limited to being banned from this project.

"but not limited", i think that is the mot interesting point:

**What else, beyond banning me, will you do?** 


> I need to inform you that we have found your language and tone to be in violation of our stated Code of Conduct. 

I need to inform you that you  people (the "we", the accusers-judges-executioners in one entity) have lost contact to reality.

**My "language and tone" is nothing special, and is part of my individual style of expression, especially when I am in a condition of "super-flow" and exposed to any form of inconsistency.**

**I will neither change my language, nor my tone or style. Both, language and tone, are perfectly valid, given the circumstances. I will remain myself, and will repel this attack to my individuality.**

## Counter

I found your processes, tone and language: discriminating, rude, unprofessional. It lacks focusing on the issues at hand (technical, terminology).

## The CoC-Fascism

Be aware that I have at this point no way to defend myself, as the construct "CoC and its application", is a one-way governance construct. One can easily label this as "CoC-Fascism" (so, now, you geniuses, did i just call you "fascists", or is this (... fill this in ...)

## Microsoft

It really looked like Microsoft is changing, and I really believe this sympathetic new CEO does well. But then, Open-Source has become less and less about freedom, even entities like Linux-Foundation behave like IT-fascists.

## Note to Readers 

Remember what this is all about. Essentially, a **Microsoft/Linux** governed project redefines "Multi-Platform" to be "Windows and Linux", ignoring 'macOS'.

I insist here, so Microsoft does not "loose face" (I don't care about Linux/Linux Foundation, they are damaged to no repair anyways).

Giving up: https://github.com/openenclave/openenclave/issues/2610#issuecomment-596048628

.
User ghost: Please feel free to close this issue.
User andschwa: On behalf of the Open Enclave Community Governance Committee, since the user @lazaridiscom violated our Code of Conduct after a formal warning and clearly stated refusal to abide by said Code of Conduct, the user has now been banned until further notice.
---------END---------
144
User federicomartinlara1976: In the source code, after run autogen.sh and configure shows the error:

config.status: error: cannot find input file: `Makefile.in'

And the command make shows:

make: *** No se especific√≥ ning√∫n objetivo y no se encontr√≥ ning√∫n makefile.  Alto.

This software is not well reviewed and is useless. There is no clear manual on how to make the application, which is more serious.
User tbo47: Did you look at this document http://wiki.amule.org/wiki/Compile ?
User federicomartinlara1976: Yes, and this NOT WORKING in Ubuntu 20.04
User gonosztopi: Can you please describe us _in detail_ what you did to understand why it fails for you?
User Vollstrecker: autogen.sh didn't generate Makefile.in because of missing autopoint.
---------END---------
146
User Adramelramalech: I have said this before and I am saying this again, how is any one using this? It does not work at all. No matter what you do you will always get an error screen, it always crashes, it refuses to accept any user made settings and discards user settings and uses it's own crappy settings. I have no idea how any one is using this, I can not even encode one movie, do a batch? Forgot about it, not possible. This has to be the worse program ever built by man, seriously. And Then finally after fighting with this program to batch encode it finally ran but then it decided to crash and quit on me because one file name was too long. Okay who cares, move on, do the next one, why crash and burn over it? 
Features request? Remove the stupid 1 pass at 7,000 default settings. Why is it there? stupid. If a file name is too long, move on and lastly, make this program work, just for once.
User Adramelramalech: Just restarted the whole thing over, it gave a dumb error and it just quit and crashed on me, waste of time.
User Dendraspis: > I have no idea how any one is using this, I can not even encode one movie,...

Seems like you've figured out what's the reason.  üòÑ 
I would recommend you use one of those two forums to get support/help instead of spamming the issue tracker.  ü§î 
User Adramelramalech: What? what ever, you make no sense. Worlds worst program and i will spam where the fuck i want
User Dendraspis: At least we can now be sure why it doesn't work for you.  üòÑ 
User ghost: @Dendraspis don't bother, this user has already made the same input a few months ago, now he returned. Same content, same language.

@Adramelramalech there are many users using this program. So stop spamming, and get OUT.

@stax76 Please report this user or block him. He is the same author of the threads https://github.com/staxrip/staxrip/issues/109 and https://github.com/staxrip/staxrip/issues/101 . He changed his name afterwards or unregistered.
He has already insulted this software and you before.
You don't need anyone remind you how grateful we all are of your excellent work !!
User stax76: I've blocked this user, hopeless case, whatever his talent is, it's not technical or social, unbelievable.
---------END---------
147
User ralyodio: 
User acelaya: Closing this, as it's impossible to reproduce with such little information.

Please, on future issues, make sure to fill in all the information requested in the issue template. It's there for a reason.
---------END---------
148
User ndorigatti: https://github.com/immuni-app/immuni-app-android/blob/8c4739b471754977ea42b5cc2090cd28804a141f/app/src/main/AndroidManifest.xml#L34

It is really ugly to see app locked in portrait in 2020... with livedata, viewmodels and SavedStateViewModel!

User powext: How would be this a priority, the app must be ready to be shipped in a few days and basically no one uses the smartphone in landscape mode except for watching media.
User ndorigatti: not a priority, obviously, but an enhancement to consider.
User rscano: > How would be this a priority, the app must be ready to be shipped in a few days and basically no one uses the smartphone in landscape mode except for watching media.

This is an accessibility requirement. Mandatory. Not an optional.
User medomatto: There‚Äôs no need for pleasure and entertainment enhancements. This app has just to simply work. 
User rscano: > There‚Äôs no need for pleasure and entertainment enhancements. This app has just to simply work.

This is an accessibility requirement. Required by law. Mandatory. Here we are not play or develop a game but one mobile app that must confirm to en 301549 standard that relay on wcag 2.1

Success Criterion 1.3.4 Orientation (Level AA)
Content does not restrict its view and operation to a single display orientation, such as portrait or landscape, unless a specific display orientation is essential.
User powext: > > There‚Äôs no need for pleasure and entertainment enhancements. This app has just to simply work.
> 
> This is an accessibility requirement. Required by law. Mandatory. Here we are not play or develop a game but one mobile app that must confirm to en 301549 standard that relay on wcag 2.1
> 
> Success Criterion 1.3.4 Orientation (Level AA)
> Content does not restrict its view and operation to a single display orientation, such as portrait or landscape, unless a specific display orientation is essential.

Please, provide a source with all the info.
User rscano: > > > There‚Äôs no need for pleasure and entertainment enhancements. This app has just to simply work.
> > 
> > 
> > This is an accessibility requirement. Required by law. Mandatory. Here we are not play or develop a game but one mobile app that must confirm to en 301549 standard that relay on wcag 2.1
> > Success Criterion 1.3.4 Orientation (Level AA)
> > Content does not restrict its view and operation to a single display orientation, such as portrait or landscape, unless a specific display orientation is essential.
> 
> Please, provide a source with all the info.

Never hear about wcag? International standards? En 301549? Italian accessibility law? A baseline for developers. Use Google.
User powext: @rscano I'm sorry for your embarrassment, seeing the [Linee guida of 2019](https://trasparenza.agid.gov.it/moduli/downloadFile.php?file=oggetto_allegati/19356907350O__OLinee+Guida+Accessibilit%E0+versione+IR.pdf):

> 1. comma 1. Le disposizioni del presente decreto relative ai siti web e alle applicazioni mobili, ad eccezione di
> quanto disposto dall‚Äôarticolo 11, comma 1, lettera a), della legge n. 4 del 2004, come sostituito dall'articolo 1,
> comma 10, del presente decreto, limitatamente ai siti web e alle applicazioni mobili, si applicano come segue:
> a. ai siti web non pubblicati prima del 23 settembre 2018: a decorrere dal 23 settembre 2019;
> b. ai siti web non contemplati dalla lettera a): a decorrere dal 23 settembre 2020;
> **c. alle applicazioni mobili: a decorrere dal 23 giugno 2021**

Again, I'm so sorry for your embarrassment, these are the baselines for developers, next time use Google. 


User rscano: I know them due i have develop them. But you don't know about law 67/2006. And in this case people that are discriminated receive money for discrimination. A lot of money. Also Trenitalia made this mistake but then they have fixed. Is a possible damage of image of company work without apply international standards. And this can be the case. And you are saying that actually they are no able to make this stupid interface accessible? Need to wait 2021? Great. So I will call you and them developer in 2021. For now for me you and them are bad developers. Hope is your personal position, not the one of the app developers otherwise could be interesting to publish this in newspapers. Baseline for real developers is en 301549. And is full applicable without problem. So you have time for study and fix your accessibility incompetence. Use google. Cheers.
User powext: I agree with you that this standard is very important and it should be respected by Immuni shortly even if the team is not obliged. 
I also admit that I never thought about Accessibility in my short career, it has never been mentioned in my entire educational process (ITIS + Computer Science Bachelor) and this is a bad thing. 
So I thank you for this, I'm sorry for the friction between us, have a good evening.
User gvdr: @immuniopensource this is a bug, as it limits accessibility, and it is potentially discriminatory. Please relabel the issue.
User Igaryu: > How would be this a priority, the app must be ready to be shipped in a few days and basically _**no one uses the smartphone in landscape mode except for watching media.**_

This is not correct: As example, if possible, I'd like to read FAQ in landscape mode on a mobile phone device!

J.C.


User rscano: > > How would be this a priority, the app must be ready to be shipped in a few days and basically _**no one uses the smartphone in landscape mode except for watching media.**_
> 
> This is not correct: As example, if possible, I'd like to read FAQ in landscape mode on a mobile phone device!
> 
> J.C.

Also, there are people with disabilities that cannot rotate mobile phone and prefer to have landscape mode.
User ndorigatti: @all I didn't want to start this "fight", i just wanted to note that nowadays it's not too difficult as before, given also the quality of libraries and architecture used in this app (and there also designers).
After that, the developers of this app have made choices and decide how and what to implement. 
Thank you for at least considering the feedback, and I'm really sorry for those 27 (at the moment) that downvoted my issue, laws or not, i find that allowing rotation is something we should give to users.
Just my 2cents.
User stale[bot]: This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.

---------END---------
149
User VitRom: Subj?
User stefankueng: here are the very old versions:
https://sourceforge.net/projects/grepwin/files/Archive/

but I don't know which was the latest to support XP.

And as a hint to you that you should **not use XP anymore under any circumstances**, I'll leave it up to you to figure that out yourself.
Seriously: XP???
User VitRom: > I don't know which was the latest to support XP

A bit strange to hear this from **the author** :rofl: 
Are there any signs to distinguish between the supporting or not versions? A build environment version shift for example or something? Dependency change?

> you should **not use XP anymore under any circumstances**

:rofl: "Don't tell me what to do and I'll not tell You where to f-off" (proverb)
I use **tools** regardless of a fashion trends or a marketing bullshit. I still even use (pls don't tell anybody!) DOS when it's suitable.
User stefankueng: XP is not supported anymore for a long, long time. If you're still using it then you're a security risk not just for you but for everybody else: your PC most likely is part of a botnet now because of the many, many security issues that aren't fixed anymore.
So yes, I take the liberty to tell you to not use XP anymore, no matter how many f-words you reply with.
And yes: that means you won't get any help for me regarding that issue. I will not encourage the use of XP.
User VitRom: Thank for your kindly assistance :rofl:
I have to spend some time and to do a binary search through releases, nothing scary.

And just to make some little things clear.
BTW it's so funny that your Anathema didn't touched the DOS about which I've said. May be you just knows nothing about a full tcp/ip stack for this OS? But it's exist. And works. And is used. And Win98/ME with KernelEx too. Well, doesn't matter.

Your grepWin is good, seems you're a good programmer, but as a sysadmin/engineer you're... er... well, "not so good" :smile: Trust the mine quarter century long experience in these areas. Somebody uses a tools that is and mind to make them works right, somebody replaces thoughts with a Faith -- in a usefulness of official updates, in a miriads of bots spawning around every corner (BTW I didn't use antiviruses about to 7 years) etc. etc.

But because it's a Faith I'm undisputably accept your opinion.
WBR!
User stefankueng: haven't heard one good argument about why you still use XP, no reason given why you can't use something that's still supported.
Instead, you dismiss arguments as faith so you don't have to deal with them.

please disconnect yourself from the internet...
---------END---------
150
User nixusminimax: <!-- READ THIS FIRST:
- If you need additional help with this template please refer to https://www.home-assistant.io/help/reporting_issues/
- Make sure you are running the latest version of Home Assistant before reporting an issue: https://github.com/home-assistant/home-assistant/releases
- Do not report issues for components here, plaese refer to https://github.com/home-assistant/home-assistant/issues
- This is for bugs only. Feature and enhancement requests should go in our community forum: https://community.home-assistant.io/c/feature-requests
- Provide as many details as possible. Paste logs, configuration sample and code into the backticks. Do not delete any text from this template!
- If you have a problem with a Add-on, make a issue on there repository.
-->

**Home Assistant release with the issue:**
<!--
- Frontend -> Developer tools -> Info
- Or use this command: hass --version
-->

**Operating environment (HassOS/Generic):**
<!--
Please provide details about your environment.
-->

**Supervisor logs:**
<!--
- Frontend -> Hass.io -> System
- Or use this command: hassio su logs
-->


**Description of problem:**


User nixusminimax: ?
User frenck: This violates our Code of Conduct:

<https://www.home-assistant.io/code_of_conduct/>
---------END---------
151
User asottile: :wave: I'd like to organize the next release of pyflakes

I'll be pinning this issue and collecting feedback on things that should end up in this release

Due to the number of contributions that have gone into `pyflakes` since the last release, I'm planning to do this as `2.2.0a1` first (an alpha pre-release) -- this should hopefully expand the number of people trying the release (though I suspect not all that many will use the alpha still)

___

How you can help:

- try out the latest master on your repositories: `pip install --force-reinstall git+https://github.com/pycqa/pyflakes`
- review [open PRs](https://github.com/PyCQA/pyflakes/pulls)
User asottile: I'd especially like to see the following land before release: #517, #516, #512, #511 -- though reviewing open PRs helps us a ton!  (pyflakes operates with dual-signoff)
User psandana: Please add #448
User asottile: @psandana that's already in master so of course it'll be included
User asmeurer: What is the general release cadence for pyflakes and what is the timeline for getting things into this release? 
User asottile: > What is the general release cadence for pyflakes and what is the timeline for getting things into this release?

- there hasn't really been a release cadence -- on average pyflakes releases a minor release ~about once a year
- there isn't really an urgency to releasing this so no deadlines currently (though I do want to at least see those above changes land before we cut one)
User Tinche:  is incorrect, it should be 
User asottile: derp, thanks -- pip on the brain ü§¶‚Äç‚ôÇ 
User asottile: I've put together a changelog for 2.2.0a1 in #524 
User samuelcolvin: Would be great to get this released ASAP.

> there isn't really an urgency to releasing this

I disagree. pyflakes is currently incompatible with (arguably) the number 1 headline feature of the latest version python, that was fixed in #457 8 (!!!) months ago. 

AFAIK you currently have the options of disabling linting or not using walrus operators (or installing pyflakes and pycodestyle from source which is less than optimal). 

I therefore think a release is very urgent.

---

Sorry to nag and sound negative, pyflakes is great and I know how thankless maintaining OS libraries like this can be. Thank you all for your hard work.
User samuelcolvin: Please could someone explain why my comment was marked as "off-topic"?

You may disagree with it, but it is absolutely not off-topic - it relates directly to the the subject of this issue.
User asottile: it's spam, please use the reactions (+1/etc.) -- there's also an example of how to upgrade to the latest git version in the original post so you're not blocked
User asmeurer: @asottile it's *very* disappointing to see this sort of behavior on a project this important. You and the other maintainers should strongly consider adding a code of conduct to this repo and following it. 
User asottile: @asmeurer please clarify which behaviour is disappointing, we're trying to make a release as fast as possible and comments "release faster" are not helpful to getting that done.  everything is in place within my power to get that out as fast as possible and we're all currently waiting. 
User asmeurer: Censoring comments, calling things "spam" and "off topic" when they aren't, and being rude in general. @samuelcolvin even went out of his way to thank you for your work. 

I'm hopeful this is just one of those "you're having a bad day" situations. I would suggest stepping back and revisiting this tomorrow when you can get a fresh look at what happened here. 90% of the time this sort of thing can be resolved amicably simply by taking some time to let things cool off.
User asottile: alright well the whole reason I made this issue here was to stop the "please make a release comments" so that people can come to a place and see exactly what the status of that is.

[claiming that a release is urgent and nagging](https://github.com/PyCQA/pyflakes/issues/520#issuecomment-603997743) (as the sorry indicates) is precisely what I wanted to prevent by making this issue :(  commenting to that effect does not help the situation and it doesn't help anyone make the release faster.  

there is no censorship -- or at least that was not the intention.  I hid the comments to keep the discussion about the release (I didn't edit or delete them -- I agree that would be censorship!)

 when github added the reactions feature I think that's one of the best things that's come out of github's feature department in a long time because it greatly reduces the "+1", "I have this issue too", etc. type comments which are a nuisance.

I'll take another fresh look at this tomorrow but I suspect you're _interpreting_ my behaviour as rude, I'm not trying to hurt anyone here, just trying to keep this conversation on topic, civil, and helpful without outside individuals having to sift through the noise. 

___

the most useful information I can give at this time has already been given [in a comment above](https://github.com/PyCQA/pyflakes/issues/520#issuecomment-600685295), we're ready to try and make the next alpha release but it needs someone with pypi permission to do so
User asottile: 2.2.0 has been released üéâ 
User PeterJCLaw: @asottile thanks for organising this and getting it out :)

Should there be a tag for this release in the repo?
User asottile: I just tagged the release, should be present now -- thanks for the pointer @PeterJCLaw !
User asottile: for those looking for a flake8 release containing this, I've put up 3.8.0a2 a few days ago which enables this (still an alpha as we're taking a more cautious rollout of both pycodestyle and flake8)

try it out with either `pip install --pre --upgrade flake8` (or if you're using pre-commit: `pre-commit autoupdate`)
User flamableconcrete: @asottile: you are my hero! I just saw this and moved to using `flake8==3.8.0a2` and I am SO HAPPY that I can lint my python 3.8 walrus operators now!

Feel free to mark this as off topic too, BTW since it isn't directly related to the release, I just wanted to say thank you!
User brettcannon: @asottile is there a reason why pyflakes 2.2.0 doesn't list Python 3.8 support but flake8 3.8.0a2 does? Or do you just need a PR to add Python 3.8 to CI, classifiers, and README for pyflakes?
User asottile: seems a simple oversight, nothing tests the classifiers
User brettcannon: @asottile OK, then I'll try to get you a PR this week.
---------END---------
152
User dmc5179: ##### SUMMARY

Make boinc_auth a configurable value in the community_grid role to enable users to allocate the WU points to an account that they choose.

##### ISSUE TYPE
 - Bug Report
User dmc5179: The default value in the role should be set to annoymous or the ibm grid equivalent.
User IPvSean: >The default value in the role should be set to annoymous or the ibm grid equivalent.

If we just document this why would this matter?  We are not getting paid by IBM Community Grid?  It is already completely configurable.  I can just add additional documentation.  It is a totally optional role to run.
User dmc5179: There are 2 lines in the documentation that even mention the existence of the role which don't even explain what the role is doing. It is on by default and contributing back to a RedHat team. There are absolutely ethical implications of doing so. If, as you say, it doesn't matter, then there should be no issues setting the default to anonymous. The goal is to contribute to the cause. But it isn't as interesting to put out a snap of the grid dashboard showing how much the anonymous team has contributed. It's far more interesting to put out a snap of the dashboard showing how much the RedHatAnsible team has contributed by defaulting the role to on and sending points to their account.
User IPvSean: @dmc5179 there is a blog post here explaining everything publicly on the open internet: https://www.ansible.com/blog/ansible-and-ibm-community-grid

I have also been publicly tweeting the data until we can find another technical solution.

if you look through the release PRs you see the release notes for every release. 

I think it is disingenuous to use the word ethical here...  The goal is to contribute to the cause but the primary and default contributor is the Red Hat Ansible Team hence why it is the default.  You can easily override that var with extra_vars, see the documentation here: https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#variable-precedence-where-should-i-put-a-variable
User dmc5179: OK, if you feel that it is clear enough that 2 lines in an extra_vars example to disable the role shows users how to configure the role using an extra_vars file to set a role variable that isn't listed as a variable in the role defaults or vars and defaults to the RedHatAnsible team then I'm good to close this issue.

Defaulting distributed compute points back to your own team when this can be run by anyone, in their own environments, including non RedHat, is absolutely an ethical question. We came to different conclusions regarding the answer.

It is also not disrespectful or disingenuous to debate the ethical implications of such a decision. Some seem to interpret such a discussion by default as an attack on them personally. Yet the debate over an ethical question can center around perception and how such a choice will be perceived by others. In cases where a choice can easily and quite reasonably be perceived in a negative light it is often best to choose the option which leaves the least doubt as to one's motivations.

Tweets have been put out showing how much this effort has contributed to the cause highlighting the AnsibleRedHat team. It is clear from those tweets the goal is to show how much RedHat is contributing to the cause. The combination of defaulting the role to on and contributing to RedHat with tweets showcasing our efforts can easily leave one with ethical questions.

User IPvSean: @dmc5179 I want to remind you and the community that these points are purely meaningless internet points to help non-profit causes from the IBM Community Grid.  I am simply tracking all the points the "project" is tracking and it is completely configurable.  We can add more documentation but I really confused by the ethical question.  I will wait for others like @cloin and @liquidat to weigh in on this.  Just because you THINK one way doesn't mean that others THINK another way, there is not an absolutes with ethical concerns.  Everything in this repo is 100% open source so I find it really hard to believe that someone would run this Ansible Playbook without investigating what it does.   
User dmc5179: @IPvSean It's not meaningless if you put out a tweet showing how much you've contributed with your team name on the top.

If it were meaningless you'd have defaulted to anonymous which is what we did in the F@H effort. Because you're right, they are meaningless. But the tweets suggest perhaps they are not meaningless to everyone.

User abenokraitis: > > The default value in the role should be set to annoymous or the ibm grid equivalent.
> 
> If we just document this why would this matter? We are not getting paid by IBM Community Grid? It is already completely configurable. I can just add additional documentation. It is a totally optional role to run.

+1 to this
User IPvSean: >@IPvSean It's not meaningless if you put out a tweet showing how much you've contributed with your team name on the top.

![sigh](https://media.tenor.com/images/d8fcca857e34afac73069fb02c5b5b0b/tenor.gif)

I would have hoped that all IBMers and Red Hatters, our partners and community would be happy we are trying.  Sorry that we tried to contribute to make the world a better place.

User dmc5179: > > @IPvSean It's not meaningless if you put out a tweet showing how much you've contributed with your team name on the top.
> 
> ![sigh](https://camo.githubusercontent.com/af720f36df938a403e2d5bc2ea9f9cd9b5d836d2/68747470733a2f2f6d656469612e74656e6f722e636f6d2f696d616765732f64386663636138353765333461666163373330363966623032633562356230622f74656e6f722e676966)
> 
> I would have hoped that all IBMers and Red Hatters, our partners and community would be happy we are trying. Sorry that we tried to contribute to make the world a better place.

Hahahaha, that's awesome. I've questioned why you would default the points to your account and not anonymous. And why you would have almost no documentation in the repo about the role. And your response is childish; clearly you don't want to help the world.

The F@H team has done more to help the Covid effort than this team. We've worked directly with the team that runs the entire effort. We've watched as the team has scrambled up the leader board. You know what we didn't do? Default points to our own account:

https://stats.foldingathome.org/team/11812

We also don't waste time setting boinc to run on t2/t3 instances which have to be the worst instances to run these jobs on.

I've learned a great deal here today. About what some people think is ethical and what some people's response to disagreement. The fact that your team thinks this is ok and the fact that RedHat legal ok'd it is impressive. Though I give RH legal a pass. I doubt defaulting to your account was presented to them
User cloin: What's the exchange rate between IBM points and Schrute bucks?
User dmc5179: @IPvSean I also love that Sean refuses to address the inefficiency of running CPU based WUs at all, let alone running them on t2/t3 instances. You know how many points you get from running 24 cores on FAH CPU WUs is for several days, like 12000. In the same time GPUs get millions of points. So when you look at the Points Per Day (PPD) and the Watts per Point (WPP) you find that CPU compute is so bad that essentially you'd contribute more to the Covid cause if you went outside and burned a barrel of paper.
User rhenshall: @dmc5179 I think that's enough now, I'm unsure what point is being made so suggest the back and forth stops
User liquidat: As discussed, we chose a different approach. Right now all the configuration can be easily turned off. But we are always open for improvements, and if a configurable boinc_auth is requested by the community, why not.

Do you happen to be willing to work on a PR here? My time is limited right now so this has a low priority right now for me.
User IPvSean: more info added here, you can see this now on my fork: https://ipvsean.github.io/workshops/provisioner/#ibm-community-grid
---------END---------
153
User marinofaggiana: The new "big" next version of Nextcloud iOS will have all internal viewers totally rewritten and **coherent** with each other.

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 09 20](https://user-images.githubusercontent.com/8616947/75965820-a5b8a380-5ec9-11ea-8205-dbc117a34180.png)

Each viewer will always have the menu with the available options and always at touch:

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 15 29](https://user-images.githubusercontent.com/8616947/75966523-ca614b00-5eca-11ea-9833-6bda284d28ba.png)

The use with ipad has been totally revised

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 22 39](https://user-images.githubusercontent.com/8616947/75966963-668b5200-5ecb-11ea-9ee6-bc75ec109d81.png)

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 21 51](https://user-images.githubusercontent.com/8616947/75966955-63906180-5ecb-11ea-8c8a-848261fc1127.png)

The new PDF (Apple PDFKit) viewer will now be able to view the notes you have inserted in your PDFs

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 29 51](https://user-images.githubusercontent.com/8616947/75967724-866f4580-5ecc-11ea-9ee3-f4cad955371c.png)

Browsing your images is now easier

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 34 18](https://user-images.githubusercontent.com/8616947/75968003-faa9e900-5ecc-11ea-9a28-d7edd65fe40b.png)

With a improved zoom

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 34 34](https://user-images.githubusercontent.com/8616947/75968028-072e4180-5ecd-11ea-9630-43ce4b4d1efd.png)

And more convenient to edit your texts with iPad & Only Office 

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 37 51](https://user-images.githubusercontent.com/8616947/75968284-6a1fd880-5ecd-11ea-84e0-4329550812ca.png)

... and much more in development.

Available an **Alpha** version in TestFlight, today !

https://testflight.apple.com/join/GjNbfo2a

User jancborchardt: Good good stuff! :) Some design review points, not only about the viewers but generally while we‚Äôre at it:

----------------

> ![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 09 20](https://user-images.githubusercontent.com/8616947/75965820-a5b8a380-5ec9-11ea-8205-dbc117a34180.png)
- [ ] The filter/funnel icon for sorting is a bit strange. Better would be a "Sort" icon like this: https://material.io/resources/icons/?search=sort&icon=sort&style=baseline
- [ ] The "Select" action next to it can also be moved into that "Sort" menu. Otherwise there‚Äôs too many icons up top there.
- [ ] The Share icon in the list seems rather large, and the 3-dot icon rather small, that should be adjusted

---------------

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 15 29](https://user-images.githubusercontent.com/8616947/75966523-ca614b00-5eca-11ea-9833-6bda284d28ba.png)

- [ ] "Close" should be in the top left instead of the 3-dot menu
- [ ] The "Share" action should be left of the 3-dot menu, to mimic how it is in the file list
- [ ] Same for the actions, they should be the same as in the file list:
   - Add to favorites ("o", not "ou" spelling)
   - Details
   - Open in ‚Ä¶
   - Rename
   - Move
   - Delete

------------------

![Simulator Screen Shot - iPad Pro (11-inch) - 2020-03-05 at 10 37 51](https://user-images.githubusercontent.com/8616947/75968284-6a1fd880-5ecd-11ea-84e0-4329550812ca.png)

cc @juliushaertl and @tobiasKaminsky on these for Android & iOS cross-platform coordination

- [ ] We are duplicating the document title there in our header bar, this doesn‚Äôt seem necessary?
- [ ] Same for undo/redo, it‚Äôs duplicated from the OnlyOffice bar
User marinofaggiana: Hi @jancborchardt can you explain this ? Remember the use with iPhone:

>  "Close" should be in the top left instead of the 3-dot menu
  The "Share" action should be left of the 3-dot menu, to mimic how it is in the file list
  Same for the actions, they should be the same as in the file list:
Add to favorites ("o", not "ou" spelling)
Details
Open in ‚Ä¶
Rename
Move
Delete

![Simulator Screen Shot - iPhone 11 - 2020-03-06 at 09 05 27](https://user-images.githubusercontent.com/8616947/76064584-42db1100-5f8a-11ea-9664-f7889d71b355.png)


> cc @juliushaertl and @tobiasKaminsky on these for Android & iOS cross-platform coordination
> 
>   We are duplicating the document title there in our header bar, this doesn‚Äôt seem necessary?
>   Same for undo/redo, it‚Äôs duplicated from the OnlyOffice bar
> 

the header bar is necessary because I want always the "menu ..." available, and the redo/undo in bottom is the automatic bar of webview 




User tobiasKaminsky: Header bar is on ios needed as long as thgere is no back button in OnlyOffice.
On Android I show OO in fullscreen, as there is always a back button/gesture.

@juliushaertl do you know if/when they implement this?
User juliushaertl: > Header bar is on ios needed as long as thgere is no back button in OnlyOffice.
On Android I show OO in fullscreen, as there is always a back button/gesture.
> @juliushaertl do you know if/when they implement this?


I think this should be there with their next document server release 5.5.0 but I've no idea if that fix has made it in.
User juliushaertl: > "Close" should be in the top left instead of the 3-dot menu

Why would there be a close action anyway, isn't that what the back button in the top left is for on iOS?
User marinofaggiana: @juliushaertl some issue with iOS 

![IMG_0003](https://user-images.githubusercontent.com/8616947/76067327-78362d80-5f8f-11ea-8790-85843d6fb6cd.PNG)

![IMG_0002 2](https://user-images.githubusercontent.com/8616947/76067390-8edc8480-5f8f-11ea-8c07-7e887d4bee41.PNG)

User marinofaggiana: P.S. remember all iOS is not Android and do not exists a button hardware for back and exists iPhone and iPad with different view (split mode o single view mode), so every detail view (Nextcloud text, Only Office, collabora, PDF, images, video ...) **must** have a Navigation Bar + back button "<", or magnification button "two arrow" for iPad, + title + menu button "..."

User juliushaertl: @marinofaggiana Second one is expected as we center the editor. For the first one please open an issue in https://github.com/nextcloud/text with details on which version this happens.
User marinofaggiana: Build 4

![IMG_0008](https://user-images.githubusercontent.com/8616947/76201943-5c7f9100-61f4-11ea-9002-a36f50454a75.PNG)

![IMG_0006](https://user-images.githubusercontent.com/8616947/76201951-62757200-61f4-11ea-9ce6-1d77cf54838d.PNG)


User JorisBodin: Hello,
> We are duplicating the document title there in our header bar, this doesn‚Äôt seem necessary?

Maybe display "Only Office" in the Navigation Bar to avoid this duplication?
Or maybe customize the display of the Onlyoffice bar, not to display a title?

> The "Select" action next to it can also be moved into that "Sort" menu. Otherwise there‚Äôs too many icons up top there.

I think the "Select" should not be displayed in the "sort" modal. It's an inconsistency. 

The sort button could be on the right side of the search bar. This would prevent too many buttons on the navigation bar.
User marinofaggiana: Build 5

View video & audio when viewing images. [ touch for view ]

![IMG_0010](https://user-images.githubusercontent.com/8616947/76221135-e04b7480-6218-11ea-804f-3bb932a5f0ab.PNG)

User marinofaggiana: > Hello,
> 
> > We are duplicating the document title there in our header bar, this doesn‚Äôt seem necessary?
> 
> Maybe display "Only Office" in the Navigation Bar to avoid this duplication?
> Or maybe customize the display of the Onlyoffice bar, not to display a title?
> 

For now I have only removed the title: ( @jancborchardt )

![IMG_9AD5F1BAC58E-1](https://user-images.githubusercontent.com/8616947/76226896-c9a81c00-621e-11ea-9b29-c652d37dbe42.jpeg)

User ghost: Internal viewers are bull. Just make the sync actually work (right now you need to wait with the app on screen for an hour for offline downloads to even start) and make the iOS Files integration work. There are a zillion much better apps then than shoddy internal *nonsense*. 

Completely misplaced priorities in this completely dysfunctional piece of shit project that kills the whole NextCloud ecosystem.

Not that any other iOS cloud files service besides the exorbitantly priced Dropbox is any better.
**Dropbox**: works, but cannot afford it. **OneDrive**: No Unicode in Files.app integration in 2020, prone to failed saves generating phantom files that cannot be deleted or moved, constant 100% CPU in the macOS client. **NextCloud**: the iOS app is super slow, I guess it always checks the server when opening a directory, frequently does not give access to (offline) files through the iOS Files.app integration, does not sync offline files automatically in the background (OneDrive very much appears to, so it's possible), takes days even when not in background to start sync, every update introduces new bugs and unrealiability especially in the Files app integration, randomly deletes/forgets entire accounts and gigabytes of offline files, author refuses to even consider E2E through Files.app (I'm sure there are ways at least with offline directories). Not that E2E would be any more functional in the desktop app either https://github.com/nextcloud/desktop/issues/774. **iCloud Drive**: Reasonably priced, but no offline files, so forget about it. Not particularly reliable sync; can sometimes take days. **Google Drive**: Google is spyware, forget about it. **SeaFile:** Seems to work but requires server and database root access so not a low cost option / not an option for me.

I'm almost considering hacking a reliable encrypted no-nonsense syncing system based on Borg backup‚Ä¶ but I refuse to pay Apple $90/year for the basic right of being able to run my own apps on my own device, and distribute them to others. Even more so I refuse to use Google's spyware (Android).

Software is shit. Period.
User ghost: If I wanted to be locked into just a single app having access to my files I‚Äôd be using PDFExpert. Its WebDAV-based sync from the nextCloud server is much more reliable and faster than the shoddy pile of garbage that this official iOS app is. Instead I‚Äôm using PDFViewer which is much better as a PDF app but I‚Äôm suffering daily from the garbage that this formerly sync app but now apparently a closed micro-ecosystem is. Surely to include a total pile of crap for a PDF annotator..
User jancborchardt: @yadayadaydadaa this tone is not welcome here, and it won‚Äôt help to motivate anyone improving the app. If you want to participate, read and respect our community code of conduct: https://nextcloud.com/contribute/code-of-conduct/ ‚Äì and at the very least, don‚Äôt be counter-productive with inflammatory comments. Thank you.
User ghost: Deleting my comments describing the true ‚Äúquality‚Äù this app. Flash-closing my bug reports. Says a lot about the priorities, just like this present ‚Äúissue‚Äù that should not even be listed, at the very least not until the plentiful problems are fixed. 
User ghost: There's pretty much no point in even trying to contribute to this project, because based on my experience the next update will undo any fixes done in the previous one. 
User jancborchardt: @yadayadaydadaa your comments were merely hidden, which is what we do with any comments which contribute nothing. It‚Äôs also unnecessary to still comment if you use iCloud instead of Nextcloud now as said in [the other issue](https://github.com/nextcloud/ios/issues/1229#issuecomment-620734320). Bye
User ghost: Again. No discussion allowed. No critique allowed. Just pushing fingers into the ears and singing. To avoid hearing about the impending doom, to avoid hearing the truth about the quality of this project. Bye. 
---------END---------
154
User andersonfaaria: <!-- Welcome to the issues section if it's your first time! -->

### Before creating an issue, please ensure:
- [x] This is a bug in the software that resides in this repository, and not a
      support matter (use https://otland.net/forums/support.16/ for support)
- [x] This issue is reproducible without changes to the C++ code in this repository

I just ran a report with [Visual Code Grepper](https://github.com/nccgroup/VCG) and it returned 127 potentially dangerous code in this repo.
(a few examples)
![image](https://user-images.githubusercontent.com/8560144/81466512-b2e35e80-91a8-11ea-9c79-28c717738c12.png)
While some of those are actually false positives or can be considered safe under the conditions we are using, I would recommend fixing them as well to avoid buffer overflow situations.

You can find the full report exported here: https://github.com/andersonfaaria/VCG-forgottenserver/blob/master/VCG%20Report.xml


Some functions considered unsafe by Microsoft report:
![image](https://user-images.githubusercontent.com/8560144/81466204-0bfdc300-91a6-11ea-8c87-1ed5ad2236ab.png)

You can also find similar lists [all over github](https://github.com/intel/safestringlib/wiki/SDL-List-of-Banned-Functions)

User Xaekai: **200% Extreme Risk: _Having network cable plug into your computer._**

You know what to do OP. Best unplug that. Abstinence is the best way to keep from the dangers online! If you can't resist the urge, at least put a condom over your network cable before plugging it in! As an added benefit, it would also shield you from the unsafety of iterating over... Vector.size()..  causing an infinite loop. Maybe if you are [doing really strange things](https://stackoverflow.com/questions/50136612/vector-returns-negative-size-c), but this is not one of those times.

I mean it's only [getSpectators](https://github.com/otland/forgottenserver/blob/master/src/spectators.h#L54), one of the most critical most called functions of the engine. Lets add a whole bunch of method dispatch overhead in the form of excessive type safeties, and make the daemon perform like a sloth living in a tree downwind of an burning opium poppy field. Genius.

Tiles get made and not deleted.... wow shocker. I heard rumors that sort of thing may happen in software revolving around hosting a persistent gameworld, I guess they were true!  Tasks get deleted a lot... better phone the president.

I get that you're trying to be helpful... but you're not trying very hard. It's obvious most of this list is bullshit, (a clear indicator that Microsoft has no business sniffing the code of Unix-centric network service daemons) and you just dump the whole thing here... because why? You aren't qualified to determine if these are problems? Either, you know enough to determine which of them actually warrant concern, and that means you should have **curated the list** before presenting it, or you don't know enough to determine if these are problems and you should have asked if running this tool on this codebase **was even worthwhile** in the first place, perhaps on that forum you totally ignored in the issue template.

tl;dr; Don't bring bomb sniffing dogs into the weapons factory. Especially ones wearing the XML service dog vestments and are clearly regular [text-log pets](https://github.com/andersonfaaria/VCG-forgottenserver/commit/bf5f132fa3931212440d68ce021dc6875f54d6e3#diff-7a2b35d049302dd2fa4bc751ac773e87).
User andersonfaaria: wow, you are totally right. Why not just fucking drop all the checks and CI tools then if everyone here is a genius that make reliable code 100% of time?
Common Lessaire, you can't be that stupid. It's a shock that just yesterday I was talking how intelligent you are.

I guess you weren't taught about safe programming at university....
As I stated, some of them are false positives but take a look at the reports.. there's banned functions and mismatched type comparisons that can easily be solved with a cast. That's the type of thing that can easily avoid warnings (and possibly crashes). 


User Xaekai: Yeah, you are correct that the ones that are actual problems won't take much to fix.

But my point was you put in like **zero effort yourself** on curating. This idea should have been hashed out on the forum, and the list _reduced_, perhaps by collaborative effort, to _**actionable items**_, and then **issue**'d. Instead it comes across as like a blob of busywork.

I guess it's a matter of perspective the division of labor between the forum and the GitHub. This place is not the drawing board. I take GitHub notifications more seriously than emails that something I'm interested in was posted on the forum.

![Gordon wouldnt put up with this](https://comicsandmemes.com/wp-content/uploads/2013/06/Gordon-Ramsay-Angry-Kitchen-RAW-FISH-NEMO.jpg)
User andersonfaaria: Point taken, I'll try to go a little deeper in the most critical ones and try to give explanations why I believe they need to be solved showing cases where they could lead to actual vulnerabilities.
My initial understandment was that someone who would be able to fix those would also be able to understand the report and the messages, so this is more of an overview to raise awareness that this areas need to be worked on, I was not really expecting to give more details than the report itself already does...
User nekiro: So instead of contributing (with meaningful information) you gonna just run a tool that spits out results (most false positive or completely redundant) for us to contribute (a.k.a look through it)? I don't think that's how it works. It's like you wanted to feed a dog, but you threw at him bunch of trash and some edible for eating things and make him search for it. If you try to contribute at least please **try** to do so.
User andersonfaaria: it's not like it's my fault this repo don't even pass the checks it has.
Automatic checking is automatic so everyone agrees that this is important and that is why it was inserted in first place, right? What I'm doing here is raising awareness of the backlog. Among it, there are thousands of things to be checked for future PRs. Yes, there are false positives but aren't those OBVIOUS?

I mean, memory mismanagement and unsigned comparison with signed may not broken the repo depending on the situation, but that does not change the fact that this is a flaw that can ruin the server if the context is changed. This is the issue area so I'm only raising the issues, you saying that this isn't valid because I didn't provided the solution not only is wrong but also gives me the sensation that the goal of TFS repo is to be a time-bomb ready to explode when people make source edits by themselves. And we will have that old good excuse "we do not take responsability for changes outside of the repo"...

Well, if you insist that this is how things works here I can close the issue. But let's be honest with ourselves and also turn off the builds and checks since they are automatic tools that don't give us the solution but rather just point out where it is wrong...
User Xaekai: The issue is not that the maintainers of the TFS reject the idea of memory leaks or sanity checking the code. The issue is you presented it as an uncurated blob. There was dozens of items even you yourself knew for a fact were chaff. Invalid, false positives. You could have truncated those in advance, and mentioned their existence. Your failure to do this exhibits a lack of understanding in how this facet of software QA should be introduced to a codebase that doesn't have existing facilities for it.

If you really want to do this and do it right, you need to use several of the best of breed of this class of tools, tally which issues are detected _multiple_ times, and sort your final list by tally count descending. Discard everything not detected at least twice. Then go through and filter the list by examining for which appear to create genuine risk, and present just those. I'm guessing you'll find less than 20. Why? Because severe bugs generally get discovered fast with this number of eyes on the software already.

It's still a good idea, your execution of it was just profoundly clumsy. And doing it correctly may be beyond you, as the **right** tools for this job are not simple FOSS. They tend to cost serious money or even if they offer a discounted or free license for opensource projects often require jumping through hoops to prove your project is worthy of a license. That is a hoop you are not in a position to jump through, only a maintainer can.

The very best are Parasoft, Coverity, PVS, and Klocwork; none of which are free. The first has requirements for gratis opensource license that TFS probably can't meet. Coverity is free for OSS projects but requires sign up. TFS probably also qualifies for a gratis 1-year PVS license. (I've been meaning to discuss these matters with Danitello, but I've gotten busy because of politics in another project.)  Klocwork requires signing up for a trial using a business email address, and are unlikely to grant one to a developer seeking to primarily use it for FOSS projects. The have a rather laser focus on corporate customers.

Then you have SonarQube and Resharper, both of which require license to really shine. Qube isn't the best at CPP SCA, but it offers a plugin interface in it's scanning tool which is super handy as some really good SCA tools offer Qube plugins as an option, for example CppDepend, which would be next in this list of big names static analysis.

The best free ones I know of are cppcheck and Clang tidy. If you can get a project to make tidy happy, it's a point of pride.

The tool you used? I had literally never heard of it till this, and it's not making a good first impression. :man_shrugging:  Anyway, don't give up on this. Just start a thread in the TFS dev forum, and hash it out. Opening this is an issue was merely premature.
User lgrossi: Yeah, yeah.. TFS code base is awesome, there is nothing to improve, no memory leak, no useless wrongly used std:: structures, no shameful lua codes that do much more that it should in an idiot way, etc. TBH, I think cipsoft should use tfs instead.. That's why this repo is dead as f. The maintainers acts like children being passive-aggressive when people are only trying to help as much as they can. Shame on you.

Edit:
Sorry, everything is just false positive, and you guys have the perfect code.
User andersonfaaria: The only name that rings a bell is SonarQube because of 'SonarCloud', are they from the same company?

VCG is vastly used. Have you ever heard of CERN, the particle accelerator? https://home.cern/

Check their 'recommendations' tab:  https://security.web.cern.ch/recommendations/en/codetools/vcg.shtml

The world largest computing/physics institute consider this the best option to check, if you haven't heard of it I think it's time you get down the pedestal and understand other people may know more in what they are talking about than you do.

The fact that you're giving more importance to the 'impression' rather than the content of the file says it all... it's a report for end users, just open and read it, you know what needs to be done. This could be easily divided into a few developers and made a checklist just to fix the backlog for future checks, but no, we are making a discussion on how should I have presented the data.

Those are all potential issues waiting to happen, I'm just making a pro-active job and pointing them out before people find out and exploit it. Yes, it was with an automatic tool and yes there might be cases where we can let the unsafe code exist because it's easier than changing TFS archtecture.
Does that make it less wrong? No.








User nekiro: Yeah, thanks for the input guys.
---------END---------
155
User MateoNikolic: Some of the NBT Tags still aren't saved while creating the kit.
I've made a gif that shows it really good.
The first or upper set I cover is the real set with good and corrent NBT Tags
The seconds or lower set is the set that's take after kit is saved and loaded with Essentials:

![0MrEcykDTt](https://user-images.githubusercontent.com/59744325/82736215-39ab4600-9d28-11ea-9ad2-881d99b6cf84.gif)


Also there are some NBT Tags that are being saved like Armor and Armor Toughness.
This were tested with plugin called MythicMobs, it allows creating custom items with nbt tags.
User JRoy: Duplicate of #3323
User MateoNikolic: What duplicates? Fix the fucking bugs, it's not even duplicated it's a new freaking thread you dumb fuck
User pop4959: Please have a better attitude. It's a duplicate because someone has posted exactly the same thing, and it's still open for someone to implement. Re-creating the same issue over and over is not going to make it happen faster. Watch or reply to the original "thread" (issue, this is not a forum) if you want to keep up to date on its progress.
User MateoNikolic: Sorry I was pissed off because I took my time taking the gif and reporting everything.
I just want this fixed, it's the problem that's like 2 years old. I don't wanna use external kit plugin just for this reason.
User pop4959: It's a low priority addition currently, so it might be a while before Essentials supports this. You must understand that Essentials development is mostly driven by individual efforts of contributors, which means that if no one is interested in adding the feature, it doesn't get added. Since these metadata tags are not very uncommon (pertaining only to plugins/modded experiences), my guess is not many people are interested or even know about them. Currently the recommendation for third party plugins (such as your "MythicMobs") is to add the item to the kit in the form of a command, provided that the plugin has something like that. You could ask them to add something like this if they don't already. Otherwise, you might want to look into another kit plugin, as you're at the mercy of someone willing to add this feature (it could be another 3 years, or longer!).
---------END---------
156
User TheVekter: <!-- Write **BELOW** The Headers and **ABOVE** The comments else it may not be viewable -->
## Round ID: #138035
<!--- **INCLUDE THE ROUND ID**
If you discovered this issue from playing tgstation hosted servers:
[Round ID]: #138035 (It can be found in the Status panel or retrieved from https://atlantaned.space/statbus/round.php ! The round id let's us look up valuable information and logs for the round the bug happened.)-->

## Reproduction:
1) Find or make a bunch of bodies
2) Either place them on a space tile or soak them in CLF3 and ignite them
3) Place surgery drapes on all of them
4) Begin tends wounds surgery
5) Revel at your ability to farm XP for no downside

It also spams the EVERLOVING FUCK out of chat. This was all within about a second of each other.

![image](https://user-images.githubusercontent.com/7019927/82869014-33b59080-9ef3-11ea-95f7-19b4c0847e1e.png)

<!-- Explain your issue in detail, including the steps to reproduce it. Issues without proper reproduction steps or explanation are open to being ignored/closed by maintainers.-->

<!-- **For Admins:** Oddities induced by var-edits and other admin tools are not necessarily bugs. Verify that your issues occur under regular circumstances before reporting them. -->

Yeah, this is a minor one, but if we're rewarding people for this sort of thing we should probably start fixing the bugs/exploits.
User LemonInTheDark: How would we fix this?
User TheVekter: @Rohesie and @MrPerson were talking about some method of limiting do_after proc calls but that's a bit above my head.
User LemonInTheDark: Ohhhhh that sounds fun
User Time-Green: I honestly never really considered it a bug. Same can be done with different body parts. It's nice to be able to instantly augment someone, the downside is that they die from shock.

I've also used it for research when it was still a thing, but yeah the way you describe it being used is kinda broken
User Rohesie: You can limit this easily by, on `/datum/surgery_step/proc/initiate(mob/user, mob/living/target)`, adding a check such as

So it doesn't start a new surgery if it's already performing a timed action.
User Timberpoes: ![image](https://user-images.githubusercontent.com/24975989/83238088-d6893b80-a18d-11ea-9d86-df2f43b9c3ca.png)
![image](https://user-images.githubusercontent.com/24975989/83238127-e30d9400-a18d-11ea-8fc3-2306828a420c.png)
We're now metaperforming surgery on burning corpses to max out our medical skill and get our fancy #50656 capes.
---------END---------
157
User SweptWasTaken: Our jumpsuits look like shit.

![image](https://user-images.githubusercontent.com/49448379/83221885-9d89a080-a166-11ea-8f22-dbf53465e578.png)
![image](https://user-images.githubusercontent.com/49448379/83221896-a37f8180-a166-11ea-9cc1-ac9eb9562bc5.png)
![image](https://user-images.githubusercontent.com/49448379/83221904-aa0df900-a166-11ea-8a25-f7c96c5929f9.png)
![image](https://user-images.githubusercontent.com/49448379/83221929-bdb95f80-a166-11ea-8909-5286f4c56251.png)
(I know this one isn't used for assistants anymore but it still looks like garbage.)

Making this a discussion thread with a catchy title so that we can have a discussion which hopefully ends up in us switching to tg jumpsuits or a better alternative.

User SweptWasTaken: I am actually a stupid fucking brainlet and made an issue for the wrong github repo.

This was meant for SS14 so sorry
User Akrilla: swept moment
User ghost:   :grimacing: 
User ghost: swept moment
---------END---------
160
User Vaelatern: It's meant "orphaned package" for years now. Nothing is broken.
---------END---------
162
User MohammadMirsafaei: https://github.com/kataras/iris
I think iris should be on list too
User ketloss: Because [Iris is purposefully not included on awesome-go](https://github.com/avelino/awesome-go/pull/1135) and [some other reasons](https://github.com/julienschmidt/httprouter/issues/148).

I would recommend **anything else** from the [above project](https://github.com/mingrammer/go-web-framework-stars) and would honestly prefer to write a web application in assembly language before using anything written by @kataras.  

He is a cancer to open source and it is only a matter of time before Microsoft cracks down [on this thing](https://www.reddit.com/r/golang/comments/b481q7/a_warning_about_githubcomkatarasiris/).


User vzool: @ketloss Indeed, I got to use **Iris Framework** at the beginning when started my first web development using `golang` and it does behave weird which made me felt unsafe an vulnerable to all sort of danger.
---------END---------
163
User Yudi: Kinda related to #1‚Ä¶?
[Try this for now.](https://github.com/Tyrrrz/DiscordChatExporter/wiki/Storing-HTML-locally)
User Yudi: > your people hash id's

Do you mean User IDs?
If so, try exporting to JSON or inspecting the HTML.
User Tyrrrz: Duplicate of #1 and #21
---------END---------
169
User VendicarKahn: Doing a manual projection with a user defined matrix [[1,0,0,0],[0,1,0,0],[0,0,0,0],[0,0,0,1]]
results in a 2d projection onto the xy plane.

Requesting a linear extrude of this 2d object residing on the xy plane results in an error extruding a 3d object when no 3d object exists.


User nophead: Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.
User t-paul: As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module [`projection()`](https://en.wikibooks.org/wiki/OpenSCAD_User_Manual/Using_the_2D_Subsystem#3D_to_2D_Projection).
User VendicarKahn: So you are saying that there are two kinds of 2d objects in openscad.¬†The first has zero dimensions along one axis and the second has zero directions along one axis and has special magical properties.¬†Unacceptable.¬†11.04.2020, 07:29, "Torsten Paul" <notifications@github.com>:
As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module projection().

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.
User nophead: No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.

If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.
User t-paul: OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.
User VendicarKahn: Manifolds are only defined in 3 or more dimensions. ¬†So, ya. ¬†Using a matrix to project a 3d object onto a 2d surface necessarily means not having a manifold.¬†The projection function also does not return a manifold since it returns a 2d object.¬†There is no such thing as a 2-manifold.¬†"Lots of self intersections" -¬† That is your problem, not mine.¬†If you provide a multmatrix function you should support any matrix, not just an unspecified¬†subset of them.¬†¬†11.04.2020, 05:40, "Chris" <notifications@github.com>:
Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.
User VendicarKahn: ¬†If you provide a multmatrix function then you should support all matricies, not just an unspecified subset of them.¬†What other matrix operations don't you support.¬†¬†12.04.2020, 14:05, "Torsten Paul" <notifications@github.com>:
OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.
User VendicarKahn: Maybe you should actually write that down somewhere. ¬†You know, like in the documentation. ¬†Especially in the Multmatrix section of the documentation to make it clear that this program is incapable of managing ¬† all matrix operations.¬†LOL¬†It's half baked.¬†¬†12.04.2020, 13:58, "Chris" <notifications@github.com>:
No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.
If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.
User t-paul: Ok, I think that is enough.
User nophead: The matrix operations are fine but just like polyhedron they are GIGO. You can make an infinite number of 3D objects that CGAL will not accept because they are not manifolds. 

OpenSCAD provides projection() to turn a 3D object into a 2D object. It doesn't simply set all the Z coordinates to 0. It creates a new polygon outline like a shadow and removes all the internal edges that would collapse on top of each making a polygon with lots of self intersections.

On the other hand multmatrix does a matrix multiplication on all the vertices. It doesn't add or remove any of them or change the edges.
---------END---------
170
User Magzit: **Describe the bug**
<!-- A clear and concise description of what the bug is. -->

**To Reproduce**
<!-- Steps to reproduce the behavior. -->

**Expected behavior**
<!-- A clear and concise description of what you expected to happen. -->

**Additional Information**
<!-- For instance, relevant logs from terminals or the browser's devtool console. -->

User JesterOrNot: Please stop with the fowl language
---------END---------
171
User ruslan051: Dont work with fb abd google üëéüëéüëé
User ThomasHabets: Thank you!
---------END---------
172
User ImMrMa: # üêõ Bug

can't run office example correctly.
https://hydra.cc/docs/configure_hydra/logging
## To reproduce

** Minimal Code/Config snippet to reproduce **
config.yaml
defaults:
  - hydra/job_logging: custom
files
![image](https://user-images.githubusercontent.com/24520617/82426441-707c2480-9aba-11ea-9aa5-05555116f0b7.png)

 File "my_app.py", line 11, in <module>
    my_app()
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/site-packages/hydra/main.py", line 24, in decorated_main
    strict=strict,
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/site-packages/hydra/_internal/utils.py", line 174, in run_hydra
    overrides=args.overrides,
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 86, in run
    job_subdir_key=None,
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/site-packages/hydra/plugins/common/utils.py", line 101, in run_job
    configure_log(config.hydra.job_logging, config.hydra.verbose)
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/site-packages/hydra/plugins/common/utils.py", line 24, in configure_log
    logging.config.dictConfig(conf)
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/logging/config.py", line 800, in dictConfig
    dictConfigClass(config).configure()
  File "/data00/home/mayingxin/anaconda3/envs/pysot/lib/python3.7/logging/config.py", line 496, in configure
    raise ValueError("dictionary doesn't specify a version")
ValueError: dictionary doesn't specify a version



** Stack trace/error message **


## Expected Behavior
<!-- A clear and concise description of what you expected to happen. -->

## System information
- **Hydra Version** :  
- **Python version** : 
- **Virtual environment type and version** : 
- **Operating system** : 

## Additional context
I think your tutorials are dogshit!
https://hydra.cc/docs/configure_hydra/logging

User omry: learn to communicate with respect and come back.
User omry: in fact, don't bother.
User ImMrMa: in fact, you write a shit doc. I'm a real man, it's my feeling of your holy shit doc.
User omry: Please don't use Hydra.
---------END---------
173
User vectorselector: [NOTE]: # ( ^^ Provide a general summary of the issue in the title above. ^^ )

I have spent 1 year trying to make clusters reliably. I follow every step of https://docs.couchdb.org/en/stable/setup/cluster.html
I have tried with versions 2.3.0, 2.3.1, 3.0.0

After a few weeks of manual suffering (literally, full time weeks.) destroying and recreating servers,
I've switched to Ansible and now get my errors much quicker. I have an error factory, it seems:
The "enable cluster" post attempt returns either {"error":"unknown_error","reason":"badarg","ref":2080903964}
or "cluster is already enabled" error, 
or (my favorite one)
{"ok":true}
 followed by the add-node post attempt getting a 
{"error":"bad_request","reason":"Cluster is not enabled."}

The end result is always all nodes in "cluster_nodes" and only the setup-node in "all_nodes".
yet I can "nc -vz private-ip 5984" with success for all 3 ports 5984 4369 9100 for all hosts
attempting to finish the cluster times out.

Using Ansible has allowed me to try hundreds of combinations of the order of things, templating my vm.args and local.ini... I've literally attempted to build thousands of clusters. 
I have now spent over 3 months of my life, full time, trying to consistently make couchdb clusters.
I know your documents by heart at this point. I have a working cluster from about a year ago that I am afraid to touch, that I managed to make with Fauxton. I know it's possible. 

I must be extremely stupid (possibly a total idiot!) , but I've also literally spent months of my life trying to make couchdb clusters, so I'm fairly confident that this process can and should be improved.
At this point,  I'm miserable. This is causing health issues. CouchDB is a malicious entity laughing at me while it tries to murder me.  Am I crazy? Maybe. This is what CouchDB made me. I am its problem child. If CouchDB were not the only FOSS multi-master document DB in the world I would have run away ages ago.

## Description

"all_nodes":[only-setup-node], "cluster_nodes":[everyone]

I ensure that the vm.args has a shared cookie
I ensure that local.ini has same admin & password, same uuid, same secret.

If I manage to enable_cluster and add_node for all nodes, then when attempting to finish_cluster I usually get a {"error":"setup_error","reason":"Cluster setup timed out waiting for nodes to connect"}


## Steps to Reproduce
build servers. set firewall rules. install couchdb. mess with vm.args and local.ini. 
try to make a cluster. fail. wish that cluster configuration was in a file, not erlang memory or unicorn land. curl a few dozen more times anyway. destroy servers. repeat x10000000000000000000

[NOTE]: # ( Include commands to reproduce, if possible. curl is preferred. )

## Expected Behaviour

[NOTE]: # ( Tell us what you expected to happen. )
a cluster

## Your Environment

[TIP]:  # ( Include as many relevant details about your environment as possible. )
[TIP]:  # ( You can paste the output of curl http://YOUR-COUCHDB:5984/ here. )

* CouchDB version used: 2.3.0, 2.3.1, 3.0.0
* Browser name and version: chromium 80
* Operating system and version: ubuntu 16.04 & 18.04

## Additional Context

[TIP]:  # ( Add any other context about the problem here. )
clustering is stressful, difficult, apparently-magical, and contradicts this "just relax" concept.
User vectorselector: very sorry for the strong language. i have literally suffered a lot, however, and this must be recorded for posterity, either as a testament to my stupidity, or the difficulty I outlined above.

User vectorselector: vm.args file


local.ini

User vectorselector: firewall is ufw, each host allows from each other host on private ip for ports 5984, 4369, 9100
netcat to each from each works until trying to cluster, after which 5984 refuses, but port is open and works before cluster attempts
User vectorselector: ok, it just worked suddenly, first pass adding nodes gave document-update-conflict, finished single-node cluster, then enabled other nodes all over again, worked, then added other nodes again, then finish cluster again, then it worked. suddenly.
User vectorselector: but no joke, i still have no idea how it works, can't repeat it in ansible, can only suggest "jiggling the handle randomly" over and over again, then complaining like I did, at which point the contrarian tricky monster decides to work suddenly. 
User wohali: @vectorselector As you realised yourself, language like this is **not** welcome here and explicitly against our Code of Conduct.

If you are looking for general support with using CouchDB, and want a faster response, please try one of these other options:

* The user mailing list. Signup instructions are [here](http://couchdb.apache.org/#mailing-lists)
* The Slack/IRC chat room. Joining instructions are [here](http://couchdb.apache.org/#chat)

Next time try asking for help _before_ you get frustrated ;)
---------END---------
174
User microsoftceo2: <!-- NOTE: If you ignore this template, we will send it again and ask you to fill it out anyway. -->

**Have you read the [FAQ](https://bit.ly/ShakaFAQ) and checked for duplicate open issues?**
yes

**What version of Shaka Player are you using?**
3.0

**Can you reproduce the issue with our latest release version?**
yes

**Can you reproduce the issue with the latest code from `master`?**
yes

**Are you using the demo app or your own custom app?**
demo app

**If custom app, can you reproduce the issue using our demo app?**


**What browser and OS are you using?**
chrome, nexus 6, android

**For embedded devices (smart TVs, etc.), what model and firmware version are you using?**


**What are the manifest and license server URIs?**
<!-- NOTE:
  You can send the URIs to <shaka-player-issues@google.com> instead,
  but please use GitHub and the template for the rest.
  A copy of the manifest text or an attached manifest will **not** be
  enough to reproduce your issue, and we **will** ask you to send a
  URI instead.  You can copy the URI of the demo app to send us the
  exact asset, licence server, and settings you have selected there.
-->
https://bitmovin-a.akamaihd.net/content/art-of-motion_drm/mpds/11331.mpd

**What did you do?**
<!-- Steps to reproduce the bug -->
why u revoke nexus 6 ?? give keybox to me now .  nvidia shield . 
i am from india my name is ramjeet widevin is shit .!!

**What did you expect to happen?**
get amazon prime sunnxt key

**What actually happened?**
<!-- A clear and concise description of what the bug is -->
<!-- If applicable, you may add screenshots to help explain your problem. -->
nexus revoked :(.
User spookyahell: > Why you revoke Nexus 6?

I'll be brief: It's because of guys like you that this device is being revoked.

You - the person creating this issue @microsoftceo2 is shit btw - not widevine or the player, not even hiding the fact that you're using decryption keys illegally. Shame. It's because of people like you that real device owners and users can't use their devices as expected anymore.
Because suddenly an entire class of devices is being revoked.
User joeyparrish: This is not Widevine.  This is an open source video player.  You can contact Widevine here to share your complaints: https://www.widevine.com/contact

However, this thread is both off-topic and against our [Code of Conduct](https://github.com/google/shaka-player/blob/master/CODE_OF_CONDUCT.md).

In case you haven't read it, here's a relevant snippet, with **bold** added for emphasis:

> ## Our Standards
> 
> Examples of **behavior that contributes to creating a positive environment**
include:
> 
> *   **Using welcoming and inclusive language**
> *   **Being respectful** of differing viewpoints and experiences
> *   Gracefully accepting constructive criticism
> *   Focusing on what is best for the community
> *   Showing empathy towards other community members
> 
> Examples of **unacceptable behavior** by participants include:
> 
> *   The use of sexualized language or imagery and unwelcome sexual attention or
>     advances
> *   **Trolling, insulting/derogatory comments, and personal or political attacks**
> *   Public or private harassment
> *   Publishing others' private information, such as a physical or electronic
>     address, without explicit permission
> *   **Other conduct which could reasonably be considered inappropriate in a
>     professional setting**

@spookyahell, please refrain from name-calling, even if you feel the OP was out of line.

@microsoftceo2, please do not post something like this again.  This is your warning.  You are welcome in this community if you will follow our [Code of Conduct](https://github.com/google/shaka-player/blob/master/CODE_OF_CONDUCT.md), and we would be happy to help you with any issues you may have with Shaka Player itself.  But if you violate our [Code of Conduct](https://github.com/google/shaka-player/blob/master/CODE_OF_CONDUCT.md) again, you will be permanently banned.

Thanks for listening, everyone!
---------END---------
178
User bertoli517: ## Description

### What steps will reproduce the problem?

<!--- You can use Markdown here --->

I don't know what will reproduce the problem because I don't know what the problem is.  I just tried reinstalling you buggy, shitty software for the third time.

Maybe you guys can get one that works right and stick to it without changing it all the time.

### Traceback


## Versions

* Spyder version: 4.1.2 
* Python version: 3.7.6
* Qt version: 5.12.5
* PyQt5 version: 5.12.3
* Operating System: Windows 10

### Dependencies



User jitseniesen: Please moderate your language. The people who produce Spyder give it away for free so that you can use it. Developers are less likely to answer impolite bug reports.
User ccordoba12: > I just tried reinstalling you buggy, shitty software for the third time.

Closing and locking because we don't admit this behavior here, according to our Code of Conduct:

https://github.com/spyder-ide/spyder/blob/master/CODE_OF_CONDUCT.md

Please reopen a new issue and tone down your language if you want our help. If you continue with this behavior, we will simply ban you from our organization.
---------END---------
179
User DankBong420: <!---

Please read the FAQ:
https://citra-emu.org/wiki/faq/

THIS IS NOT A SUPPORT FORUM, FOR SUPPORT GO TO:
https://community.citra-emu.org/

If the FAQ does not answer your question, please go to:
https://community.citra-emu.org/

====================================================

When submitting an issue, please check the following:

- You have read the above.
- You have provided the version (commit hash) of Citra you are using.
- You have provided sufficient detail for the issue to be reproduced.
- You have provided system specs (if relevant).
- Please also provide:
  - For any issues, a log file
  - For crashes, a backtrace.
  - For graphical issues, comparison screenshots with real hardware.
  - For emulation inaccuracies, a test-case (if able).

--->like seriously, not everyone has a huge ass C drive. Most users secondary drive are bigger than their primary drive. And with Citra being the same GUI and shit as yuzu, it should take two seconds to add a file system tab inside Citra like you did with yuzu. 

And then a user in your Discord says it's currently in development? Citra's been out a lot longer than yuzu, even though they're made in the same language. 

I want my shit installed on my secondary drive. 

User B3n30: serously, I just explained to you that this is currently in development. See #5076

Also this is a duplicate issue. 

And this isn't a support forum but an issue tracker for development. 

Thus closed
---------END---------
180
User dmproia: He took the code beginning of 2017, not 7 years ago. Don‚Äôt be a dick as the source code was stolen at that point and implemented on this.
User vytas7: Hi again,
please be civilized and refrain from profanities as required by our [Code of Conduct](https://github.com/falconry/falcon/blob/master/CODEOFCONDUCT.md), or I'll ban you from all Falconry projects. This is the last warning.

If you are claiming that some of your code (that was proprietary or open source with a license that is incompatible to [ours](https://github.com/falconry/falcon/blob/master/LICENSE)) was allegedly appropriated into the Falcon code base, please specify which files, lines etc are affected, and please provide proof behind your claims.



User jmvrbanac: Falcon has a well known open-source track record since 2013. Back then it was built and primarily used by Rackspace for internal projects and was licensed as Apache v2 for open-source development; as was Rackspace policy at the time. Outside of this repository, you can also find discussion back from 2013, on the Openstack mailing lists regarding the potential usage of Falcon in Openstack projects as well.
User dmproia: While that may be true, I started it using a baremetal Rackspace hack I created locally around beginning of Jan 2017 using Ubuntu 12 and updating to Ubuntu‚Äôs 16 from an msige72-6qf laptop where I have comodo certificates linked to my original source code. If you require me to get a lawyer I can but this was legally registered to me and from what it appears kgriffs did a few major overhauls to his code using 99% of my stuff within a few months. I have you guys with your names hard coded all over my machine where you kept doing updates. See you in Tahiti, David
User kgriffs: We simply can not address unsubstantiated claims. Regardless, this appears to be bot spam.
User dmproia: I‚Äôm not a bot but if you wish to ignore me go ahead and I‚Äôll get a lawyer
User dmproia: ![image](https://user-images.githubusercontent.com/62180168/83285129-5a99ee00-a1a3-11ea-9e0c-143817c4a476.jpeg)

Proof and your name is tied to it which is tied to my source code and certificates.
User vytas7: Getting a lawyer sounds like a smart idea, particularly when one is involved in questionable activities such as operating spam bots.

In any case, this is a technical bug tracker, and we cannot resolve legal disputes here. Closing this issue.
---------END---------
181
User karls08: **Describe the bug**
Hey i cant even Find the ipa and i cant add a repo to AltStore!
User ThatStella7922: before leaving an issue like this check the fucking [releases](https://github.com/utmapp/UTM/releases)  page, that‚Äôs where the altstore repo and the IPAs are lmao
---------END---------
182
User ah1102: Where to f—Åk python2? Why, when I give the brew install python command, python3 is installed, not python2. Developers are you stoned there? Why is separation so difficult?
User Bo98: > Where to f—Åk python2?

I recommend not having intercourse with EOL software.

Python 2 is no longer included in Homebrew, but Apple still ship it at /usr/bin/python.
User ah1102: And I recommend you quit! There are many more where python2 is used and only some modules are installed under it, and you deleted it from the repository. Do you think at all with your head or do you have a hamburger head place?
User ah1102: And who attributed it to end-of-life? You yourself?
User Bo98: https://www.python.org/doc/sunset-python-2/
User SMillerDev: Please read our code of conduct and adjust your interactions accordingly.

https://github.com/Homebrew/.github/blob/master/CODE_OF_CONDUCT.md
User ah1102: You have an unfinished raw product with some problems. What can be your attitude? Treating users like shit.
---------END---------
183
User ghost: This name is in use and should be changed imediately. Yes, I have seen the other issues. Yes, I am opening a new one because fuck you Microsoft.You are merely trying to cast a shadow on other, truly open source projects. 

https://itsfoss.com/microsoft-maui-kde-row/?fbclid=IwAR3lO4SDxw3H01YsuKZ9G5D2zc1K-OOpjInCKnnY5GBpqiAdnhqfXJGG_EI
User Joshua-Ashton: It's also trademark infringement:

https://euipo.europa.eu/eSearch/#details/trademarks/013410601
User ChaseFlorell: Microsoft and the PMs here are already aware and are already looking into solutions. This "Issue" is just inflammatory and violates the code of conduct. There is an existing (open) issue that is much more civil and constructive.

https://github.com/dotnet/maui/issues/41 and https://github.com/dotnet/maui/issues/58#issuecomment-631436935

Above all else, be kind.
User Hual: This is why Linux products are the way they are, and why no one had ever heard of KDE MAUI until now -- you lot just argue about inane stuff instead of getting work done.
User andrewBezerra: In
https://devblogs.microsoft.com/dotnet/introducing-net-multi-platform-app-ui/
is written:

    "that: .NET Multi-platform App UI, affectionately call .NET MAUI."
Note:
 
     "affectionatelly call..."
and not:

    "also call"

For me, MAUI is not the REAL name of "product".
User legistek: > It's also trademark infringement:
> 
> https://euipo.europa.eu/eSearch/#details/trademarks/013410601

Are you a lawyer? Maybe you should check with one before you potentially commit business libel.
User davidortinau: @Native-Coder your tone and style of commenting is violating our [Code of Conduct](https://dotnetfoundation.org/about/code-of-conduct). We have marked the problematic comments as abusive and blocked you for 7 days. If this behavior persists, we‚Äôre going to block you permanently.


User davidortinau: Duplicate of #34 
---------END---------
185
User yusosov: **THE ISSUE: in Jupyter/Lab for Python, no matter what Kernel is selected, the actual available environment will be the one from which Jupyter Lab is started.**

DISCUSION: what is the purpose of different Jupyter Kernels if nothing changes, the available environment stays the same no matter what the chosen Kernel is?

WHY IMPORTANT:
I am working in Azure ML Cloud with MS sponsored account, where I do not have admin privileges and I can only start Jupyter/Lab through the link provided by Azure automatically. The link starts Jupyter from the (base) environment. No matter what other environments I create, I cannot access them. I have to always stay in (base). Jupyter/Lab gives impression that I/it can switch environments, but in fact 
it DOES NOT SWITCH ENVIRONMENTS.

WHERE VERIFIED: 
* in Jupyter and Jupyter Lab on Azure ML Cloud (ubuntu 16.04 +python 3.6 ),
jupyter                   1.0.0                    py36_7  
jupyter_client            5.3.3                    py36_1  
jupyter_core              4.5.0                      py_0  
* my local machine (ubuntu 19.10 + Python 3.8.2), 
jupyter_client            6.1.2                      py_0    conda-forge
jupyter_core              4.6.3            py38h32f6830_1    conda-forge
* another unrelated local ubuntu environment (ubuntu 19.10 + python 3.6)
jupyter_client            6.1.2                      py_0  
jupyter_core              4.6.3                    py36_0  

REPRODUCING THE ISSUE:
 following jupyter recommended https://jupyterlab.readthedocs.io/en/stable/getting_started/issue.html

go home:
cd ~
**_/home/user_**
create clean test environment test1:
**conda create -n jlab-test1 --override-channels --strict-channel-priority -c conda-forge -c anaconda jupyterlab
conda activate jlab-test1
python -m ipykernel install --user --name test1 --display-name 'test1'
conda deactivate**

create clean test environment test2:
**conda create -n jlab-test2 --override-channels --strict-channel-priority -c conda-forge -c anaconda jupyterlab
conda activate jlab-test2
python -m ipykernel install --user --name test2 --display-name 'test2'
conda deactivate**

start jupyter lab in test1:
**conda activate jlab-test1
jupyter lab**

**in jupyter/lab: file>New>Notebook>Select Kerlel>test2**
in Notebook with selected test2 Kernel: execute: 
**!conda env list**
output: 
**_jlab-test1            *  /home/user/anaconda3/envs/jlab-test1
jlab-test2                  /home/user/anaconda3/envs/jlab-test2_**
#
**in the same notebook, change Kernel: Select Kerlel>test1**
in the same notebook, with selected test1 Kernel: execute: 
**!conda env list**
output: 
**_jlab-test1            *  /home/user/anaconda3/envs/jlab-test1
jlab-test2                  /home/user/anaconda3/envs/jlab-test2_**

the same can be verified if Jupyter/Lab is started from test2
User Vinnitsky: I faced with the same behavior. [IPython kernel](https://ipython.readthedocs.io/en/5.x/install/kernel_install.html) says 

> If you want to have multiple IPython kernels for different virtualenvs or conda environments, you will need to specify unique names for the kernelspecs.

according to the citation - it is a bug
User jasongrout: You can also try the nb_conda_kernels package
User yusosov: Why changing the title? If it's a feature, it is USELESS.
User timkpaine: @yusosov try to be respectful
User timkpaine: @yusosov here is the code of conduct: https://github.com/jupyter/governance/blob/master/conduct/code_of_conduct.md
User timkpaine: closing as this is unrelated to jupyterlab and well documented issue across various other repos and stack overflow. 
User yusosov: Please indicate where this issue it documented?
User timkpaine: google is a good tool, here is a sample article I found by googling "multiple conda evironments jupyter" https://dwflanagan.github.io/2018-04-20-conda-envs-in-jupyter/

-----

It looks like the tests were added in https://github.com/matplotlib/matplotlib/pull/15962/ as an exercise in increasing the test coverage and codified the existing behavior (rather than representing the intention of the original author).  These tests also identified that the proposed change affects how ever-other half integer is rounded (as `int(1.5) == 1` and `round(1.5) == 2` (which is to avoid the "directional" bias by always going to towards even numbers)) so the effects are wider than just this fix.   

An alternative fix would be to discard any negative values and leave the rounding behavior as-is.
User efiring: I recommend leaving the IndexFormatter deprecated and removing it as scheduled.  @Wlodarski, what are some examples of real-world usage of the IndexFormatter's functionality?  Would it make sense to add an example to the docs or elsewhere using FuncFormatter?  There is no need for the lambda; a normal function definition can be used, with as many lines as are needed to make it easy to read and understand.  
User Wlodarski: @ImportanceOfBeingErnest With the proposed solution, neither option 1 nor option 2 is needed. And surely, those are not the only 2 options, nor the only conceivable solution. @tacaswell just proposed an alternative fix, the first fix that actually makes sense to me.

@ImportanceOfBeingErnest If an index of ``-1`` is not sensical then the actual test is also not sensical since it makes sure the label associated with an index of ``0`` is displayed as a substitute for an index of ``-1``.

In summary: The system is beyond tedious. It allowed a bug to remain hidden for more than a decade and gives the false impression the developers can infer actual usage based on saliency of problems. Trouble free functionalities are mistaken for useless functionalities, while functionalities that frequently suck the attention of the developers are promoted to further monopolize their efforts. A simple evident solution, rejected by a test that requires the bug to be present, is about to be swept under the rug in favour of increased complexity imposed on the users and malpractices like deprecations within a minor version. Explaining away malpractices by pointing out others also are tempted to commit them is not a valid point. What should have taken no time to solve definitively years ago while maintaining backward compatibility evolves toward a situation driven by a sunk cost fallacy. Criticism of the system is perceived as criticism of the volunteer developers, surely overworked in view of the heaviness of the system. You can tell yourself whatever you want but the fact is that a simple fix like rounding a value instead of truncating it ‚Äïreplacing a ``int(x + 0.5)`` by ``round(x)``‚Äï has become for many of you an unthinkable option while it seems perfectly _reasonable_ to ask users to rewrite their programs with lambda functions encapsulating zipped iterables while, on your side, envisioning rewriting all the related documentation.

@anntzer I am not blaming any of you. You are the primary victims of this bureaucratic mess. The way the system is set up, for historical reasons I assume, robs of agility and makes the loss of perspective almost ineluctable. The handling of this extremely simple bug is symptomatic of this involuntary loss of perspective. It's bound to happen, especially after you have contributed thousands of hours of our free time to this project. When the easy solutions seem inconceivable and the hard convoluted ones seem far easier to implement, the system is not working for you anymore: you are working for it.
User Wlodarski: >Would it make sense to add an example to the docs ...

If you are going the deprecation route, a recipe how to refactor ``IndexFormatter`` with minimal changes to the user's software might be helpful. For instance,
>   Add to your code :
> 
> 
> and rename all instances of the uppercase ``IndexFormatter`` for the lowercase``indexformatter``.

User tacaswell: @Wlodarski I will take you at your word that you intended to criticize the system, however your text reads as very direct personal criticisms of our volunteers which is not acceptable behavior.
User anntzer: As noted by @efiring the lambda is not optimal for legibility, I simply crammed everything in one line as this was easier to type quickly but a more legible version would be e.g.

As for most API deprecations, an even simpler route for people not wanting to move to the new API would be to just copy-paste the old IndexFormatter implementation to their codebase (and possibly fix the round() bug at the same time).

In any case, I think this issue is in good hands now and will thus unsubscribe from it to avoid further miscommunication problems.
User tacaswell: I am inclined to close this with no-action because the `IndexFormatter` is slated to be deprecated.  It has many issues (as both @ImportanceOfBeingErnest and @anntzer pointed out) and at least one bug (as reported here) when used in unintended ways.

If someone wants to open a PR that checks for being less than 0 before rounding (and correct 1 test case + document the behavior change) that would probably be merged for 3.3, but I do not think we need an open issue to track it.

I want to remind everyone again that we expect everyone to engage with each other in a professional manner and follow the PSF Code of Conduct (https://www.python.org/psf/conduct/) on github.

-----

I think the OP's use case could be solved using the string categorical support:
![gh](https://user-images.githubusercontent.com/199813/76973057-cd662d80-6905-11ea-85f6-19d9cc6a8e6e.png)

---------END---------
188
User delata: I think if you don't want to use C, then at first you should design and implement your own language. Not C++ hell.

Like how Terry Davis did.
User sunverwerth: ü§î I'm partial to INTERCAL
User emanuele6: https://youtu.be/_qjPeZH4afQ
---------END---------
189
User NicTanghe: Here's an id

Make 100% sure you dont fuck up the ui for coutries where people speak diferent languages like Belgium (ur collegues fucked up bigtime with the store)

we dont really need a gui tho. could be nice as a full store replacer tho.
User megamorf: The translations are created using the internal localizations services of Microsoft. If you have a concrete improvement then create a PR so that it can be forwarded to the team that is responsible. 

According to @JohnMcPMS  (see #240):
> The only option that I'm aware of is either using Feedback Hub, which will create a bug that can be routed internally, or something like a PR. The nice thing about the PR here is we can point the internal team to it, where they can see the string's symbolic name and I assume it makes the process easier.
User doctordns: I believe it's Snover's 4th law:  Don't tell me I screwed up, tell me HOW I screwed up. If you want, hope, or expect traction,  you need to provide specific actionable feedback. So far I see nothing actionable and suggest you either add specifics or close this issue.
User TeddyAlbina: > Here's an id
> 
> Make 100% sure you dont fuck up the ui for coutries where people speak diferent languages like Belgium (ur collegues fucked up bigtime with the store)
> 
> we dont really need a gui tho. could be nice as a full store replacer tho.

La belgique n'est pas un vrai pays  :p
User zachcarp: @NicTanghe This looks like a duplicate of #238 

Is there anything in this issue that is not covered there?
User NicTanghe: > 
> 
> > Here's an id
> > Make 100% sure you dont fuck up the ui for coutries where people speak diferent languages like Belgium (ur collegues fucked up bigtime with the store)
> > we dont really need a gui tho. could be nice as a full store replacer tho.
> 
> La belgique n'est pas un vrai pays :p

Bwa kweni ze vriend.
J`aime bien ici.

Well the Windows store is allot worse than using the language of my country it uses 
Dutch and French and English completely mangled for 0 reason since day 1 and 0 response on my tickets even tho everyone in Belgium has this issue but I'm not here to complain about that disaster. Obv Microsoft has seen the light because of vscode and using so much Linux to run their own systems now they have a better idea about how to improve their products.

This is just because i fear someone might reuse the same systems that were in place on the store. And warn them not to do that see screenshot.

![image](https://user-images.githubusercontent.com/13624265/83172127-693ac380-a117-11ea-9919-a7209af16415.png)

User NicTanghe: ![image](https://user-images.githubusercontent.com/13624265/83172479-f0883700-a117-11ea-8898-225dce842d11.png)
Red : French
Orange: Dutch
Green: English

User NicTanghe: > 
> 
> I believe it's Snover's 4th law: Don't tell me I screwed up, tell me HOW I screwed up. If you want, hope, or expect traction, you need to provide specific actionable feedback. So far I see nothing actionable and suggest you either add specifics or close this issue.

Sorry i thought everyone ws verry aware of the issue and never fixed it because all the places you could file bugs had a mention issues with the language being worked on.
User KevinLaMS: tweaking title
User NicTanghe: Thanks i should have done that myself but this issue has frustrated me since day 1 and i really wanted to use that store because it would have made softwaremanagment allot easier and that was a large part of my job. Now it seems like all problems will be fixed anyway.
---------END---------
190
User NicTanghe: PS. fucking finaly dont you guys think this is  years overdue ?

User megamorf: This repo's README.md points you to https://github.com/microsoft/winget-pkgs/ which contains all manifests as well as steps on how to submit your own. This issue can be closed.
User chausner: There is already a community-built package catalogue online: https://winstall.app/
User the-taj: > This repo's README.md points you to https://github.com/microsoft/winget-pkgs/ which contains all manifests as well as steps on how to submit your own. This issue can be closed.

Agreed. Should be closed. Also comment is Off-Topic and not productive.
User NicTanghe: The repo's repo points to a GitHub page with all the packages which is not at all like a website you can search for packages on like the aur or chocolatey has. Closed because it has solved by chausner's answer.

:edit damn so much hostility here.
---------END---------
191
User Delitants: #### Problem Description
mitmproxy --mode socks5 --listen-port 1080 --ssl-insecure

"Error in HTTP connection: Certificate verification error for None: self signed certificate in certificate chain (errno: 19, depth: 2)
clientdisconnect
clientconnect
Certificate verification error for None: self signed certificate in certificate chain (errno: 19, depth: 2)
Invalid certificate, closing connection. Pass --ssl-insecure to disable validation."

#### Steps to reproduce the behavior:
Just open anything with self-signed cert, socks5 mode.

#### System Information
> Paste the output of "mitmproxy --version" here.
Mitmproxy: 5.0.1
Python:    3.8.2
OpenSSL:   OpenSSL 1.1.1f  31 Mar 2020
Platform:  macOS-10.15.3-x86_64-i386-64bit

User mhils: I can't reproduce this.

works fine. Here's how I invoke mitmproxy:



User Delitants: You kidding me, right? There is a bunch of reports **of the same issue** in your issue tracker. I've posted an exact output of the log. If you can't reproduce, than this **is solely your problem**, but it doesn't meant bug doesn't exist. Perhaps you should follow what I have wrote, that it's on MACOS.
Also why don't you try to access a naked ip host with self signed cert? Did I mention it has to be domain?
I used ssl split and it worked fine, comparing to mitmproxy. Bye!
User Delitants: Whoever will find this issue and gets pissed off, because author doesn't bother to fix it for years, **ditch mitmproxy and use SSL SPLIT**
User mhils: I'm glad you have found something that works for you! 

Closing this as a duplicate of #3865.
---------END---------
192
User TheTechsTech: **Symfony version(s) affected**: 
Process 5.0.4

**Description**  
Need a way to retrieve just the last output to `stdout`, or in my case the return value from the process. Currently only all output, or incrementally, which I shouldn't have to read first. 
- I want the last line written to `stdout`. 
- None of your tests use or check for a `return` value.

**How to reproduce**  
In tests at:
https://github.com/symfony/process/blob/55bebe44b76b82d618e9eeea83f6562ad03a1a67/Tests/ProcessTest.php#L1212
Could check for:
`$this->assertSame('pong', $process->getResult());`
in tests at:
https://github.com/symfony/process/blob/55bebe44b76b82d618e9eeea83f6562ad03a1a67/Tests/ProcessTest.php#L1242
Could check for:
`$this->assertSame('456', $process->getResult());`

<!-- Code and/or config needed to reproduce the problem. If it's a complex bug,
     create a "bug reproducer" as explained in:
     https://symfony.com/doc/current/contributing/code/reproducer.html -->

**Possible Solution**  
add: `private $result;`
add to method `addOutput(string $line)`:  `$this->result = $line;`


**Additional context**  
Additional there is not way to return a final result in any of your tests.
Here is a rework of one of your tests 
https://github.com/symfony/process/blob/55bebe44b76b82d618e9eeea83f6562ad03a1a67/Tests/ProcessTest.php#L1195

I have a package that wraps this library with the above test, but `getOutput()` returns full with `3` encoded. With the change i could a least get that part directly to decode, without local changes.
https://github.com/symplely/processor/blob/147b6328981559255401b70cbba943f101e6f16a/tests/ProcessorTest.php#L134. 

Maybe the way your tests setup to not return any usable value?

User nicolas-grekas: A subprocess is a separate process. The return value from a PHP script, as in `return 3;` is ignored, there is no way around.
Processes have only few limited ways to communicate with each other.
Writing to stdout/stderr is one of them: you can "echo 3;" instead and parse the output from the main process.

I am sorry, but I have to close your issue as we use GitHub issues only to discuss new features and for bug reports. For support, please refer to one of the support channels. Thank you for understanding.
User TheTechsTech: > Processes have only few limited ways to communicate with each other.

What? 
It seems you missed the point, or no foresight.
For one, I wasn't looking for a solution. I already had one for you. I already get a return value from my scripts in the links i posted.  So:

> Writing to stdout/stderr is one of them: you can "echo 3;" instead and parse the output from the main process.

Makes no sense, your tests doesn't pick it up. And still from your `"echo 3;"`
How would you get just that part separate from the other output? 
The solution would do just that. It should have been clear, from the test/example the shortcoming of your tests, they wasn't capturing the return or even the just the last the thing written.

> I am sorry, but I have to close your issue as we use GitHub issues only to discuss new features and for bug reports. For support, please refer to one of the support channels. Thank you for understanding.

I seems you looked at the title and left it there, you didn't review or anything, I would hate to have you in charge of any security issues. 
User Pierstoval: Retrieving the exit code is okay, the component does it already.
Retrieving the last `stdout` line should be possible with `$lastLine = end($process->getOutput());`

Return value might be relevant for PHP scripts only, then, but this means that instead of running a sub-process, you actually need to **include** the PHP file.

Else, any `return` statement in a PHP script executed in a sub-process can't be fetched at all (it's PHP, so the `return` is executed in the PHP engine, and the process itself cannot know that).

User TheTechsTech: > Retrieving the last `stdout` line should be possible with `$lastLine = end($process->getOutput());`

Maybe add a test for that, I already have a working solution. Also, isn't `end()` just for arrays?

> Else, any return statement in a PHP script executed in a sub-process can't be fetched at all (it's PHP, so the return is executed in the PHP engine, and the process itself cannot know that).

The links I provided do just the opposite. I get the return value to `stdout` right from symfony process. It's a script the process will execute any passed in function in a **try/catch** block the result to `stdout`, or `stderr` on exception.



User Nyholm: Hey. 
Im Tobias and I work with the [CARE team](https://symfony.com/doc/current/contributing/code_of_conduct/care_team.html). 

@techno-express, I would like to remind you about our [code of conduct](https://symfony.com/doc/current/contributing/code_of_conduct/code_of_conduct.html). Comments like you made on this issue is a violation to our COC. I would recommend you to use a more welcoming language and be more respectful. That will create a better community for everyone. 

It is okey to have different opinions or to think that somebody else is wrong. But there are better ways to express your dissatisfaction. 
User TheTechsTech: Well, replies offering a solution I already implemented and stated, does
not seem like anyone actually read the submition.

At least address that first.

On Wed, Mar 11, 2020, 4:48 PM Tobias Nyholm <notifications@github.com>
wrote:

> Hey.
> Im Tobias and I work with the CARE team
> <https://symfony.com/doc/current/contributing/code_of_conduct/care_team.html>
> .
>
> @techno-express <https://github.com/techno-express>, I would like to
> remind you about our code of conduct
> <https://symfony.com/doc/current/contributing/code_of_conduct/code_of_conduct.html>.
> Comments like you made on this PR is a violation to our COC. I would
> recommend you to use a more welcoming language and be more respectful. That
> will create a better community for everyone.
>
> It is okey to have different opinions or to think that somebody else is
> wrong. But there are better ways to express your dissatisfaction.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/symfony/symfony/issues/35919#issuecomment-597871065>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AHDHVFJOHUNUZWILT6SZJLLRG72I7ANCNFSM4K7K4E5Q>
> .
>

User Nyholm: I understand that you may be annoyed. It is okey to be annoyed. 
I understand that you think others are incorrect. That is okey too. 
It is also okey to make a possible wrong decision and close an issue. 

What is **not** okey is: 
- Insulting/derogatory comments and personal attacks
- Public or private harassment
- Other conduct which could reasonably be considered inappropriate in a professional setting

... among other things stated in the [Code of conduct](https://symfony.com/doc/current/contributing/code_of_conduct/code_of_conduct.html)

--------------

I am fine leaving this issue unlocked for now. But if there is more discussions about CoC or more CoC violations, then I'll lock it. 

If somebody have questions about the CoC, feel free to email me at care@symfony.com.
User TheTechsTech: I think someone been misreading, there is no PR open, and did not request
one.

Your reply should  be directed elsewhere, the issue been pre close, without
addressing the actual contents. The replies afterwards was also misplaced.

At least point out each part you all detailed, issue submit requirements.
That no seem to have read.

Everybody do have there opinions, code is not one, it either does want said
or not , and that can be proven.


On Wed, Mar 11, 2020, 5:59 PM Tobias Nyholm <notifications@github.com>
wrote:

> I understand that you may be annoyed. It is okey to be annoyed.
> I understand that you think others are incorrect. That is okey too.
> It is also okey to make a possible wrong decision and close a PR.
>
> What is *not* okey is:
>
>    - Insulting/derogatory comments and personal attacks
>    - Public or private harassment
>    - Other conduct which could reasonably be considered inappropriate in
>    a professional setting
>
> ... among other things stated in the Code of conduct
> <https://symfony.com/doc/current/contributing/code_of_conduct/code_of_conduct.html>
> ------------------------------
>
> I am fine leaving this PR unlocked for now. But if there is more
> discussions about CoC or more CoC violations, then I'll lock it.
>
> If somebody have questions about the CoC, feel free to email me at
> care@symfony.com.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/symfony/symfony/issues/35919#issuecomment-597898831>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AHDHVFNCF7ZOXGMADYIHXBLRHACUVANCNFSM4K7K4E5Q>
> .
>

---------END---------
193
User w0dm4n: Hello, there is an issue with the endpoint '/auth/local/register'

While i add a required column to 'Users' on the admin dashboard:

> {
> 	"username": "foo2",
> 	"email": "foo2@foo.com",
> 	"password": "hello"
> }

this request fail everytime with the error 'username already exist' when i add on User collection another required field, i suppose that it due to the fact that it did not take the new required column as part of the scheme?
Thanks.
User derrickmehaffy: > This is a templated message

Hello, please follow the issue template.

A proper issue submission let's us better understand the origin of your bug and therefore help you.

I will reopen your issue when we receive the issue following the template guidelines and properly fill out the template.

Please update the issue.

Thank you.

> A copy of the bug report template can be found below:


User w0dm4n: Yeah, not really sorry i'm lazy, and it's more to help you then me.
It's simple to understand: with your registration system there is no ways to add required columns or it block completely the calls saying that the basic 'username' one isnt in the payload. don't need a retarded format to understand that!
thanks.
---------END---------
194
User duzenko: After upgrading flutter today to the current "stable" version I can no longer run the app under debugger.



The device shows the splash screen and that's it.

`flutter run` works but no debugging options are available.


User TahaTesser: Hi @duzenko 
You might wanna run `flutter run --verbose` or `Shift + F9`?
Thank you
User duzenko: > 
> 
> Hi @duzenko
> You might wanna run `flutter run --verbose` or `Shift + F9`?
> Thank you

`Shift + F9` is exactly what I mean by debugger 

`flutter run` works but no debugger options are available

<details>
 <summary>logs</summary>



</details>
User TahaTesser: Hi @duzenko 
Can you reinstall Flutter plugin and try again with Shift + F9?
Do you have any breakpoints for testing?
Thank you
User duzenko: > 
> 
> Hi @duzenko
> Can you reinstall Flutter plugin ...?

How do I do that?
User TahaTesser: @duzenko 
Go to File>Settings>Plugins>Installed> Uninstall Dart and Flutter not disable and restart Android Studio and go to same Plugins Window, switch to Markpetplace and search for Flutter Plugin and it will also install Dart and run `Shift + F9` with breakpoints in your app for debugging
Thank you
User duzenko: > 
> 
> @duzenko
> Go to File>Settings>Plugins>Installed> Uninstall Dart and Flutter not disable and restart Android Studio and go to same Plugins Window, switch to Markpetplace and search for Flutter Plugin and it will also install Dart and run `Shift + F9` with breakpoints in your app for debugging
> Thank you

Same result: 

App stuck at splash screen
I don't think I have any breakpoints set, at least they don't show anywhere
User TahaTesser: Hi @duzenko 
Do you see debug  window at bottom?

![image](https://user-images.githubusercontent.com/48603081/75791481-a0a50880-5d92-11ea-9bb0-aa1d865af295.png)

User duzenko: > 
> 
> Hi @duzenko
> Do you see debug window at bottom?
> 
> ![image](https://user-images.githubusercontent.com/48603081/75791481-a0a50880-5d92-11ea-9bb0-aa1d865af295.png)

I can see the window but no breakpoints (and neither on your screenshot)
User TahaTesser: @duzenko 
i added breakpoint 

![image](https://user-images.githubusercontent.com/48603081/75791854-2f198a00-5d93-11ea-876e-9504245e5e36.png)

No issues here
User duzenko: I just did a clean install of Android Studio and Flutter plugin 
The debugger still does not work, including the hello world app
It gets stuck on the splash screen
What exactly response do you expect from me at this point?
User TahaTesser: @duzenko 
If you're seeing the debug window at bottom then debug works 
Here you check how to [Run app with breakpoints](https://flutter.dev/docs/development/tools/android-studio#run-app-with-breakpoints)

Also, to better address the issue, would be helpful
if you could post a minimal code sample to reproduce the problem
Thank you

User duzenko: Read again

THE APP IS STUCK AT SPLASH SCREEN

Thank you
User TahaTesser: @duzenko 
to better address the issue, would be helpful
if you could post a minimal code sample to reproduce the problem
Thank you
User duzenko: > 
> 
> @duzenko
> to better address the issue, would be helpful
> if you could post a minimal code sample to reproduce the problem
> Thank you

> The debugger still does not work, including the hello world app


User TahaTesser: Hi @duzenko 

### Code Sample


![image](https://user-images.githubusercontent.com/48603081/75794723-35116a00-5d97-11ea-8a5d-36f3f5675e8e.png)

![flutter_01](https://user-images.githubusercontent.com/48603081/75794768-43f81c80-5d97-11ea-9b63-c027d32faa0a.png)

Your code runs and debug
User duzenko: Great!
How do I get it running and debugging on my side?
User TahaTesser: @duzenko 

Have you tried setting breakpoints? 

User duzenko: > 
> 
> @duzenko
> 
> Have you tried setting breakpoints?

Yes I did
User duzenko: I just checked and it works in hotfix 7
User TahaTesser: Hi @duzenko 
Debugging works normally on `Flutter (Channel stable, v1.12.13+hotfix.8` and there are no issues related your issue in hotfix 8 

Please try again on Hotfix 8 and let me know if your problem is resolved
Thank you
User duzenko: It does NOT work on hotfix 8
Hotfix 7 works
User iapicca: same for me
(on my home laptop)
logs are clean


same story on dev
User iapicca: https://github.com/flutter/flutter/issues/48789
User RanCohenn: > 
> 
> It does NOT work on hotfix 8
> Hotfix 7 works

dude you have no idea how long I have been trying to fix this and now it works ty
User goderbauer: @christopherfujino The issue claims that hotfix8 is breaking IDE debugging. Could your patches that went into the hotfix have caused this?
User christopherfujino: > @christopherfujino The issue claims that hotfix8 is breaking IDE debugging. Could your patches that went into the hotfix have caused this?

I'll investigate
User christopherfujino: I can't repro this from either macOS or linux. **edit** Actually macOS or Windows.
User duzenko: > 
> 
> I can't repro this from either macOS or linux.

> [‚àö] Flutter (Channel stable, v1.12.13+hotfix.8, on Microsoft Windows [Version 10.0.17134.1246], locale en-GB)

Wow, great testing effort

User christopherfujino: > > I can't repro this from either macOS or linux.
> 
> > [‚àö] Flutter (Channel stable, v1.12.13+hotfix.8, on Microsoft Windows [Version 10.0.17134.1246], locale en-GB)
> 
> Wow, great testing effort

@duzenko
I'm sorry, I mispoke, I tested this on macOS and Windows.
User dnfield: @duzenko - I understand that this is a really frustrating issue, but please take a moment to reread our [Code of Conduct](https://github.com/flutter/flutter/blob/master/CODE_OF_CONDUCT.md). In particular, the code expects members of this community to actively work at being pleasant towards one another.  If you cannot see why some of your comments in this thread violate those expectations, I would encourage you to take a less active role.

There are multiple people who are actively trying to help you with this problem. Let's try to work together on it rather than against each other.
---------END---------
195
User mryellow: This Code of Conduct excludes rather than includes.

https://github.com/twbs/bootstrap/blob/83b49aa688e30f0d2af22110b94c7d8b3deffcf0/CODE_OF_CONDUCT.md

Competition in comparison:

https://github.com/foundation/foundation-sites/blob/30f3247db9ca61ab1d525cf36b373760db96a85b/CONTRIBUTING.md

Bootstrap includes politically motivated tone-policing anti-debate tactics against meritocratic contributors. Where Foundation does not.

This makes framework choice easy in that I can choose the one which doesn't have such a policy.


User patrickhlauke: you're comparing bootstrap's code of conduct with foundation's contributing document...which then points out that all contributions are expected to follow foundation's code of conduct

> Every participant is expected to follow the project's [Code of Conduct](https://github.com/foundation/foundation-sites/blob/30f3247db9ca61ab1d525cf36b373760db96a85b/code-of-conduct.md) so please be courteous and respectful.

 ... which seems to me pretty much the same (unless there's a specific wording that makes you say it's any different?)


User mryellow: Yeah does look like the same parasites (`contributor-covenant.org`) got to them too.

Bummer. 

This is a much better template:


User patrickhlauke: you do you.
User mdo: Rarely do we have to enforce these guidelines and take action‚Äîgeneral guideline as always is to not be a jerk and you‚Äôll be just fine :).
---------END---------
